{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UNet++ 脑肿瘤分割 - 训练Notebook\n",
        "\n",
        "本notebook用于训练基于UNet++的医学图像分割模型.\n",
        "\n",
        "## 模型特点:\n",
        "- UNet++: 改进的UNet架构，具有嵌套的跳跃连接\n",
        "- 使用segmentation-models-pytorch库\n",
        "- 支持预训练编码器\n",
        "\n",
        "## 功能:\n",
        "1. 从3D体积中提取2D切片\n",
        "2. 加载和训练UNet++模型\n",
        "3. 模型评估和可视化\n",
        "4. 与之前的模型进行对比\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 安装依赖和挂载Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 挂载Google Drive\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "# 尝试挂载，如果失败则重试\n",
        "max_retries = 3\n",
        "retry_count = 0\n",
        "\n",
        "while retry_count < max_retries:\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"Google Drive 挂载成功！\")\n",
        "        break\n",
        "    except ValueError as e:\n",
        "        retry_count += 1\n",
        "        if retry_count < max_retries:\n",
        "            print(f\"挂载失败，{retry_count}/{max_retries} 次重试...\")\n",
        "            print(\"请确保：\")\n",
        "            print(\"1. 已点击授权链接并完成授权\")\n",
        "            print(\"2. 网络连接正常\")\n",
        "            print(\"3. 等待几秒后重试\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            print(\"挂载失败，请手动运行以下命令：\")\n",
        "            print(\"from google.colab import drive\")\n",
        "            print(\"drive.mount('/content/drive')\")\n",
        "            raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的包\n",
        "%pip install segmentation-models-pytorch -q\n",
        "%pip install nibabel -q\n",
        "%pip install albumentations -q\n",
        "%pip install einops -q\n",
        "%pip install matplotlib -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 导入库和配置参数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用设备: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据路径配置\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/data-brain-2024\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/brain-tumor-models-unetpp\"\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# 训练参数\n",
        "IMG_SIZE = 256  # 2D图像尺寸\n",
        "BATCH_SIZE = 8  # 2D数据可以使用较大的batch size\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 50\n",
        "VAL_INTERVAL = 1\n",
        "NUM_CLASSES = 4  # 背景 + 3个肿瘤类别\n",
        "\n",
        "# 预训练编码器选择\n",
        "PRETRAINED_ENCODER = 'resnet34'  # 可选: resnet34, resnet50, efficientnet-b0等\n",
        "\n",
        "# 切片提取参数\n",
        "SLICE_START = 22\n",
        "NUM_SLICES = 100\n",
        "\n",
        "print(f\"数据路径: {DRIVE_DATA_PATH}\")\n",
        "print(f\"模型保存路径: {MODEL_SAVE_PATH}\")\n",
        "print(f\"图像尺寸: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"学习率: {LEARNING_RATE}\")\n",
        "print(f\"预训练编码器: {PRETRAINED_ENCODER}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 数据加载和预处理（提取2D切片）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_patient_groups(data_path):\n",
        "    \"\"\"获取所有患者的数据分组\"\"\"\n",
        "    all_files = glob.glob(os.path.join(data_path, \"*.nii\"))\n",
        "    patient_groups = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    for file_path in all_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        match = re.match(r'BraTS-GLI-(\\d+)-(\\d+)-(t1n|t2f|t2w|t1c|seg)\\.nii', filename)\n",
        "        if match:\n",
        "            patient_id = match.group(1)\n",
        "            sequence_id = match.group(2)\n",
        "            modality = match.group(3)\n",
        "            patient_groups[patient_id][sequence_id][modality] = file_path\n",
        "\n",
        "    complete_patients = {}\n",
        "    for patient_id, sequences in patient_groups.items():\n",
        "        for seq_id, modalities in sequences.items():\n",
        "            if 't2f' in modalities and 't1c' in modalities and 'seg' in modalities:\n",
        "                if patient_id not in complete_patients:\n",
        "                    complete_patients[patient_id] = {}\n",
        "                complete_patients[patient_id][seq_id] = modalities\n",
        "\n",
        "    return complete_patients\n",
        "\n",
        "def load_nifti_volume(file_path):\n",
        "    \"\"\"加载NIfTI文件并返回numpy数组\"\"\"\n",
        "    nii = nib.load(file_path)\n",
        "    data = nii.get_fdata()\n",
        "    return data\n",
        "\n",
        "def extract_slices_from_volume(volume, start_idx=22, num_slices=100):\n",
        "    \"\"\"从3D体积中提取2D切片（沿z轴）\"\"\"\n",
        "    depth = volume.shape[2]\n",
        "    end_idx = min(start_idx + num_slices, depth)\n",
        "    slices = volume[:, :, start_idx:end_idx]\n",
        "    return slices\n",
        "\n",
        "def normalize_slice(slice_data):\n",
        "    \"\"\"归一化单个切片\"\"\"\n",
        "    slice_data = slice_data.astype(np.float32)\n",
        "    max_val = np.max(slice_data)\n",
        "    if max_val > 0:\n",
        "        slice_data = slice_data / max_val\n",
        "    return slice_data\n",
        "\n",
        "def remap_labels(label_slice):\n",
        "    \"\"\"将标签值4映射到3\"\"\"\n",
        "    label_slice = label_slice.astype(np.int64)\n",
        "    label_slice[label_slice == 4] = 3\n",
        "    return label_slice\n",
        "\n",
        "# 获取所有患者数据\n",
        "all_patient_groups = get_patient_groups(DRIVE_DATA_PATH)\n",
        "patient_ids = list(all_patient_groups.keys())\n",
        "\n",
        "print(f\"找到 {len(patient_ids)} 个患者\")\n",
        "print(f\"前5个患者ID: {patient_ids[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备2D切片数据\n",
        "def prepare_2d_slice_data(patient_groups, patient_ids):\n",
        "    \"\"\"从3D体积中提取2D切片，准备训练数据\"\"\"\n",
        "    slice_data_list = []\n",
        "\n",
        "    for patient_id in patient_ids:\n",
        "        if patient_id not in patient_groups:\n",
        "            continue\n",
        "\n",
        "        for seq_id, modalities in patient_groups[patient_id].items():\n",
        "            if 't2f' in modalities and 't1c' in modalities and 'seg' in modalities:\n",
        "                # 加载3D体积\n",
        "                t2f_volume = load_nifti_volume(modalities['t2f'])\n",
        "                t1c_volume = load_nifti_volume(modalities['t1c'])\n",
        "                seg_volume = load_nifti_volume(modalities['seg'])\n",
        "\n",
        "                # 提取切片\n",
        "                t2f_slices = extract_slices_from_volume(t2f_volume, SLICE_START, NUM_SLICES)\n",
        "                t1c_slices = extract_slices_from_volume(t1c_volume, SLICE_START, NUM_SLICES)\n",
        "                seg_slices = extract_slices_from_volume(seg_volume, SLICE_START, NUM_SLICES)\n",
        "\n",
        "                # 为每个切片创建数据项\n",
        "                num_slices = t2f_slices.shape[2]\n",
        "                for slice_idx in range(num_slices):\n",
        "                    slice_data = {\n",
        "                        't2f_slice': t2f_slices[:, :, slice_idx],\n",
        "                        't1c_slice': t1c_slices[:, :, slice_idx],\n",
        "                        'label_slice': seg_slices[:, :, slice_idx],\n",
        "                        'patient_id': patient_id,\n",
        "                        'sequence_id': seq_id,\n",
        "                        'slice_idx': slice_idx\n",
        "                    }\n",
        "                    slice_data_list.append(slice_data)\n",
        "\n",
        "    return slice_data_list\n",
        "\n",
        "# 准备所有切片数据\n",
        "all_slice_data = prepare_2d_slice_data(all_patient_groups, patient_ids)\n",
        "print(f\"总共提取了 {len(all_slice_data)} 个2D切片\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 按患者ID划分数据（避免数据泄露）\n",
        "unique_patient_ids = list(set([item['patient_id'] for item in all_slice_data]))\n",
        "train_patients, temp_patients = train_test_split(\n",
        "    unique_patient_ids, test_size=0.3, random_state=42\n",
        ")\n",
        "val_patients, test_patients = train_test_split(\n",
        "    temp_patients, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# 根据患者ID划分切片数据\n",
        "train_slice_data = [item for item in all_slice_data if item['patient_id'] in train_patients]\n",
        "val_slice_data = [item for item in all_slice_data if item['patient_id'] in val_patients]\n",
        "test_slice_data = [item for item in all_slice_data if item['patient_id'] in test_patients]\n",
        "\n",
        "print(f\"训练集: {len(train_slice_data)} 个切片 ({len(train_patients)} 个患者)\")\n",
        "print(f\"验证集: {len(val_slice_data)} 个切片 ({len(val_patients)} 个患者)\")\n",
        "print(f\"测试集: {len(test_slice_data)} 个切片 ({len(test_patients)} 个患者)\")\n",
        "print(f\"\\n训练患者: {train_patients}\")\n",
        "print(f\"验证患者: {val_patients}\")\n",
        "print(f\"测试患者: {test_patients}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 创建Dataset类（使用Albumentations进行数据增强）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class BrainTumor2DDataset(Dataset):\n",
        "    def __init__(self, slice_data_list, img_size=256, is_train=True):\n",
        "        self.slice_data_list = slice_data_list\n",
        "        self.img_size = img_size\n",
        "        self.is_train = is_train\n",
        "\n",
        "        # 数据增强（训练时）\n",
        "        if is_train:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(img_size, img_size),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.Rotate(limit=15, p=0.5),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "                A.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "            self.label_transform = A.Compose([\n",
        "                A.Resize(img_size, img_size, interpolation=0),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        else:\n",
        "            # 验证/测试时只做resize和归一化\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(img_size, img_size),\n",
        "                A.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "            self.label_transform = A.Compose([\n",
        "                A.Resize(img_size, img_size, interpolation=0),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slice_data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.slice_data_list[idx]\n",
        "\n",
        "        # 获取切片\n",
        "        t2f_slice = normalize_slice(item['t2f_slice'])\n",
        "        t1c_slice = normalize_slice(item['t1c_slice'])\n",
        "        label_slice = remap_labels(item['label_slice'])\n",
        "\n",
        "        # 合并两个模态为2通道图像\n",
        "        image = np.stack([t2f_slice, t1c_slice], axis=0)  # (2, H, W)\n",
        "        image = np.transpose(image, (1, 2, 0))  # (H, W, 2)\n",
        "\n",
        "        # 应用变换\n",
        "        transformed = self.transform(image=image)\n",
        "        image = transformed['image']  # (2, H, W)\n",
        "\n",
        "        # 处理标签\n",
        "        label_transformed = self.label_transform(image=label_slice)\n",
        "        label = label_transformed['image'].squeeze(0).long()  # (H, W)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': label,\n",
        "            'patient_id': item['patient_id'],\n",
        "            'slice_idx': item['slice_idx']\n",
        "        }\n",
        "\n",
        "# 创建数据集\n",
        "train_dataset = BrainTumor2DDataset(train_slice_data, img_size=IMG_SIZE, is_train=True)\n",
        "val_dataset = BrainTumor2DDataset(val_slice_data, img_size=IMG_SIZE, is_train=False)\n",
        "test_dataset = BrainTumor2DDataset(test_slice_data, img_size=IMG_SIZE, is_train=False)\n",
        "\n",
        "# 创建数据加载器\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"训练集大小: {len(train_dataset)}\")\n",
        "print(f\"验证集大小: {len(val_dataset)}\")\n",
        "print(f\"测试集大小: {len(test_dataset)}\")\n",
        "\n",
        "# 测试数据加载\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\n样本形状:\")\n",
        "print(f\"  图像: {sample['image'].shape}\")\n",
        "print(f\"  标签: {sample['label'].shape}\")\n",
        "print(f\"  标签值范围: {sample['label'].min().item()} - {sample['label'].max().item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# 创建UNet++模型（使用预训练编码器）\n",
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name=PRETRAINED_ENCODER,\n",
        "    encoder_weights='imagenet',  # 使用ImageNet预训练权重\n",
        "    in_channels=2,  # 2通道：FLAIR + T1CE\n",
        "    classes=NUM_CLASSES,\n",
        "    activation=None,  # 使用logits，在loss中处理softmax\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# 打印模型信息\n",
        "print(f\"模型类型: UNet++\")\n",
        "print(f\"编码器: {PRETRAINED_ENCODER}\")\n",
        "print(f\"输入通道: 2 (FLAIR + T1CE)\")\n",
        "print(f\"输出类别: {NUM_CLASSES}\")\n",
        "\n",
        "# 计算参数量\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n总参数量: {total_params / 1e6:.2f}M\")\n",
        "print(f\"可训练参数量: {trainable_params / 1e6:.2f}M\")\n",
        "\n",
        "# 测试前向传播\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    test_input = sample_batch['image'].to(device)\n",
        "    test_output = model(test_input)\n",
        "    print(f\"\\n测试输出形状: {test_output.shape}\")\n",
        "    print(f\"期望输出形状: (batch_size, {NUM_CLASSES}, {IMG_SIZE}, {IMG_SIZE})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 定义损失函数和优化器\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 损失函数：Dice Loss + CrossEntropy Loss\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, num_classes=4, smooth=1e-6):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # pred: (B, C, H, W) - logits\n",
        "        # target: (B, H, W) - class indices\n",
        "        pred = torch.softmax(pred, dim=1)\n",
        "\n",
        "        # 转换为one-hot\n",
        "        target_one_hot = torch.zeros_like(pred)\n",
        "        target_one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
        "\n",
        "        # 计算Dice系数（跳过背景类）\n",
        "        dice_scores = []\n",
        "        for c in range(1, self.num_classes):  # 跳过背景\n",
        "            pred_c = pred[:, c]\n",
        "            target_c = target_one_hot[:, c]\n",
        "\n",
        "            intersection = (pred_c * target_c).sum()\n",
        "            union = pred_c.sum() + target_c.sum()\n",
        "\n",
        "            dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
        "            dice_scores.append(dice)\n",
        "\n",
        "        dice_loss = 1.0 - torch.stack(dice_scores).mean()\n",
        "        return dice_loss\n",
        "\n",
        "# 组合损失函数\n",
        "dice_loss = DiceLoss(num_classes=NUM_CLASSES)\n",
        "ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    dice = dice_loss(pred, target)\n",
        "    ce = ce_loss(pred, target)\n",
        "    return 0.5 * dice + 0.5 * ce\n",
        "\n",
        "# 优化器和学习率调度器\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5\n",
        ")\n",
        "\n",
        "print(f\"损失函数: Dice Loss + CrossEntropy Loss\")\n",
        "print(f\"优化器: AdamW (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
        "print(f\"学习率调度器: ReduceLROnPlateau\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 训练和验证函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, loss_function, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"训练\"):\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "def calculate_dice_score(pred, target, num_classes=4, smooth=1e-6):\n",
        "    \"\"\"计算Dice系数\"\"\"\n",
        "    pred = torch.softmax(pred, dim=1)\n",
        "    pred_classes = torch.argmax(pred, dim=1)\n",
        "\n",
        "    dice_scores = []\n",
        "    for c in range(1, num_classes):  # 跳过背景\n",
        "        pred_c = (pred_classes == c).float()\n",
        "        target_c = (target == c).float()\n",
        "\n",
        "        intersection = (pred_c * target_c).sum()\n",
        "        union = pred_c.sum() + target_c.sum()\n",
        "\n",
        "        if union > 0:\n",
        "            dice = (2.0 * intersection + smooth) / (union + smooth)\n",
        "            dice_scores.append(dice.item())\n",
        "\n",
        "    return np.mean(dice_scores) if dice_scores else 0.0\n",
        "\n",
        "def val_epoch(model, loader, loss_function, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_dice_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"验证\"):\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # 计算Dice分数\n",
        "            dice = calculate_dice_score(outputs, labels, NUM_CLASSES)\n",
        "            all_dice_scores.append(dice)\n",
        "\n",
        "    val_loss /= len(loader)\n",
        "    mean_dice = np.mean(all_dice_scores)\n",
        "\n",
        "    return val_loss, mean_dice\n",
        "\n",
        "print(\"训练和验证函数定义完成\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 开始训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练历史\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_dice_scores = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_dice_score = 0.0\n",
        "\n",
        "# 检查是否有检查点\n",
        "checkpoint_path = os.path.join(MODEL_SAVE_PATH, \"checkpoint_latest.pth\")\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"找到检查点: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    train_losses = checkpoint.get('train_losses', [])\n",
        "    val_losses = checkpoint.get('val_losses', [])\n",
        "    val_dice_scores = checkpoint.get('val_dice_scores', [])\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    best_dice_score = checkpoint.get('best_dice_score', 0.0)\n",
        "    print(f\"从Epoch {start_epoch}继续训练\")\n",
        "else:\n",
        "    print(\"未找到检查点，从头开始训练\")\n",
        "\n",
        "print(f\"\\n开始训练，共 {NUM_EPOCHS} 个epoch\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练循环\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # 训练\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, combined_loss, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 验证\n",
        "    if (epoch + 1) % VAL_INTERVAL == 0:\n",
        "        val_loss, mean_dice = val_epoch(model, val_loader, combined_loss, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_dice_scores.append(mean_dice)\n",
        "\n",
        "        print(f\"\\n验证结果:\")\n",
        "        print(f\"  训练Loss: {train_loss:.4f}\")\n",
        "        print(f\"  验证Loss: {val_loss:.4f}\")\n",
        "        print(f\"  Dice系数: {mean_dice:.4f}\")\n",
        "\n",
        "        # 更新学习率\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"  当前学习率: {current_lr:.6f}\")\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_path = os.path.join(MODEL_SAVE_PATH, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'dice_score': mean_dice,\n",
        "            }, best_model_path)\n",
        "            print(f\"  保存最佳模型 (Loss: {val_loss:.4f}, Dice: {mean_dice:.4f})\")\n",
        "\n",
        "        if mean_dice > best_dice_score:\n",
        "            best_dice_score = mean_dice\n",
        "\n",
        "    # 保存检查点\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_dice_scores': val_dice_scores,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_dice_score': best_dice_score,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"训练完成！\")\n",
        "print(f\"最佳验证Loss: {best_val_loss:.4f}\")\n",
        "print(f\"最佳Dice系数: {best_dice_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 可视化训练历史\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制训练历史\n",
        "# 如果变量不存在，从检查点加载\n",
        "if 'train_losses' not in globals() or 'val_losses' not in globals() or 'val_dice_scores' not in globals():\n",
        "    print(\"训练历史变量不存在，尝试从检查点加载...\")\n",
        "    \n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "        train_losses = checkpoint.get('train_losses', [])\n",
        "        val_losses = checkpoint.get('val_losses', [])\n",
        "        val_dice_scores = checkpoint.get('val_dice_scores', [])\n",
        "        print(f\"成功从检查点加载训练历史\")\n",
        "    else:\n",
        "        print(\"检查点文件不存在\")\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_dice_scores = []\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(train_losses, label='Training Loss')\n",
        "axes[0].plot(val_losses, label='Validation Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "if len(val_dice_scores) > 0:\n",
        "    axes[1].plot(val_dice_scores, label='Validation Dice Score')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Dice Score')\n",
        "    axes[1].set_title('Validation Dice Score')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    axes[1].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(MODEL_SAVE_PATH, \"training_history.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 加载最佳模型并在测试集上评估\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载最佳模型\n",
        "best_model_path = os.path.join(MODEL_SAVE_PATH, \"best_model.pth\")\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"加载最佳模型 (Epoch {checkpoint['epoch']}, Dice: {checkpoint['dice_score']:.4f})\")\n",
        "else:\n",
        "    print(\"未找到最佳模型，使用当前模型\")\n",
        "\n",
        "# 在测试集上评估\n",
        "print(\"\\n在测试集上评估...\")\n",
        "test_loss, test_dice = val_epoch(model, test_loader, combined_loss, device)\n",
        "\n",
        "print(f\"\\n测试集结果:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  平均Dice系数: {test_dice:.4f}\")\n",
        "\n",
        "# 保存测试结果\n",
        "test_results = {\n",
        "    'test_loss': test_loss,\n",
        "    'test_dice_mean': test_dice,\n",
        "    'model_type': 'UNet++',\n",
        "    'encoder': PRETRAINED_ENCODER,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "results_path = os.path.join(MODEL_SAVE_PATH, \"test_results.json\")\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(test_results, f, indent=2)\n",
        "print(f\"\\n测试结果已保存到: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 训练结果分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练结果分析\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING RESULTS ANALYSIS REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[1. Training Progress]\")\n",
        "print(f\"   Completed Epochs: {len(train_losses)}/{NUM_EPOCHS}\")\n",
        "print(f\"   Completion: {len(train_losses)/NUM_EPOCHS*100:.1f}%\")\n",
        "\n",
        "print(\"\\n[2. Loss Function Analysis]\")\n",
        "if len(train_losses) > 0:\n",
        "    print(f\"   Training Loss:\")\n",
        "    print(f\"      - Initial: {train_losses[0]:.4f}\")\n",
        "    print(f\"      - Final: {train_losses[-1]:.4f}\")\n",
        "    improvement = train_losses[0] - train_losses[-1]\n",
        "    improvement_pct = (improvement / train_losses[0]) * 100\n",
        "    print(f\"      - Improvement: {improvement:.4f} ({improvement_pct:.1f}%)\")\n",
        "\n",
        "if len(val_losses) > 0:\n",
        "    print(f\"\\n   Validation Loss:\")\n",
        "    print(f\"      - Initial: {val_losses[0]:.4f}\")\n",
        "    print(f\"      - Final: {val_losses[-1]:.4f}\")\n",
        "    print(f\"      - Best: {best_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n[3. Dice Coefficient Analysis]\")\n",
        "if len(val_dice_scores) > 0:\n",
        "    print(f\"   Validation Dice Score:\")\n",
        "    print(f\"      - Initial: {val_dice_scores[0]:.4f}\")\n",
        "    print(f\"      - Final: {val_dice_scores[-1]:.4f}\")\n",
        "    print(f\"      - Best: {best_dice_score:.4f}\")\n",
        "    print(f\"      - Average: {np.mean(val_dice_scores):.4f}\")\n",
        "\n",
        "print(\"\\n[4. Test Set Performance]\")\n",
        "if 'test_results' in locals():\n",
        "    print(f\"   Test Loss: {test_results['test_loss']:.4f}\")\n",
        "    print(f\"   Test Dice Score: {test_results['test_dice_mean']:.4f}\")\n",
        "\n",
        "print(\"\\n[5. Model Configuration]\")\n",
        "print(f\"   Model Type: UNet++\")\n",
        "print(f\"   Encoder: {PRETRAINED_ENCODER}\")\n",
        "print(f\"   Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Analysis Complete!\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
