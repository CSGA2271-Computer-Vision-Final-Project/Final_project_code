{"cells":[{"cell_type":"markdown","metadata":{"id":"ATwr8wIBKGS4"},"source":["# Brain Tumor Segmentation with CMDiff (Conditional Diffusion Model)\n","\n","Implementation of the paper: \"Conditional diffusion model for high-accuracy brain tumor segmentation in MRI images\"\n","\n","**Key Features:**\n","- Conditional diffusion model with DDPM framework\n","- Channel attention mechanism at UNet bottleneck\n","- Fourier filtering preprocessing\n","- 4-class segmentation: Background, Necrotic/Core, Edema, Enhancing\n","- BraTS 2020 dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKj7gIOWKGS6"},"outputs":[],"source":["# Install required packages\n","!pip install kaggle nibabel scipy -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KSEdYvhKGS6","executionInfo":{"status":"ok","timestamp":1764737144686,"user_tz":300,"elapsed":6,"user":{"displayName":"Xiaoyan He","userId":"06623686306774640861"}},"outputId":"352d900d-49b4-48ed-8954-9c0d3205e5a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Kaggle credentials set successfully.\n"]}],"source":["# Setup Kaggle credentials\n","from google.colab import userdata\n","import os\n","\n","kaggle_username = 'xiaoyanhe0713'\n","kaggle_key = 'cfeedab0085bb4e4a3468b7b2c11f006'\n","\n","os.environ['KAGGLE_USERNAME'] = kaggle_username\n","os.environ['KAGGLE_KEY'] = kaggle_key\n","\n","print(\"Kaggle credentials set successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpBaCexbKGS7","executionInfo":{"status":"ok","timestamp":1764737341754,"user_tz":300,"elapsed":197067,"user":{"displayName":"Xiaoyan He","userId":"06623686306774640861"}},"outputId":"8afea331-d76a-4f6a-83e2-3a92077e678b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation\n","License(s): CC0-1.0\n","brats20-dataset-training-validation.zip: Skipping, found more recently modified local copy (use --force to force download)\n"]}],"source":["# Download dataset\n","!kaggle datasets download awsaf49/brats20-dataset-training-validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrxG6rzWKGS8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764737376439,"user_tz":300,"elapsed":34682,"user":{"displayName":"Xiaoyan He","userId":"06623686306774640861"}},"outputId":"9cb9e527-d66a-4efa-90c8-d549f1ae22c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["replace BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_flair.nii? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["# Extract subset of data (first 60 cases for faster training)\n","!unzip -q brats20-dataset-training-validation.zip \"BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_0[0-5][0-9]/*\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P7wNS3d9KGS8","executionInfo":{"status":"ok","timestamp":1764737380282,"user_tz":300,"elapsed":2009,"user":{"displayName":"Xiaoyan He","userId":"06623686306774640861"}},"outputId":"cd2e57a2-071c-41f1-aa75-69b992cf9577"},"outputs":[{"output_type":"stream","name":"stdout","text":["Segment classes:\n","  0: NOT tumor\n","  1: NECROTIC/CORE\n","  2: EDEMA\n","  3: ENHANCING\n","\n","Using device: cuda\n"]}],"source":["# Import libraries\n","import numpy as np\n","import nibabel as nib\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import glob\n","from pathlib import Path\n","import random\n","from scipy import ndimage\n","import csv\n","from collections import defaultdict\n","import math\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Set random seeds for reproducibility\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","# Define BraTS segment classes\n","SEGMENT_CLASSES = {\n","    0: 'NOT tumor',\n","    1: 'NECROTIC/CORE',\n","    2: 'EDEMA',\n","    3: 'ENHANCING'\n","}\n","\n","print(\"Segment classes:\")\n","for k, v in SEGMENT_CLASSES.items():\n","    print(f\"  {k}: {v}\")\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'\\nUsing device: {device}')"]},{"cell_type":"code","source":["TRAIN_DATASET_PATH = \"/content/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\""],"metadata":{"id":"4mzd1P18iy14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select Slices and Image Size\n","VOLUME_SLICES = 100\n","VOLUME_START_AT = 22 # first slice of volume that we will include\n","IMG_SIZE=128"],"metadata":{"id":"wCHqoRPijLAP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eq0Z7qckKGS-"},"source":["## Dataset Class"]},{"cell_type":"code","source":["def fourier_low_pass_filter(image_slice, radius_ratio=0.1):\n","    \"\"\"\n","    Applies Fourier frequency-domain filtering to remove high-frequency noise.\n","    Implements Eq. 7 from the CMDiff paper.\n","    \"\"\"\n","    # 1. Transform to frequency domain\n","    f = np.fft.fft2(image_slice)\n","    fshift = np.fft.fftshift(f)\n","\n","    # 2. Create Circular Low Pass Filter Mask\n","    rows, cols = image_slice.shape\n","    crow, ccol = rows // 2, cols // 2\n","    mask = np.zeros((rows, cols), np.uint8)\n","\n","    r = int(min(rows, cols) * radius_ratio)\n","    y, x = np.ogrid[:rows, :cols]\n","    mask_area = (x - ccol) ** 2 + (y - crow) ** 2 <= r*r\n","    mask[mask_area] = 1\n","\n","    # 3. Apply mask and Inverse FFT\n","    fshift_filtered = fshift * mask\n","    f_ishift = np.fft.ifftshift(fshift_filtered)\n","    img_back = np.fft.ifft2(f_ishift)\n","\n","    return np.abs(img_back)\n","\n","class BrainDataset(Dataset):\n","    def __init__(self, list_IDs, root_dir, dim=(128, 128), use_fourier=True):\n","        self.dim = dim\n","        self.list_IDs = list_IDs\n","        self.root_dir = root_dir\n","        self.use_fourier = use_fourier\n","        # Paper uses 4 modalities: FLAIR, T1, T2, T1CE\n","        self.modalities = ['flair', 't1', 't2', 't1ce']\n","\n","        self.samples = []\n","        for ID in list_IDs:\n","            # Select middle slices (e.g., 60-100) where tumors are most likely to appear\n","            # to speed up this demonstration.\n","            for slice_idx in range(60, 100):\n","                self.samples.append((ID, slice_idx))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        case_id, slice_idx = self.samples[idx]\n","        case_path = os.path.join(self.root_dir, case_id)\n","\n","        channels = []\n","        for mod in self.modalities:\n","            # Construct filename (e.g., BraTS20_Training_001_flair.nii)\n","            file_path = os.path.join(case_path, f\"{case_id}_{mod}.nii\")\n","\n","            # Load NIfTI\n","            img = nib.load(file_path).get_fdata()\n","            slice_img = cv2.resize(img[:, :, slice_idx], self.dim)\n","\n","            # Apply Fourier Filtering if enabled\n","            if self.use_fourier:\n","                slice_img = fourier_low_pass_filter(slice_img)\n","\n","            # Normalize (Z-score normalization per slice)\n","            if np.std(slice_img) > 0:\n","                slice_img = (slice_img - np.mean(slice_img)) / np.std(slice_img)\n","\n","            channels.append(slice_img)\n","\n","        # Stack channels -> (4, H, W)\n","        X = np.stack(channels, axis=0)\n","\n","        # Load Segmentation Mask\n","        seg_path = os.path.join(case_path, f\"{case_id}_seg.nii\")\n","        seg = nib.load(seg_path).get_fdata()\n","        mask_slice = cv2.resize(seg[:, :, slice_idx], self.dim, interpolation=cv2.INTER_NEAREST)\n","\n","        # BraTS Re-mapping: 4 -> 3 (enhancing tumor)\n","        mask_slice[mask_slice == 4] = 3\n","\n","        return torch.FloatTensor(X), torch.LongTensor(mask_slice)"],"metadata":{"id":"jHR-NyqpnOJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZfNZF3jKGS-"},"source":["## Channel Attention Module (from paper)"]},{"cell_type":"code","source":["class DiffusionUtils:\n","    def __init__(self, timesteps=1000):\n","        self.timesteps = timesteps\n","\n","        # Cosine schedule (often preferred for medical images over linear)\n","        self.betas = self.cosine_beta_schedule(timesteps).to(device)\n","        self.alphas = 1. - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n","        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n","        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n","        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n","        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n","        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n","\n","    def cosine_beta_schedule(self, timesteps, s=0.008):\n","        steps = timesteps + 1\n","        x = torch.linspace(0, timesteps, steps)\n","        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n","        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n","        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n","        return torch.clip(betas, 0.0001, 0.9999)\n","\n","    def extract(self, a, t, x_shape):\n","        batch_size = t.shape[0]\n","        out = a.gather(-1, t)\n","        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n","\n","    def q_sample(self, x_start, t, noise=None):\n","        if noise is None:\n","            noise = torch.randn_like(x_start)\n","        sqrt_alphas_cumprod_t = self.extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n","        sqrt_one_minus_alphas_cumprod_t = self.extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n","        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n","\n","    @torch.no_grad()\n","    def p_sample(self, model, x, t, condition, t_index):\n","        # Equation 6: Predict mean\n","        betas_t = self.extract(self.betas, t, x.shape)\n","        sqrt_one_minus_alphas_cumprod_t = self.extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n","        sqrt_recip_alphas_t = self.extract(self.sqrt_recip_alphas, t, x.shape)\n","\n","        # Model predicts noise, conditioned on MRI image\n","        model_output = model(x, t, condition)\n","\n","        model_mean = sqrt_recip_alphas_t * (x - betas_t * model_output / sqrt_one_minus_alphas_cumprod_t)\n","\n","        if t_index == 0:\n","            return model_mean\n","        else:\n","            posterior_variance_t = self.extract(self.posterior_variance, t, x.shape)\n","            noise = torch.randn_like(x)\n","            return model_mean + torch.sqrt(posterior_variance_t) * noise"],"metadata":{"id":"-vbJSEv6nctx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJJJaFUtKGS_"},"outputs":[],"source":["class ChannelAttention(nn.Module):\n","    \"\"\"\n","    Channel Attention Mechanism (Fig. 3 in paper).\n","    Uses Global Average Pooling and Max Pooling to re-weight channels.\n","    \"\"\"\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","\n","        self.fc = nn.Sequential(\n","            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n","            nn.ReLU(),\n","            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc(self.avg_pool(x))\n","        max_out = self.fc(self.max_pool(x))\n","        out = avg_out + max_out\n","        return x * self.sigmoid(out)\n","\n","class CMDiffUNet2D(nn.Module):\n","    def __init__(self, in_channels=4, out_channels=4, base=64):\n","        super().__init__()\n","\n","        # Time embedding MLP\n","        self.time_mlp = nn.Sequential(\n","            nn.Linear(1, base),\n","            nn.GELU(),\n","            nn.Linear(base, base),\n","        )\n","\n","        # --- Condition Encoder (MRI Image) ---\n","        self.cond_enc1 = nn.Sequential(nn.Conv2d(in_channels, base, 3, padding=1), nn.ReLU())\n","        self.cond_enc2 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(base, base*2, 3, padding=1), nn.ReLU())\n","        self.cond_enc3 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(base*2, base*4, 3, padding=1), nn.ReLU())\n","\n","        # --- Noisy Mask Encoder ---\n","        self.mask_enc1 = nn.Sequential(nn.Conv2d(out_channels, base, 3, padding=1), nn.ReLU())\n","        self.mask_enc2 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(base, base*2, 3, padding=1), nn.ReLU())\n","        self.mask_enc3 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(base*2, base*4, 3, padding=1), nn.ReLU())\n","\n","        # --- Bottleneck with Attention ---\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(base*4 * 2, base*8, 3, padding=1), # *2 for concat\n","            nn.ReLU(),\n","            ChannelAttention(base*8), # Paper's Attention Block\n","            nn.Conv2d(base*8, base*4, 3, padding=1),\n","            nn.ReLU()\n","        )\n","\n","        # --- Decoder ---\n","        self.up1 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n","        self.dec1 = nn.Sequential(nn.Conv2d(base*2 * 3, base*2, 3, padding=1), nn.ReLU()) # *3 for skip (mask+cond) + up\n","\n","        self.up2 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n","        self.dec2 = nn.Sequential(nn.Conv2d(base * 3, base, 3, padding=1), nn.ReLU())\n","\n","        self.final = nn.Conv2d(base, out_channels, 1)\n","\n","    def forward(self, x, t, condition):\n","        # 1. Time Embedding\n","        t_emb = self.time_mlp(t.float().view(-1, 1)) # (B, base)\n","        t_emb = t_emb[:, :, None, None].expand(-1, -1, x.shape[2], x.shape[3])\n","\n","        # 2. Encode Condition\n","        c1 = self.cond_enc1(condition)\n","        c2 = self.cond_enc2(c1)\n","        c3 = self.cond_enc3(c2)\n","\n","        # 3. Encode Noisy Mask (inject time into first layer)\n","        m1 = self.mask_enc1(x) + t_emb\n","        m2 = self.mask_enc2(m1)\n","        m3 = self.mask_enc3(m2)\n","\n","        # 4. Bottleneck Fusion\n","        b = torch.cat([m3, c3], dim=1)\n","        b = self.bottleneck(b)\n","\n","        # 5. Decoder\n","        d1 = self.up1(b)\n","        # Skip connections from both Mask encoder and Condition encoder\n","        d1 = torch.cat([d1, m2, c2], dim=1)\n","        d1 = self.dec1(d1)\n","\n","        d2 = self.up2(d1)\n","        d2 = torch.cat([d2, m1, c1], dim=1)\n","        d2 = self.dec2(d2)\n","\n","        return self.final(d2)"]},{"cell_type":"markdown","metadata":{"id":"cKIuKRg6KGTA"},"source":["## Loss Functions and Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebZYnfV2KGTB","executionInfo":{"status":"ok","timestamp":1764738912668,"user_tz":300,"elapsed":41,"user":{"displayName":"Xiaoyan He","userId":"06623686306774640861"}},"outputId":"b150cb83-6d07-4fd3-b7d6-989b3fd5daa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Metrics defined!\n"]}],"source":["def dice_coefficient(pred, target, num_classes=4, smooth=1e-6):\n","    \"\"\"\n","    Calculate Dice coefficient for multi-class segmentation.\n","    \"\"\"\n","    dice_scores = []\n","\n","    for c in range(num_classes):\n","        pred_c = (pred == c).float()\n","        target_c = (target == c).float()\n","\n","        intersection = (pred_c * target_c).sum()\n","        union = pred_c.sum() + target_c.sum()\n","\n","        dice = (2.0 * intersection + smooth) / (union + smooth)\n","        dice_scores.append(dice.item())\n","\n","    return np.mean(dice_scores[1:])  # Exclude background\n","\n","def iou_score(pred, target, num_classes=4, smooth=1e-6):\n","    \"\"\"\n","    Calculate IoU (Intersection over Union) for multi-class segmentation.\n","    \"\"\"\n","    iou_scores = []\n","\n","    for c in range(num_classes):\n","        pred_c = (pred == c).float()\n","        target_c = (target == c).float()\n","\n","        intersection = (pred_c * target_c).sum()\n","        union = pred_c.sum() + target_c.sum() - intersection\n","\n","        iou = (intersection + smooth) / (union + smooth)\n","        iou_scores.append(iou.item())\n","\n","    return np.mean(iou_scores[1:])  # Exclude background\n","\n","print(\"Metrics defined!\")"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","def foreground_weighted_mse(\n","    pred_noise: torch.Tensor,\n","    target_noise: torch.Tensor,\n","    masks: torch.Tensor,\n","    fg_weight: float = 40.0,\n","    bg_weight: float = 1.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Weighted MSE loss in noise space.\n","\n","    pred_noise:  (B, C, H, W) - model output\n","    target_noise:(B, C, H, W) - true noise used in q_sample\n","    masks:       (B, H, W)    - GT labels (0 = background, >0 = foreground)\n","    fg_weight:   multiplicative weight for foreground pixels\n","    bg_weight:   multiplicative weight for background pixels\n","    \"\"\"\n","\n","    # Per-pixel binary mask: 1 for foreground, 0 for background\n","    with torch.no_grad():\n","        fg_mask = (masks != 0).float()           # (B, H, W)\n","        fg_mask = fg_mask.unsqueeze(1)          # (B, 1, H, W)\n","        fg_mask = fg_mask.expand_as(pred_noise) # (B, C, H, W)\n","\n","        # Combine foreground/background weights\n","        weights = fg_mask * fg_weight + (1.0 - fg_mask) * bg_weight  # (B, C, H, W)\n","\n","    mse = (pred_noise - target_noise) ** 2\n","\n","    # Normalize by sum of weights so the overall scale is stable\n","    weighted_loss = (weights * mse).sum() / weights.sum()\n","\n","    return weighted_loss\n"],"metadata":{"id":"B-GBkVwGx-Pt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56qSCY1FKGTB"},"source":["## Training Functions"]},{"cell_type":"code","source":["import cv2\n","# --- Configuration ---\n","# UPDATE THIS PATH to your actual data location\n","ROOT_DIR = \"/content/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n","\n","# Generate dummy list of IDs for demonstration\n","# (In real usage, list actual directory names, e.g. \"BraTS20_Training_001\")\n","train_ids = [f\"BraTS20_Training_{i:03d}\" for i in range(1, 31)]\n","# Assuming you already have `train_ids` (a list of case IDs)\n","val_fraction = 0.2\n","num_val = int(len(train_ids) * val_fraction)\n","val_ids = train_ids[-num_val:]\n","train_ids = train_ids[:-num_val]\n","\n","train_dataset = BrainDataset(train_ids, ROOT_DIR, use_fourier=True)\n","val_dataset   = BrainDataset(val_ids, ROOT_DIR, use_fourier=True)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n","val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False, num_workers=2)\n","\n","diffuser = DiffusionUtils(timesteps=1000)\n","\n","model = CMDiffUNet2D().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.MSELoss()"],"metadata":{"id":"31Duo2Tuo1eQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch.nn.functional as F\n","\n","def validate(model, diffuser, val_loader, device):\n","    model.eval()\n","    criterion = nn.MSELoss()\n","\n","    all_losses = []\n","    all_dices = []\n","    all_ious = []\n","\n","    with torch.no_grad():\n","        for images, masks in val_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)  # (B, H, W)\n","\n","            # One-hot encode masks, scale to [-1, 1] (same as training)\n","            masks_onehot = F.one_hot(masks.long(), num_classes=4).permute(0, 3, 1, 2).float()\n","            masks_scaled = masks_onehot * 2 - 1  # (B, 4, H, W)\n","\n","            # Random t and noise as in training\n","            t = torch.randint(0, diffuser.timesteps, (images.shape[0],), device=device).long()\n","            noise = torch.randn_like(masks_scaled)\n","            x_t = diffuser.q_sample(masks_scaled, t, noise)\n","\n","            # Predict noise and compute validation loss\n","            pred_noise = model(x_t, t, condition=images)\n","            loss = criterion(pred_noise, noise)\n","            all_losses.append(loss.item())\n","\n","            # --- Reconstruct x0 estimate and compute segmentation metrics ---\n","            sqrt_alpha_t = diffuser.extract(diffuser.sqrt_alphas_cumprod, t, x_t.shape)\n","            sqrt_one_minus_alpha_t = diffuser.extract(diffuser.sqrt_one_minus_alphas_cumprod, t, x_t.shape)\n","\n","            # x_t = sqrt(alpha_t) * x0 + sqrt(1 - alpha_t) * noise\n","            # => x0 ≈ (x_t - sqrt(1 - alpha_t) * eps_theta) / sqrt(alpha_t)\n","            x0_pred = (x_t - sqrt_one_minus_alpha_t * pred_noise) / (sqrt_alpha_t + 1e-8)\n","\n","            # Get discrete predicted labels by argmax over channels\n","            preds = torch.argmax(x0_pred, dim=1)  # (B, H, W)\n","\n","            # Accumulate Dice & IoU (exclude background as your functions do)\n","            for i in range(preds.shape[0]):\n","                d = dice_coefficient(preds[i], masks[i])\n","                j = iou_score(preds[i], masks[i])\n","                all_dices.append(d)\n","                all_ious.append(j)\n","\n","    avg_loss = float(np.mean(all_losses)) if all_losses else 0.0\n","    avg_dice = float(np.mean(all_dices)) if all_dices else 0.0\n","    avg_iou  = float(np.mean(all_ious))  if all_ious  else 0.0\n","\n","    return avg_loss, avg_dice, avg_iou"],"metadata":{"id":"l4b46801xbf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 50\n","\n","for epoch in range(epochs):\n","    model.train()\n","    train_losses = []\n","\n","    for images, masks in train_loader:\n","        images = images.to(device)\n","        masks = masks.to(device)  # (B, H, W)\n","\n","        # One-hot encode masks and scale to [-1, 1]\n","        masks_onehot = F.one_hot(masks.long(), num_classes=4).permute(0, 3, 1, 2).float()\n","        masks_scaled = masks_onehot * 2 - 1\n","\n","        # Sample t and noise\n","        t = torch.randint(0, diffuser.timesteps, (images.shape[0],), device=device).long()\n","        noise = torch.randn_like(masks_scaled)\n","        x_t = diffuser.q_sample(masks_scaled, t, noise)\n","\n","        # Predict noise\n","        predicted_noise = model(x_t, t, condition=images)\n","\n","        loss = foreground_weighted_mse(\n","            pred_noise=predicted_noise,\n","            target_noise=noise,\n","            masks=masks,          # (B, H, W) GT labels\n","            fg_weight=4.0,        # try 3–10, tune later\n","            bg_weight=1.0\n","        )\n","\n","        train_losses.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_train_loss = np.mean(train_losses)\n","    val_loss, val_dice, val_iou = validate(model, diffuser, val_loader, device)\n","\n","    print(\n","        f\"Epoch {epoch+1}/{epochs} | \"\n","        f\"Train Loss: {avg_train_loss:.4f} | \"\n","        f\"Val Loss: {val_loss:.4f} | \"\n","        f\"Val Dice: {val_dice:.4f} | \"\n","        f\"Val IoU: {val_iou:.4f}\"\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvgWJEuZxsjw","outputId":"00281f52-ebea-4c6c-8350-f9761272a09d","executionInfo":{"status":"ok","timestamp":1764754264899,"user_tz":300,"elapsed":3206625,"user":{"displayName":"Xiaoyan He","userId":"06623686306774640861"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50 | Train Loss: 22.8748 | Val Loss: 1.0518 | Val Dice: 0.3306 | Val IoU: 0.3014\n","Epoch 2/50 | Train Loss: 1.0060 | Val Loss: 0.9910 | Val Dice: 0.2872 | Val IoU: 0.2568\n","Epoch 3/50 | Train Loss: 0.9902 | Val Loss: 0.9845 | Val Dice: 0.3280 | Val IoU: 0.2997\n","Epoch 4/50 | Train Loss: 0.9814 | Val Loss: 0.9767 | Val Dice: 0.3754 | Val IoU: 0.3489\n","Epoch 5/50 | Train Loss: 0.9725 | Val Loss: 0.9663 | Val Dice: 0.3235 | Val IoU: 0.2967\n","Epoch 6/50 | Train Loss: 1.0105 | Val Loss: 0.9621 | Val Dice: 0.3263 | Val IoU: 0.3015\n","Epoch 7/50 | Train Loss: 0.9572 | Val Loss: 0.9522 | Val Dice: 0.3578 | Val IoU: 0.3341\n","Epoch 8/50 | Train Loss: 0.9469 | Val Loss: 0.9433 | Val Dice: 0.3602 | Val IoU: 0.3348\n","Epoch 9/50 | Train Loss: 0.9319 | Val Loss: 0.9240 | Val Dice: 0.3553 | Val IoU: 0.3282\n","Epoch 10/50 | Train Loss: 0.9189 | Val Loss: 0.9067 | Val Dice: 0.3569 | Val IoU: 0.3303\n","Epoch 11/50 | Train Loss: 0.8985 | Val Loss: 0.8864 | Val Dice: 0.3612 | Val IoU: 0.3323\n","Epoch 12/50 | Train Loss: 1.0423 | Val Loss: 0.8732 | Val Dice: 0.4002 | Val IoU: 0.3727\n","Epoch 13/50 | Train Loss: 0.9626 | Val Loss: 0.8441 | Val Dice: 0.3166 | Val IoU: 0.2853\n","Epoch 14/50 | Train Loss: 0.8395 | Val Loss: 0.8386 | Val Dice: 0.4153 | Val IoU: 0.3869\n","Epoch 15/50 | Train Loss: 0.8184 | Val Loss: 0.8121 | Val Dice: 0.3805 | Val IoU: 0.3516\n","Epoch 16/50 | Train Loss: 0.8042 | Val Loss: 0.7823 | Val Dice: 0.3271 | Val IoU: 0.2981\n","Epoch 17/50 | Train Loss: 0.7870 | Val Loss: 0.7617 | Val Dice: 0.3249 | Val IoU: 0.2961\n","Epoch 18/50 | Train Loss: 0.7715 | Val Loss: 0.7497 | Val Dice: 0.3856 | Val IoU: 0.3536\n","Epoch 19/50 | Train Loss: 0.8016 | Val Loss: 0.7330 | Val Dice: 0.3677 | Val IoU: 0.3374\n","Epoch 20/50 | Train Loss: 0.7364 | Val Loss: 0.7039 | Val Dice: 0.3474 | Val IoU: 0.3195\n","Epoch 21/50 | Train Loss: 1.3359 | Val Loss: 0.7403 | Val Dice: 0.3549 | Val IoU: 0.3256\n","Epoch 22/50 | Train Loss: 0.7180 | Val Loss: 0.7116 | Val Dice: 0.4157 | Val IoU: 0.3884\n","Epoch 23/50 | Train Loss: 0.7020 | Val Loss: 0.6953 | Val Dice: 0.4346 | Val IoU: 0.4038\n","Epoch 24/50 | Train Loss: 0.6801 | Val Loss: 0.6622 | Val Dice: 0.3720 | Val IoU: 0.3445\n","Epoch 25/50 | Train Loss: 0.6663 | Val Loss: 0.6865 | Val Dice: 0.3112 | Val IoU: 0.2769\n","Epoch 26/50 | Train Loss: 0.7783 | Val Loss: 0.6406 | Val Dice: 0.4372 | Val IoU: 0.4084\n","Epoch 27/50 | Train Loss: 0.6325 | Val Loss: 0.6036 | Val Dice: 0.3550 | Val IoU: 0.3222\n","Epoch 28/50 | Train Loss: 0.7511 | Val Loss: 0.6714 | Val Dice: 0.3843 | Val IoU: 0.3609\n","Epoch 29/50 | Train Loss: 0.6062 | Val Loss: 0.5804 | Val Dice: 0.3966 | Val IoU: 0.3641\n","Epoch 30/50 | Train Loss: 0.5834 | Val Loss: 0.5314 | Val Dice: 0.3655 | Val IoU: 0.3343\n","Epoch 31/50 | Train Loss: 0.5752 | Val Loss: 0.5444 | Val Dice: 0.4220 | Val IoU: 0.3933\n","Epoch 32/50 | Train Loss: 0.5355 | Val Loss: 0.5441 | Val Dice: 0.4730 | Val IoU: 0.4425\n","Epoch 33/50 | Train Loss: 1.0039 | Val Loss: 0.6809 | Val Dice: 0.4424 | Val IoU: 0.4096\n","Epoch 34/50 | Train Loss: 0.5611 | Val Loss: 0.5114 | Val Dice: 0.4688 | Val IoU: 0.4404\n","Epoch 35/50 | Train Loss: 0.4994 | Val Loss: 0.4978 | Val Dice: 0.4786 | Val IoU: 0.4482\n","Epoch 36/50 | Train Loss: 0.4790 | Val Loss: 0.4667 | Val Dice: 0.4857 | Val IoU: 0.4579\n","Epoch 37/50 | Train Loss: 0.4674 | Val Loss: 0.4562 | Val Dice: 0.4453 | Val IoU: 0.4192\n","Epoch 38/50 | Train Loss: 0.5184 | Val Loss: 0.7735 | Val Dice: 0.3719 | Val IoU: 0.3439\n","Epoch 39/50 | Train Loss: 0.7872 | Val Loss: 0.3956 | Val Dice: 0.4603 | Val IoU: 0.4297\n","Epoch 40/50 | Train Loss: 0.4226 | Val Loss: 0.4110 | Val Dice: 0.4988 | Val IoU: 0.4695\n","Epoch 41/50 | Train Loss: 0.4053 | Val Loss: 0.4025 | Val Dice: 0.5124 | Val IoU: 0.4840\n","Epoch 42/50 | Train Loss: 0.4844 | Val Loss: 0.8323 | Val Dice: 0.4600 | Val IoU: 0.4334\n","Epoch 43/50 | Train Loss: 0.4806 | Val Loss: 0.3652 | Val Dice: 0.4520 | Val IoU: 0.4209\n","Epoch 44/50 | Train Loss: 0.3780 | Val Loss: 0.3543 | Val Dice: 0.5431 | Val IoU: 0.5142\n","Epoch 45/50 | Train Loss: 0.3660 | Val Loss: 0.3301 | Val Dice: 0.5213 | Val IoU: 0.4924\n","Epoch 46/50 | Train Loss: 0.3479 | Val Loss: 0.3378 | Val Dice: 0.5570 | Val IoU: 0.5281\n","Epoch 47/50 | Train Loss: 0.3384 | Val Loss: 0.3890 | Val Dice: 0.4846 | Val IoU: 0.4588\n","Epoch 48/50 | Train Loss: 0.3341 | Val Loss: 0.3956 | Val Dice: 0.4913 | Val IoU: 0.4658\n","Epoch 49/50 | Train Loss: 0.3535 | Val Loss: 0.3173 | Val Dice: 0.5148 | Val IoU: 0.4886\n","Epoch 50/50 | Train Loss: 0.4106 | Val Loss: 0.2757 | Val Dice: 0.5574 | Val IoU: 0.5266\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"1S9p4IAEFbJrG0QGS0XJi8HBiCUAP5VZR","timestamp":1764718877808}],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}