{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer-based 脑肿瘤分割 - 训练Notebook\n",
        "\n",
        "本notebook用于训练基于Transformer的医学图像分割模型.\n",
        "\n",
        "## 支持的模型:\n",
        "1. SwinUNet - 基于Swin Transformer的UNet\n",
        "2. Swin-UNETR - MONAI的Swin-UNETR (3D)\n",
        "3. UNETR - MONAI的UNETR (3D)\n",
        "\n",
        "## 功能:\n",
        "1. 从3D体积中提取数据\n",
        "2. 加载和训练Transformer-based模型\n",
        "3. 模型评估和可视化\n",
        "4. 与之前的模型进行对比\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 安装依赖和挂载Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 挂载Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的包\n",
        "%pip install monai[all] -q\n",
        "%pip install nibabel -q\n",
        "%pip install timm -q  # Swin Transformer\n",
        "%pip install einops -q\n",
        "%pip install matplotlib -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 导入库和配置参数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# MONAI导入\n",
        "import monai\n",
        "from monai.networks.nets import UNETR, SwinUNETR\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Spacingd, Orientationd,\n",
        "    ScaleIntensityRanged, CropForegroundd, Resized, ToTensord,\n",
        "    Compose, RandRotate90d, RandFlipd, RandShiftIntensityd, MapTransform\n",
        ")\n",
        "from monai.data import Dataset as MonaiDataset, DataLoader as MonaiDataLoader\n",
        "from monai.utils import set_determinism\n",
        "from monai.inferers import sliding_window_inference\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用设备: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"MONAI version: {monai.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据路径配置\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/data-brain-2024\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/brain-tumor-models-transformer\"\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# 训练参数\n",
        "IMG_SIZE = 128  # 3D图像尺寸 (可以根据GPU内存调整)\n",
        "BATCH_SIZE = 2  # 3D数据需要较小的batch size\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 50\n",
        "VAL_INTERVAL = 1\n",
        "NUM_CLASSES = 4  # 背景 + 3个肿瘤类别\n",
        "\n",
        "# 模型选择: 'unetr', 'swin_unetr', 'swinunet'\n",
        "MODEL_TYPE = 'unetr'  # 可以修改为 'swin_unetr' 或 'swinunet'\n",
        "\n",
        "# 设置随机种子\n",
        "set_determinism(seed=42)\n",
        "\n",
        "print(f\"数据路径: {DRIVE_DATA_PATH}\")\n",
        "print(f\"模型保存路径: {MODEL_SAVE_PATH}\")\n",
        "print(f\"图像尺寸: {IMG_SIZE}x{IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"学习率: {LEARNING_RATE}\")\n",
        "print(f\"模型类型: {MODEL_TYPE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 数据加载和预处理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 自定义变换：合并多模态图像\n",
        "class ConcatModalitiesd(MapTransform):\n",
        "    \"\"\"将多模态图像列表合并为多通道图像\"\"\"\n",
        "    def __init__(self, keys, allow_missing_keys=False):\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "\n",
        "    def __call__(self, data):\n",
        "        d = dict(data)\n",
        "        for key in self.key_iterator(d):\n",
        "            if isinstance(d[key], list):\n",
        "                d[key] = np.concatenate(d[key], axis=0)\n",
        "        return d\n",
        "\n",
        "class RemapLabeld(MapTransform):\n",
        "    \"\"\"将标签中的值4映射到3\"\"\"\n",
        "    def __init__(self, keys, allow_missing_keys=False):\n",
        "        super().__init__(keys, allow_missing_keys)\n",
        "\n",
        "    def __call__(self, data):\n",
        "        d = dict(data)\n",
        "        for key in self.key_iterator(d):\n",
        "            if isinstance(d[key], torch.Tensor):\n",
        "                d[key] = torch.where(d[key] == 4, torch.tensor(3, dtype=d[key].dtype, device=d[key].device), d[key])\n",
        "            elif isinstance(d[key], np.ndarray):\n",
        "                d[key] = np.where(d[key] == 4, 3, d[key])\n",
        "        return d\n",
        "\n",
        "def get_patient_groups(data_path):\n",
        "    \"\"\"获取所有患者的数据分组\"\"\"\n",
        "    all_files = glob.glob(os.path.join(data_path, \"*.nii\"))\n",
        "    patient_groups = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    for file_path in all_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        match = re.match(r'BraTS-GLI-(\\d+)-(\\d+)-(t1n|t2f|t2w|t1c|seg)\\.nii', filename)\n",
        "        if match:\n",
        "            patient_id = match.group(1)\n",
        "            sequence_id = match.group(2)\n",
        "            modality = match.group(3)\n",
        "            patient_groups[patient_id][sequence_id][modality] = file_path\n",
        "\n",
        "    complete_patients = {}\n",
        "    for patient_id, sequences in patient_groups.items():\n",
        "        for seq_id, modalities in sequences.items():\n",
        "            if 't2f' in modalities and 't1c' in modalities and 'seg' in modalities:\n",
        "                if patient_id not in complete_patients:\n",
        "                    complete_patients[patient_id] = {}\n",
        "                complete_patients[patient_id][seq_id] = modalities\n",
        "\n",
        "    return complete_patients\n",
        "\n",
        "def prepare_monai_data_list(patient_groups, patient_ids):\n",
        "    \"\"\"准备MONAI格式的数据字典列表\"\"\"\n",
        "    data_list = []\n",
        "\n",
        "    for patient_id in patient_ids:\n",
        "        if patient_id not in patient_groups:\n",
        "            continue\n",
        "\n",
        "        for seq_id, modalities in patient_groups[patient_id].items():\n",
        "            if 't2f' in modalities and 't1c' in modalities and 'seg' in modalities:\n",
        "                data_dict = {\n",
        "                    \"image\": [modalities['t2f'], modalities['t1c']],\n",
        "                    \"label\": modalities['seg'],\n",
        "                    \"patient_id\": patient_id,\n",
        "                    \"sequence_id\": seq_id\n",
        "                }\n",
        "                data_list.append(data_dict)\n",
        "\n",
        "    return data_list\n",
        "\n",
        "# 获取所有患者数据\n",
        "all_patient_groups = get_patient_groups(DRIVE_DATA_PATH)\n",
        "patient_ids = list(all_patient_groups.keys())\n",
        "\n",
        "print(f\"找到 {len(patient_ids)} 个患者\")\n",
        "print(f\"前5个患者ID: {patient_ids[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据划分（按患者ID，避免数据泄露）\n",
        "all_data_list = prepare_monai_data_list(all_patient_groups, patient_ids)\n",
        "\n",
        "unique_patient_ids = list(set([item['patient_id'] for item in all_data_list]))\n",
        "train_patients, temp_patients = train_test_split(\n",
        "    unique_patient_ids, test_size=0.3, random_state=42\n",
        ")\n",
        "val_patients, test_patients = train_test_split(\n",
        "    temp_patients, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# 根据患者ID划分数据\n",
        "train_data_list = [item for item in all_data_list if item['patient_id'] in train_patients]\n",
        "val_data_list = [item for item in all_data_list if item['patient_id'] in val_patients]\n",
        "test_data_list = [item for item in all_data_list if item['patient_id'] in test_patients]\n",
        "\n",
        "print(f\"训练集: {len(train_data_list)} 个样本 ({len(train_patients)} 个患者)\")\n",
        "print(f\"验证集: {len(val_data_list)} 个样本 ({len(val_patients)} 个患者)\")\n",
        "print(f\"测试集: {len(test_data_list)} 个样本 ({len(test_patients)} 个患者)\")\n",
        "print(f\"\\n训练患者示例: {train_patients[:5]}\")\n",
        "print(f\"验证患者示例: {val_patients[:3]}\")\n",
        "print(f\"测试患者示例: {test_patients[:3]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练时的数据变换\n",
        "train_transforms = Compose([\n",
        "    LoadImaged(keys=[\"image\", \"label\"], image_only=False),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    ConcatModalitiesd(keys=[\"image\"]),\n",
        "    RemapLabeld(keys=[\"label\"]),\n",
        "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ScaleIntensityRanged(\n",
        "        keys=[\"image\"],\n",
        "        a_min=0.0,\n",
        "        a_max=1000.0,\n",
        "        b_min=0.0,\n",
        "        b_max=1.0,\n",
        "        clip=True,\n",
        "    ),\n",
        "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "    Resized(keys=[\"image\", \"label\"], spatial_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE), mode=(\"trilinear\", \"nearest\")),\n",
        "    # 数据增强\n",
        "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=(0, 1)),\n",
        "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5),\n",
        "    RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n",
        "    ToTensord(keys=[\"image\", \"label\"]),\n",
        "])\n",
        "\n",
        "# 验证/测试时的数据变换（无数据增强）\n",
        "val_transforms = Compose([\n",
        "    LoadImaged(keys=[\"image\", \"label\"], image_only=False),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    ConcatModalitiesd(keys=[\"image\"]),\n",
        "    RemapLabeld(keys=[\"label\"]),\n",
        "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ScaleIntensityRanged(\n",
        "        keys=[\"image\"],\n",
        "        a_min=0.0,\n",
        "        a_max=1000.0,\n",
        "        b_min=0.0,\n",
        "        b_max=1.0,\n",
        "        clip=True,\n",
        "    ),\n",
        "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "    Resized(keys=[\"image\", \"label\"], spatial_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE), mode=(\"trilinear\", \"nearest\")),\n",
        "    ToTensord(keys=[\"image\", \"label\"]),\n",
        "])\n",
        "\n",
        "print(\"数据变换定义完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建数据集和数据加载器\n",
        "train_dataset = MonaiDataset(data=train_data_list, transform=train_transforms)\n",
        "val_dataset = MonaiDataset(data=val_data_list, transform=val_transforms)\n",
        "test_dataset = MonaiDataset(data=test_data_list, transform=val_transforms)\n",
        "\n",
        "train_loader = MonaiDataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "val_loader = MonaiDataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "test_loader = MonaiDataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "print(f\"训练集大小: {len(train_dataset)}\")\n",
        "print(f\"验证集大小: {len(val_dataset)}\")\n",
        "print(f\"测试集大小: {len(test_dataset)}\")\n",
        "print(f\"训练批次数: {len(train_loader)}\")\n",
        "print(f\"验证批次数: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 创建模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建模型\n",
        "in_channels = 2  # FLAIR + T1CE\n",
        "out_channels = NUM_CLASSES\n",
        "\n",
        "if MODEL_TYPE == 'unetr':\n",
        "    model = UNETR(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        img_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE),\n",
        "        feature_size=16,\n",
        "        hidden_size=768,\n",
        "        mlp_dim=3072,\n",
        "        num_heads=12,\n",
        "        norm_name=\"instance\",\n",
        "        res_block=True,\n",
        "        dropout_rate=0.0,\n",
        "    )\n",
        "    print(\"创建UNETR模型\")\n",
        "elif MODEL_TYPE == 'swin_unetr':\n",
        "    model = SwinUNETR(\n",
        "        img_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE),\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        feature_size=48,\n",
        "        use_checkpoint=False,\n",
        "    )\n",
        "    print(\"创建Swin-UNETR模型\")\n",
        "elif MODEL_TYPE == 'swinunet':\n",
        "    # SwinUNet需要单独实现或使用其他库\n",
        "    # 这里提供一个简单的实现示例\n",
        "    print(\"SwinUNet需要单独实现，暂时使用UNETR代替\")\n",
        "    model = UNETR(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        img_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE),\n",
        "        feature_size=16,\n",
        "        hidden_size=768,\n",
        "        mlp_dim=3072,\n",
        "        num_heads=12,\n",
        "        norm_name=\"instance\",\n",
        "        res_block=True,\n",
        "        dropout_rate=0.0,\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"不支持的模型类型: {MODEL_TYPE}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# 计算模型参数数量\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"模型创建完成\")\n",
        "print(f\"总参数数量: {total_params:,}\")\n",
        "print(f\"可训练参数: {trainable_params:,}\")\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "loss_function = DiceCELoss(include_background=False, to_onehot_y=True, softmax=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# 定义评估指标\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "\n",
        "print(\"\\n损失函数: Dice + CrossEntropy Loss\")\n",
        "print(f\"优化器: AdamW (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
        "print(f\"学习率调度器: ReduceLROnPlateau\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 训练和验证函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练一个epoch\n",
        "def train_epoch(model, loader, optimizer, loss_function, device, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "\n",
        "    for batch_data in tqdm(loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"  Step {step}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss /= len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "# 验证函数\n",
        "def val_epoch(model, loader, loss_function, dice_metric, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    dice_metric.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(loader, desc=\"Validation\"):\n",
        "            inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "\n",
        "            # 使用滑动窗口推理（适合大图像）\n",
        "            outputs = sliding_window_inference(\n",
        "                inputs=inputs,\n",
        "                roi_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE),\n",
        "                sw_batch_size=1,\n",
        "                predictor=model,\n",
        "                overlap=0.5,\n",
        "                mode=\"gaussian\",\n",
        "            )\n",
        "\n",
        "            loss = loss_function(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # 计算指标\n",
        "            pred_one_hot = torch.softmax(outputs, dim=1)\n",
        "            labels_one_hot = torch.zeros_like(pred_one_hot)\n",
        "            labels_one_hot.scatter_(1, labels.long(), 1)\n",
        "\n",
        "            dice_metric(y_pred=pred_one_hot, y=labels_one_hot)\n",
        "\n",
        "    val_loss /= len(loader)\n",
        "    dice_scores = dice_metric.aggregate()\n",
        "\n",
        "    return val_loss, dice_scores\n",
        "\n",
        "print(\"训练和验证函数定义完成\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 开始训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练历史\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_dice_scores = []\n",
        "best_val_loss = float('inf')\n",
        "best_dice_score = 0.0\n",
        "\n",
        "# 检查是否有检查点\n",
        "checkpoint_path = os.path.join(MODEL_SAVE_PATH, f\"checkpoint_latest_{MODEL_TYPE}.pth\")\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"找到检查点: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    train_losses = checkpoint.get('train_losses', [])\n",
        "    val_losses = checkpoint.get('val_losses', [])\n",
        "    val_dice_scores = checkpoint.get('val_dice_scores', [])\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    best_dice_score = checkpoint.get('best_dice_score', 0.0)\n",
        "    print(f\"从epoch {start_epoch} 恢复训练\")\n",
        "else:\n",
        "    print(\"未找到检查点，从头开始训练\")\n",
        "\n",
        "print(f\"\\n开始训练，共 {NUM_EPOCHS} 个epoch\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练循环\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # 训练\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, loss_function, device, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 验证\n",
        "    if (epoch + 1) % VAL_INTERVAL == 0:\n",
        "        val_loss, dice_scores = val_epoch(model, val_loader, loss_function, dice_metric, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_dice_scores.append(dice_scores.mean().item())\n",
        "\n",
        "        mean_dice = dice_scores.mean().item()\n",
        "\n",
        "        print(f\"\\n验证结果:\")\n",
        "        print(f\"  Loss: {val_loss:.4f}\")\n",
        "        print(f\"  Dice系数: {mean_dice:.4f}\")\n",
        "        print(f\"  Dice (各类别): {dice_scores.cpu().numpy()}\")\n",
        "\n",
        "        # 更新学习率\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"  当前学习率: {current_lr:.6f}\")\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_path = os.path.join(MODEL_SAVE_PATH, f\"best_model_{MODEL_TYPE}.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'dice_score': mean_dice,\n",
        "            }, best_model_path)\n",
        "            print(f\"  保存最佳模型 (Loss: {val_loss:.4f}, Dice: {mean_dice:.4f})\")\n",
        "\n",
        "        if mean_dice > best_dice_score:\n",
        "            best_dice_score = mean_dice\n",
        "\n",
        "    # 保存检查点\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_dice_scores': val_dice_scores,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_dice_score': best_dice_score,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    print(f\"训练Loss: {train_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"训练完成！\")\n",
        "print(f\"最佳验证Loss: {best_val_loss:.4f}\")\n",
        "print(f\"最佳Dice系数: {best_dice_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 可视化训练历史\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制训练曲线\n",
        "def plot_training_history(train_losses, val_losses, val_dice_scores, save_path=None):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # 损失曲线\n",
        "    axes[0].plot(train_losses, label='Training Loss', color='blue')\n",
        "    if val_losses:\n",
        "        val_epochs = [i * VAL_INTERVAL for i in range(len(val_losses))]\n",
        "        axes[0].plot(val_epochs, val_losses, label='Validation Loss', color='red', marker='o')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Dice系数曲线\n",
        "    if val_dice_scores:\n",
        "        val_epochs = [i * VAL_INTERVAL for i in range(len(val_dice_scores))]\n",
        "        axes[1].plot(val_epochs, val_dice_scores, label='Validation Dice Score', color='green', marker='o')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Dice Score')\n",
        "        axes[1].set_title('Validation Dice Score')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "        axes[1].set_ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(train_losses, val_losses, val_dice_scores,\n",
        "                     save_path=os.path.join(MODEL_SAVE_PATH, f\"training_history_{MODEL_TYPE}.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 加载最佳模型并在测试集上评估\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载最佳模型\n",
        "best_model_path = os.path.join(MODEL_SAVE_PATH, f\"best_model_{MODEL_TYPE}.pth\")\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"加载最佳模型 (Epoch {checkpoint['epoch']}, Dice: {checkpoint['dice_score']:.4f})\")\n",
        "else:\n",
        "    print(\"未找到最佳模型，使用当前模型\")\n",
        "\n",
        "# 在测试集上评估\n",
        "print(\"\\n在测试集上评估...\")\n",
        "test_loss, test_dice_scores = val_epoch(model, test_loader, loss_function, dice_metric, device)\n",
        "\n",
        "print(f\"\\n测试集结果:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  平均Dice系数: {test_dice_scores.mean().item():.4f}\")\n",
        "print(f\"  Dice系数 (各类别): {test_dice_scores.cpu().numpy()}\")\n",
        "\n",
        "# 保存测试结果\n",
        "test_results = {\n",
        "    'test_loss': test_loss,\n",
        "    'test_dice_mean': test_dice_scores.mean().item(),\n",
        "    'test_dice_per_class': test_dice_scores.cpu().numpy().tolist(),\n",
        "    'model_type': MODEL_TYPE,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "results_path = os.path.join(MODEL_SAVE_PATH, f\"test_results_{MODEL_TYPE}.json\")\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(test_results, f, indent=2)\n",
        "print(f\"\\n测试结果已保存到: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 训练结果分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练结果分析\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING RESULTS ANALYSIS REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[1. Training Progress]\")\n",
        "print(f\"   Completed Epochs: {len(train_losses)}/{NUM_EPOCHS}\")\n",
        "print(f\"   Completion: {len(train_losses)/NUM_EPOCHS*100:.1f}%\")\n",
        "\n",
        "print(\"\\n[2. Loss Function Analysis]\")\n",
        "if len(train_losses) > 0:\n",
        "    print(f\"   Training Loss:\")\n",
        "    print(f\"      - Initial: {train_losses[0]:.4f}\")\n",
        "    print(f\"      - Final: {train_losses[-1]:.4f}\")\n",
        "    improvement = train_losses[0] - train_losses[-1]\n",
        "    improvement_pct = (improvement / train_losses[0]) * 100\n",
        "    print(f\"      - Improvement: {improvement:.4f} ({improvement_pct:.1f}%)\")\n",
        "\n",
        "if len(val_losses) > 0:\n",
        "    print(f\"\\n   Validation Loss:\")\n",
        "    print(f\"      - Initial: {val_losses[0]:.4f}\")\n",
        "    print(f\"      - Final: {val_losses[-1]:.4f}\")\n",
        "    print(f\"      - Best: {best_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n[3. Dice Coefficient Analysis]\")\n",
        "if len(val_dice_scores) > 0:\n",
        "    print(f\"   Validation Dice Score:\")\n",
        "    print(f\"      - Initial: {val_dice_scores[0]:.4f}\")\n",
        "    print(f\"      - Final: {val_dice_scores[-1]:.4f}\")\n",
        "    print(f\"      - Best: {best_dice_score:.4f}\")\n",
        "    print(f\"      - Average: {np.mean(val_dice_scores):.4f}\")\n",
        "\n",
        "print(\"\\n[4. Test Set Performance]\")\n",
        "if 'test_results' in locals():\n",
        "    print(f\"   Test Loss: {test_results['test_loss']:.4f}\")\n",
        "    print(f\"   Test Dice Score: {test_results['test_dice_mean']:.4f}\")\n",
        "\n",
        "print(\"\\n[5. Model Configuration]\")\n",
        "print(f\"   Model Type: {MODEL_TYPE}\")\n",
        "print(f\"   Image Size: {IMG_SIZE}x{IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Analysis Complete!\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
