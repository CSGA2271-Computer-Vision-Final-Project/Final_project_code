{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The current documentation contains some Chinese comments in the output. I will rerun the code shortly and upload a version with only English text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "986Mp4X3r16X"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# 3D Brain Tumor Segmentation with Swin UNETR (BraTS 21 Challenge)\n",
        "\n",
        "\n",
        "This tutorial uses the [Swin UNETR](https://arxiv.org/pdf/2201.01266.pdf) [1,2] model for the task of brain tumor segmentation using the [BraTS 21](http://braintumorsegmentation.org/) challenge dataset [3,4,5,6]. Swin UNETR ranked among top-performing models in the BraTS 21 validation phase. The architecture of Swin UNETR is demonstrated below\n",
        "\n",
        "![swin_brats](../figures/swin_brats21.png)\n",
        "\n",
        "The following features are included in this tutorial:\n",
        "1. Transforms for dictionary format data.\n",
        "1. Define a new transform according to MONAI transform API.\n",
        "1. Load Nifti image with metadata, load a list of images and stack them.\n",
        "1. Randomly rotate across each axes for data augmentation.\n",
        "1. Randomly adjust the intensity for data augmentation.\n",
        "1. Cache IO and transforms to accelerate training and validation.\n",
        "1. Swin UNETR model, Dice loss function, Mean Dice metric for brain tumor segmentation task.\n",
        "\n",
        "For more information access to pre-trained models and distributed training, please refer to Swin UNETR BraTS 21 official repository:\n",
        "\n",
        "https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BRATS21\n",
        "\n",
        "## Data Description\n",
        "\n",
        "Modality: MRI\n",
        "Size: 1470 3D volumes (1251 Training + 219 Validation)  \n",
        "Challenge: RSNA-ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge\n",
        "\n",
        "The dataset needs to be downloaded from the official BraTS 21 challenge portal as in the following\n",
        "\n",
        "https://www.synapse.org/#!Synapse:syn27046444/wiki/616992\n",
        "\n",
        "The JSON file containing training and validation sets (internal split) needs to be downloaded from this [link](https://developer.download.nvidia.com/assets/Clara/monai/tutorials/brats21_folds.json) and placed in the same folder as the dataset. As discussed in the following, this tutorial uses fold 1 for training a Swin UNETR model on the BraTS 21 challenge.\n",
        "\n",
        "### Tumor Characteristics\n",
        "\n",
        "The sub-regions considered for evaluation in the BraTS 21 challenge are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT). The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to ‚Äúhealthy‚Äù white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by the hyper-intense signal in FLAIR [[BraTS 21]](http://braintumorsegmentation.org/).\n",
        "\n",
        "The provided segmentation labels have values of 1 for NCR, 2 for ED, 4 for ET, and 0 for everything else.\n",
        "\n",
        "![image](../figures/fig_brats21.png)\n",
        "\n",
        "Figure from [Baid et al.](https://arxiv.org/pdf/2107.02314v1.pdf) [3]\n",
        "\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "\n",
        "If you find this tutorial helpful, please consider citing [1] and [2]:\n",
        "\n",
        "[1]: Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. and Xu, D., 2022. Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv preprint arXiv:2201.01266.\n",
        "\n",
        "[2]: Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V. and Hatamizadeh, A., 2022. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20730-20740).\n",
        "\n",
        "\n",
        "### BraTS Dataset References\n",
        "\n",
        "[3] U.Baid, et al., The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification, arXiv:2107.02314, 2021.\n",
        "\n",
        "[4] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. \"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694\n",
        "\n",
        "[5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., \"Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features\", Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117\n",
        "\n",
        "[6] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., \"Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection\", The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHHioD6ur16k"
      },
      "source": [
        "## Swin UNETR Model\n",
        "\n",
        "The inputs to [Swin UNETR](https://arxiv.org/pdf/2201.01266.pdf) are 3D multi-modal MRI images with 4 channels.\n",
        "The patch partition block creates non-overlapping patches of the input data and projects them into embedding tokens with a resolution of 128x128x128.\n",
        "The projected tokens are then encoded by using a 3D [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf) in which the self-attention is computed within local windows.\n",
        "The interaction between different windows is obtained by using 3D window shifting as illustrated below.\n",
        "\n",
        "![image](../figures/shift_patch.png)\n",
        "\n",
        "The transformer-based encoder is connected to a CNN-decoder via skip connection at multiple resolutions.\n",
        "The segmentation output consists of 3 output channels corresponding to ET, WT, and TC sub-regions and is computed by using a 1x1x1 convolutional layer followed by Sigmoid activation function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAO4Feu5r16m"
      },
      "source": [
        " ## Download dataset and json file\n",
        "\n",
        "- Register and download the official BraTS 21 dataset from the link below and place them into \"TrainingData\" in the dataset folder:\n",
        "\n",
        "  https://www.synapse.org/#!Synapse:syn27046444/wiki/616992\n",
        "  \n",
        "  For example, the address of a single file is as follows:\n",
        "  \n",
        "  \"TrainingData/BraTS2021_01146/BraTS2021_01146_flair.nii.gz\"\n",
        "  \n",
        "\n",
        "- Download the json file from this [link](https://developer.download.nvidia.com/assets/Clara/monai/tutorials/brats21_folds.json) and placed in the same folder as the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0-XKQ2r16n"
      },
      "source": [
        "## GPU and Environment Configuration\n",
        "\n",
        "### Important: Select High-Performance GPU\n",
        "\n",
        "**Before running the code, please complete the following steps:**\n",
        "\n",
        "1. Click **\"Runtime\"** in the top right corner of Colab ‚Üí **\"Change runtime type\"**\n",
        "2. Select **\"GPU\"** from the **\"Hardware accelerator\"** dropdown menu\n",
        "3. In **\"GPU type\"**, select:\n",
        "   - **H100** - Best choice! (Fastest, 2x faster than A100)\n",
        "   - **A100** - Second best choice (80GB memory, excellent performance)\n",
        "4. Click **\"Save\"**\n",
        "\n",
        "**Recommendation:** If H100 is available, strongly recommend using H100! Training speed is 2x faster.\n",
        "\n",
        "### Description\n",
        "\n",
        "This notebook uses:\n",
        "- **Data source**: AWS S3 streaming loading (no pre-download required)\n",
        "- **Model saving**: Google Drive (persistent storage)\n",
        "\n",
        "Running the cell below will:\n",
        "- Mount Google Drive (for saving trained models)\n",
        "- Install necessary dependencies (boto3, monai, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyNIRYxAr16p",
        "outputId": "bb414895-abd9-4f00-bdf4-ed5eed2eba38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Ê≠£Âú®ÂÆâË£ÖÂøÖË¶ÅÁöÑÂ∫ì...\n",
            "Ê≠£Âú®ÂÆâË£Ö monai-weekly[nibabel]...\n",
            "‚úì monai ÂÆâË£ÖÂÆåÊàê\n",
            "‚úì matplotlib Â∑≤ÂÆâË£Ö\n",
            "Ê≠£Âú®ÂÆâË£Ö boto3...\n",
            "‚úì boto3 ÂÆâË£ÖÂÆåÊàê\n",
            "============================================================\n",
            "Ê£ÄÊü•GPUÁ±ªÂûã\n",
            "============================================================\n",
            "ÂΩìÂâçGPU: NVIDIA A100-SXM4-80GB\n",
            "GPUÊòæÂ≠ò: 79.32 GB\n",
            "\n",
            "‚úì Ê£ÄÊµãÂà∞A100ÔºåÊÄßËÉΩ‰ºòÁßÄÔºÅ\n",
            "\n",
            "============================================================\n",
            "GPUÊòæÂ≠òÁä∂ÊÄÅ\n",
            "============================================================\n",
            "  ÊÄªÊòæÂ≠ò: 79.32 GB\n",
            "  Â∑≤ÂàÜÈÖç: 0.00 GB\n",
            "  Â∑≤‰øùÁïô: 0.00 GB\n",
            "  ÂèØÁî®: 79.32 GB\n",
            "\n",
            "‚úì 80GB GPUÔºÅÂ∞Ü‰ΩøÁî®ÊúÄÂ§ßÈÖçÁΩÆ\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "print(\"Installing necessary libraries...\")\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package, import_name=None):\n",
        "    if import_name is None:\n",
        "        import_name = package.split(\"[\")[0].split(\"==\")[0]\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"{import_name} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        print(f\"{import_name} installation completed\")\n",
        "\n",
        "install_package(\"monai-weekly[nibabel]\", \"monai\")\n",
        "install_package(\"matplotlib\")\n",
        "install_package(\"boto3\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Checking GPU Type\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"GPU not detected, please ensure GPU runtime is selected\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "print(f\"Current GPU: {gpu_name}\")\n",
        "print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "# Check if it's a recommended GPU\n",
        "if \"H100\" in gpu_name:\n",
        "    print(\"\\nH100 detected! Best choice, fastest speed!\")\n",
        "    gpu_tier = \"H100\"\n",
        "elif \"A100\" in gpu_name:\n",
        "    print(\"\\nA100 detected, excellent performance!\")\n",
        "    gpu_tier = \"A100\"\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Warning: Current GPU performance may be insufficient\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Detected GPU: {gpu_name}\")\n",
        "    print(\"\\nRecommended GPUs:\")\n",
        "    print(\"1. H100 (Best, fastest)\")\n",
        "    print(\"2. A100 (Second best, excellent performance)\")\n",
        "    print(\"\\nTo get best performance, please switch GPU:\")\n",
        "    print(\"Runtime ‚Üí Change runtime type ‚Üí H100/A100 GPU\")\n",
        "    print(\"=\"*60)\n",
        "    gpu_tier = \"Other\"\n",
        "    # Don't force error, allow continuation\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    free = total - reserved\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GPU Memory Status\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  Total Memory: {total:.2f} GB\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"  Available: {free:.2f} GB\")\n",
        "\n",
        "    if total >= 75:\n",
        "        if gpu_tier == \"H100\":\n",
        "            print(\"\\n80GB H100! Will use maximum configuration (can be further optimized)\")\n",
        "        else:\n",
        "            print(\"\\n80GB GPU! Will use maximum configuration\")\n",
        "        use_high_end_gpu = True\n",
        "    elif total >= 38:\n",
        "        print(\"\\n40GB GPU, good performance\")\n",
        "        use_high_end_gpu = False\n",
        "    else:\n",
        "        print(\"\\nWarning: Small memory, using conservative configuration\")\n",
        "        use_high_end_gpu = False\n",
        "\n",
        "    if free < 5:\n",
        "        print(\"\\nWarning: Very little available memory, recommend restarting runtime to clear memory\")\n",
        "        print(\"   Method: Runtime -> Interrupt execution -> Restart runtime\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "use_high_end_optimization = True\n",
        "\n",
        "# H100 additional optimization suggestions\n",
        "if gpu_tier == \"H100\":\n",
        "    print(\"\\nH100 Optimization Tips:\")\n",
        "    print(\"  - H100 is approximately 2x faster than A100\")\n",
        "    print(\"  - Expected training time: 10-12 minutes/epoch (vs A100's 20-25 minutes)\")\n",
        "    print(\"  - For further optimization, can increase batch_size or use FP8 precision\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj7nzTwFcwUm"
      },
      "source": [
        "## AWS S3 Connection Configuration\n",
        "\n",
        "Configure AWS S3 connection for streaming data loading.\n",
        "\n",
        "**Note:** No pre-download of data is required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsgncvdCcwUm",
        "outputId": "f8f8540e-724b-4eae-9be4-b6ac805fbbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "AWS S3ËøûÊé•ÈÖçÁΩÆ\n",
            "============================================================\n",
            "S3Â≠òÂÇ®Ê°∂: cvhomeworkdata\n",
            "S3Âå∫Âüü: us-east-2\n",
            "Êï∞ÊçÆÂâçÁºÄ: '' (Ê†πÁõÆÂΩï)\n",
            "============================================================\n",
            "\n",
            "‚úì S3ÂÆ¢Êà∑Á´ØÂàõÂª∫ÊàêÂäü\n",
            "‚úì ÂáÜÂ§áÂ∞±Áª™ÔºåÂ∞Ü‰ΩøÁî®ÊµÅÂºèÂä†ËΩΩÊñπÂºè\n",
            "\n",
            "üí° ÊèêÁ§∫: Êï∞ÊçÆÂ∞ÜÊåâÈúÄ‰ªéS3Áõ¥Êé•ËØªÂèñÔºåÊó†ÈúÄÈ¢ÑÂÖà‰∏ãËΩΩ\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "from botocore.exceptions import ClientError, NoCredentialsError\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# AWS S3 Configuration (for streaming loading)\n",
        "# ============================================================\n",
        "\n",
        "# AWS Credentials (public dataset)\n",
        "AWS_ACCESS_KEY_ID = \"AKIA3MK6ZLWBYRDPWXVS\"\n",
        "AWS_SECRET_ACCESS_KEY = \"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\"\n",
        "S3_BUCKET_NAME = \"cvhomeworkdata\"\n",
        "S3_REGION = \"us-east-2\"\n",
        "S3_DATA_PREFIX = \"\"  # Data path prefix (empty string means root directory)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"AWS S3 Connection Configuration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"S3 Bucket: {S3_BUCKET_NAME}\")\n",
        "print(f\"S3 Region: {S3_REGION}\")\n",
        "print(f\"Data Prefix: '{S3_DATA_PREFIX}' (root directory)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create S3 client\n",
        "def get_s3_client():\n",
        "    try:\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "            region_name=S3_REGION\n",
        "        )\n",
        "        return s3_client\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create S3 client: {e}\")\n",
        "        return None\n",
        "\n",
        "s3_client = get_s3_client()\n",
        "\n",
        "if s3_client:\n",
        "    print(\"\\nS3 client created successfully\")\n",
        "    print(\"Ready, will use streaming loading mode\")\n",
        "    print(\"\\nTip: Data will be read directly from S3 on demand, no pre-download required\")\n",
        "else:\n",
        "    print(\"\\nS3 client creation failed, please check network connection or AWS credentials\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch2a5-eJcwUn"
      },
      "source": [
        "## S3 Streaming Data Loading (Large Dataset Solution)\n",
        "\n",
        "For datasets of 75GB+, implement direct streaming reading from S3 without pre-downloading all data.\n",
        "\n",
        "### Core Advantages\n",
        "- No need to download all data to local\n",
        "- Small memory footprint, only loads current batch\n",
        "- Supports datasets of any size\n",
        "- Automatic LRU cache for performance optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LARCvLlucwUn",
        "outputId": "762343c7-bf40-4d9a-ccc9-1ace18dff77a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì S3ÊµÅÂºèÊï∞ÊçÆÂä†ËΩΩÊ®°ÂùóÂ∑≤Âä†ËΩΩ\n",
            "\n",
            "‰ΩøÁî®ÊñπÊ≥ï:\n",
            "1. ÂàõÂª∫Êï∞ÊçÆÂàóË°®: train_list, val_list = create_s3_streaming_datalist(s3_client, S3_BUCKET_NAME)\n",
            "2. ÂàõÂª∫Dataset: dataset = S3StreamingDataset(s3_client, S3_BUCKET_NAME, train_list)\n",
            "3. Ê≠£Â∏∏‰ΩøÁî®MONAI transformsÂíåDataLoader\n"
          ]
        }
      ],
      "source": [
        "import tempfile\n",
        "import io\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "\n",
        "class S3StreamingDataset:\n",
        "    \"\"\"\n",
        "    Dataset for streaming data reading from S3\n",
        "    Features:\n",
        "    1. Does not pre-download all data\n",
        "    2. Reads from S3 on demand\n",
        "    3. LRU cache for recently used files\n",
        "    4. Temporary local cache to accelerate repeated access\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, s3_client, bucket_name, file_list, cache_size=20, use_local_cache=True):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            s3_client: boto3 S3 client\n",
        "            bucket_name: S3 bucket name\n",
        "            file_list: File list [{'image': [...], 'label': ...}, ...]\n",
        "            cache_size: Memory cache size (number of files)\n",
        "            use_local_cache: Whether to use local disk cache\n",
        "        \"\"\"\n",
        "        self.s3_client = s3_client\n",
        "        self.bucket_name = bucket_name\n",
        "        self.file_list = file_list\n",
        "        self.cache_size = cache_size\n",
        "        self.use_local_cache = use_local_cache\n",
        "\n",
        "        # Create temporary cache directory (in Colab VM, only cache currently used files)\n",
        "        if use_local_cache:\n",
        "            self.cache_dir = tempfile.mkdtemp(prefix='s3_cache_')\n",
        "            print(f\"Local cache directory: {self.cache_dir}\")\n",
        "        else:\n",
        "            self.cache_dir = None\n",
        "\n",
        "        print(f\"S3 streaming Dataset initialization completed:\")\n",
        "        print(f\"  Number of data samples: {len(file_list)}\")\n",
        "        print(f\"  Memory cache: {cache_size} files\")\n",
        "        print(f\"  Local cache: {'Enabled' if use_local_cache else 'Disabled'}\")\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def _get_file_from_s3(self, s3_key):\n",
        "        \"\"\"\n",
        "        Get file content from S3 (with LRU cache and retry mechanism)\n",
        "\n",
        "        Parameters:\n",
        "            s3_key: S3 file path\n",
        "\n",
        "        Returns:\n",
        "            File path (local cache) or file content\n",
        "        \"\"\"\n",
        "        # Check local cache\n",
        "        if self.cache_dir:\n",
        "            cache_filename = hashlib.md5(s3_key.encode()).hexdigest() + '.nii'\n",
        "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
        "\n",
        "            if os.path.exists(cache_path):\n",
        "                # Read from local cache\n",
        "                return cache_path\n",
        "\n",
        "        # Download from S3 (with retry mechanism)\n",
        "        max_retries = 5\n",
        "        retry_delay = 2  # seconds\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)\n",
        "                file_content = response['Body'].read()\n",
        "\n",
        "                # Save to local cache\n",
        "                if self.cache_dir:\n",
        "                    with open(cache_path, 'wb') as f:\n",
        "                        f.write(file_content)\n",
        "                    return cache_path\n",
        "\n",
        "                # Otherwise return data in memory\n",
        "                return io.BytesIO(file_content)\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    filename = os.path.basename(s3_key)\n",
        "                    print(f\"S3 read failed (attempt {attempt+1}/{max_retries}): {filename}\")\n",
        "                    print(f\"   Error type: {type(e).__name__}, waiting {retry_delay} seconds before retry...\")\n",
        "                    import time\n",
        "                    time.sleep(retry_delay)\n",
        "                    retry_delay = min(retry_delay * 1.5, 10)  # Exponential backoff, max 10 seconds\n",
        "                else:\n",
        "                    print(f\"S3 read finally failed: {s3_key}\")\n",
        "                    print(f\"   Error: {e}\")\n",
        "                    raise\n",
        "\n",
        "    def get_sample_files(self, idx):\n",
        "        \"\"\"\n",
        "        Get sample file paths for specified index\n",
        "\n",
        "        Parameters:\n",
        "            idx: Sample index\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing file paths\n",
        "        \"\"\"\n",
        "        sample = self.file_list[idx].copy()\n",
        "\n",
        "        # Download image files\n",
        "        if 'image' in sample:\n",
        "            image_paths = []\n",
        "            for img_key in sample['image']:\n",
        "                if isinstance(img_key, str):\n",
        "                    # If S3 path, download to cache\n",
        "                    if img_key.startswith('s3://') or not os.path.exists(img_key):\n",
        "                        # Extract S3 key\n",
        "                        s3_key = img_key.replace(f's3://{self.bucket_name}/', '')\n",
        "                        local_path = self._get_file_from_s3(s3_key)\n",
        "                        image_paths.append(local_path)\n",
        "                    else:\n",
        "                        image_paths.append(img_key)\n",
        "            sample['image'] = image_paths\n",
        "\n",
        "        # Download label file\n",
        "        if 'label' in sample:\n",
        "            label_key = sample['label']\n",
        "            if isinstance(label_key, str):\n",
        "                if label_key.startswith('s3://') or not os.path.exists(label_key):\n",
        "                    s3_key = label_key.replace(f's3://{self.bucket_name}/', '')\n",
        "                    sample['label'] = self._get_file_from_s3(s3_key)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary cache\"\"\"\n",
        "        if self.cache_dir and os.path.exists(self.cache_dir):\n",
        "            import shutil\n",
        "            try:\n",
        "                shutil.rmtree(self.cache_dir)\n",
        "                print(f\"Cleaned cache directory: {self.cache_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to clean cache: {e}\")\n",
        "\n",
        "\n",
        "def create_s3_streaming_datalist(s3_client, bucket_name, prefix='', max_samples=None):\n",
        "    \"\"\"\n",
        "    Create data list from S3 (does not download files, only lists paths)\n",
        "\n",
        "    Parameters:\n",
        "        s3_client: boto3 S3 client\n",
        "        bucket_name: S3 bucket name\n",
        "        prefix: S3 path prefix\n",
        "        max_samples: Maximum number of samples (None=all)\n",
        "\n",
        "    Returns:\n",
        "        Training and validation data lists\n",
        "    \"\"\"\n",
        "    print(\"Creating data list from S3...\")\n",
        "\n",
        "    # List all files\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "\n",
        "        if 'Contents' not in response:\n",
        "            print(\"S3 bucket is empty\")\n",
        "            return [], []\n",
        "\n",
        "        all_files = [obj['Key'] for obj in response['Contents']]\n",
        "        print(f\"Found {len(all_files)} files\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to list S3 files: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    # Group by samples\n",
        "    from collections import defaultdict\n",
        "    import re\n",
        "\n",
        "    patient_groups = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    for s3_key in all_files:\n",
        "        if not s3_key.endswith('.nii'):\n",
        "            continue\n",
        "\n",
        "        filename = os.path.basename(s3_key)\n",
        "        match = re.match(r'BraTS-GLI-(\\d+)-(\\d+)-(t1n|t2f|t2w|t1c|seg)\\.nii', filename)\n",
        "\n",
        "        if match:\n",
        "            patient_id = match.group(1)\n",
        "            sequence_id = match.group(2)\n",
        "            modality = match.group(3)\n",
        "            patient_groups[patient_id][sequence_id][modality] = s3_key\n",
        "\n",
        "    # Build data list\n",
        "    data_list = []\n",
        "    for patient_id, sequences in patient_groups.items():\n",
        "        for seq_id, modalities in sequences.items():\n",
        "            if all(m in modalities for m in ['t2f', 't1c', 't1n', 't2w', 'seg']):\n",
        "                data_entry = {\n",
        "                    \"image\": [\n",
        "                        modalities['t2f'],  # flair\n",
        "                        modalities['t1c'],  # t1ce\n",
        "                        modalities['t1n'],  # t1\n",
        "                        modalities['t2w'],  # t2\n",
        "                    ],\n",
        "                    \"label\": modalities['seg']\n",
        "                }\n",
        "                data_list.append(data_entry)\n",
        "\n",
        "                # Limit number of samples\n",
        "                if max_samples and len(data_list) >= max_samples:\n",
        "                    break\n",
        "\n",
        "        if max_samples and len(data_list) >= max_samples:\n",
        "            break\n",
        "\n",
        "    # Random shuffle and split training/validation sets\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(data_list)\n",
        "    split_idx = int(len(data_list) * 0.8)\n",
        "\n",
        "    train_list = data_list[:split_idx]\n",
        "    val_list = data_list[split_idx:]\n",
        "\n",
        "    print(f\"Creation completed:\")\n",
        "    print(f\"  Training set: {len(train_list)} samples\")\n",
        "    print(f\"  Validation set: {len(val_list)} samples\")\n",
        "    print(f\"  Total size: ~{len(data_list) * 5 * 0.1:.1f} GB (estimated, does not occupy local space)\")\n",
        "\n",
        "    return train_list, val_list\n",
        "\n",
        "\n",
        "print(\"S3 streaming data loading module loaded\")\n",
        "print(\"\\nUsage:\")\n",
        "print(\"1. Create data list: train_list, val_list = create_s3_streaming_datalist(s3_client, S3_BUCKET_NAME)\")\n",
        "print(\"2. Create Dataset: dataset = S3StreamingDataset(s3_client, S3_BUCKET_NAME, train_list)\")\n",
        "print(\"3. Use MONAI transforms and DataLoader normally\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MObz5GdUcwUo"
      },
      "source": [
        "## S3 Streaming DataLoader (Recommended for Large Datasets)\n",
        "\n",
        "Create DataLoader using S3 streaming reading, no need to pre-download all data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJGosJ6YcwUo",
        "outputId": "89e41ea5-fe11-4bfa-87af-04cd97d7fffc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì S3ÊµÅÂºèDataLoaderÂáΩÊï∞Â∑≤Âä†ËΩΩ\n",
            "\n",
            "‰ΩøÁî®ÊñπÊ≥ï:\n",
            "train_loader, val_loader, train_dataset, val_dataset = get_s3_streaming_loader(\n",
            "    batch_size=1,\n",
            "    s3_client=s3_client,\n",
            "    bucket_name=S3_BUCKET_NAME,\n",
            "    roi=(128, 128, 128)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from monai.data import Dataset, DataLoader, CacheDataset\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    MapTransform,\n",
        ")\n",
        "\n",
        "class S3LoadImaged(MapTransform):\n",
        "    \"\"\"\n",
        "    Custom Transform: Load images from S3 streaming\n",
        "    Integrated into MONAI transform pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, keys, s3_dataset):\n",
        "        super().__init__(keys)\n",
        "        self.s3_dataset = s3_dataset\n",
        "\n",
        "    def __call__(self, data):\n",
        "        \"\"\"\n",
        "        Load data dictionary\n",
        "        \"\"\"\n",
        "        d = dict(data)\n",
        "\n",
        "        # Process each key\n",
        "        for key in self.keys:\n",
        "            if key in d:\n",
        "                if isinstance(d[key], list):\n",
        "                    # Process multi-modal images\n",
        "                    loaded_images = []\n",
        "                    for img_path in d[key]:\n",
        "                        if isinstance(img_path, str):\n",
        "                            # If S3 path, load from S3\n",
        "                            if not os.path.exists(img_path):\n",
        "                                s3_key = img_path\n",
        "                                local_path = self.s3_dataset._get_file_from_s3(s3_key)\n",
        "                                loaded_images.append(local_path)\n",
        "                            else:\n",
        "                                loaded_images.append(img_path)\n",
        "                        else:\n",
        "                            loaded_images.append(img_path)\n",
        "                    d[key] = loaded_images\n",
        "                else:\n",
        "                    # Process single file\n",
        "                    if isinstance(d[key], str) and not os.path.exists(d[key]):\n",
        "                        s3_key = d[key]\n",
        "                        d[key] = self.s3_dataset._get_file_from_s3(s3_key)\n",
        "\n",
        "        return d\n",
        "\n",
        "\n",
        "def get_s3_streaming_loader(batch_size, s3_client, bucket_name, roi,\n",
        "                             max_samples_train=None, max_samples_val=None,\n",
        "                             num_workers=0, cache_size=50):\n",
        "    \"\"\"\n",
        "    Create DataLoader based on S3 streaming reading\n",
        "\n",
        "    Parameters:\n",
        "        batch_size: Batch size\n",
        "        s3_client: boto3 S3 client\n",
        "        bucket_name: S3 bucket name\n",
        "        roi: ROI size (H, W, D)\n",
        "        max_samples_train: Maximum number of training samples (None=all)\n",
        "        max_samples_val: Maximum number of validation samples (None=all)\n",
        "        num_workers: DataLoader worker processes (recommend 0 to avoid SSL errors, or 1-2 if network is stable)\n",
        "        cache_size: Number of local cache files\n",
        "\n",
        "    Returns:\n",
        "        train_loader, val_loader\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Creating S3 Streaming DataLoader\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Create data list (does not download files)\n",
        "    print(\"\\nStep 1: Creating data list from S3...\")\n",
        "    train_files, val_files = create_s3_streaming_datalist(\n",
        "        s3_client,\n",
        "        bucket_name,\n",
        "        prefix=S3_DATA_PREFIX if 'S3_DATA_PREFIX' in globals() else '',\n",
        "        max_samples=max_samples_train\n",
        "    )\n",
        "\n",
        "    # If validation set sample limit is specified\n",
        "    if max_samples_val and len(val_files) > max_samples_val:\n",
        "        val_files = val_files[:max_samples_val]\n",
        "        print(f\"Validation set limited to: {max_samples_val} samples\")\n",
        "\n",
        "    # 2. Create S3 streaming Dataset objects\n",
        "    print(\"\\nStep 2: Creating S3 streaming Dataset...\")\n",
        "    train_s3_dataset = S3StreamingDataset(\n",
        "        s3_client,\n",
        "        bucket_name,\n",
        "        train_files,\n",
        "        cache_size=cache_size,\n",
        "        use_local_cache=True\n",
        "    )\n",
        "\n",
        "    val_s3_dataset = S3StreamingDataset(\n",
        "        s3_client,\n",
        "        bucket_name,\n",
        "        val_files,\n",
        "        cache_size=cache_size//2,  # Validation set cache is smaller\n",
        "        use_local_cache=True\n",
        "    )\n",
        "\n",
        "    # 3. Define transforms (same as before, but using S3 loading)\n",
        "    print(\"\\nStep 3: Configuring data augmentation...\")\n",
        "    train_transform = Compose([\n",
        "        S3LoadImaged(keys=[\"image\", \"label\"], s3_dataset=train_s3_dataset),\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),  # MONAI standard loading\n",
        "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        transforms.CropForegroundd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            source_key=\"image\",\n",
        "            k_divisible=[roi[0], roi[1], roi[2]],\n",
        "            allow_smaller=True,\n",
        "        ),\n",
        "        transforms.RandSpatialCropd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            roi_size=[roi[0], roi[1], roi[2]],\n",
        "            random_size=False,\n",
        "        ),\n",
        "        transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
        "        transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
        "        transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
        "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "        transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "        transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "    ])\n",
        "\n",
        "    val_transform = Compose([\n",
        "        S3LoadImaged(keys=[\"image\", \"label\"], s3_dataset=val_s3_dataset),\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "    ])\n",
        "\n",
        "    # 4. Create Dataset\n",
        "    print(\"\\nStep 4: Creating MONAI Dataset...\")\n",
        "    train_ds = Dataset(data=train_files, transform=train_transform)\n",
        "    val_ds = Dataset(data=val_files, transform=val_transform)\n",
        "\n",
        "    # 5. Create DataLoader\n",
        "    print(\"\\nStep 5: Creating DataLoader...\")\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,  # 0=single process to avoid SSL errors\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if num_workers > 0 else False,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if num_workers > 0 else False,\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"S3 Streaming DataLoader Creation Completed\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Training set: {len(train_files)} samples, {len(train_loader)} batches\")\n",
        "    print(f\"Validation set: {len(val_files)} samples, {len(val_loader)} batches\")\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(f\"Num Workers: {num_workers}\")\n",
        "    print(f\"Local cache: {cache_size} files\")\n",
        "    print(f\"\\nAdvantages:\")\n",
        "    print(f\"   - No need to download all 75GB data\")\n",
        "    print(f\"   - Load from S3 on demand, release after use\")\n",
        "    print(f\"   - Intelligent cache for recently used files\")\n",
        "    print(f\"   - Memory usage: ~{batch_size * 0.5:.1f}GB (only current batch)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return train_loader, val_loader, train_s3_dataset, val_s3_dataset\n",
        "\n",
        "\n",
        "print(\"S3 streaming DataLoader function loaded\")\n",
        "print(\"\\nUsage:\")\n",
        "print(\"train_loader, val_loader, train_dataset, val_dataset = get_s3_streaming_loader(\")\n",
        "print(\"    batch_size=1,\")\n",
        "print(\"    s3_client=s3_client,\")\n",
        "print(\"    bucket_name=S3_BUCKET_NAME,\")\n",
        "print(\"    roi=(128, 128, 128)\")\n",
        "print(\")\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUkkij19r16s"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oolwCWLur16t",
        "outputId": "fb980125-01dd-4446-90ee-849b2095b636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.6.dev2550\n",
            "Numpy version: 2.0.2\n",
            "Pytorch version: 2.9.0+cu126\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: c07b9218c4b93239cfa158bb704dd723ed522baf\n",
            "MONAI __file__: /usr/local/lib/python3.12/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.3.3\n",
            "scikit-image version: 0.25.2\n",
            "scipy version: 1.16.3\n",
            "Pillow version: 11.3.0\n",
            "Tensorboard version: 2.19.0\n",
            "gdown version: 5.2.0\n",
            "TorchVision version: 0.24.0+cu126\n",
            "tqdm version: 4.67.1\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 2.2.2\n",
            "einops version: 0.8.1\n",
            "transformers version: 4.57.3\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://monai.readthedocs.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai import transforms\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    Activations,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.utils.enums import MetricReduction\n",
        "from monai.networks.nets import SwinUNETR\n",
        "from monai import data\n",
        "from monai.data import decollate_batch\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4pDHaNDr16w"
      },
      "source": [
        "## Model Save Directory Configuration\n",
        "\n",
        "Configure the save location for trained model checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIjTJNZRr16x",
        "outputId": "3238a8fb-c0f7-4cc7-dc43-bd96f2462f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ê®°Âûã‰øùÂ≠òÁõÆÂΩï: /content/drive/MyDrive/swin_unetr_models\n",
            "üíæ ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãcheckpointÂ∞Ü‰øùÂ≠òÂà∞Ê≠§ÁõÆÂΩï\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Model Save Directory Configuration\n",
        "# ============================================================\n",
        "\n",
        "# Save model to Google Drive (persistent storage, recommended)\n",
        "root_dir = \"/content/drive/MyDrive/swin_unetr_models\"\n",
        "\n",
        "# If you don't want to use Drive, can change to temporary directory (lost after restart):\n",
        "# import tempfile\n",
        "# root_dir = tempfile.mkdtemp()\n",
        "\n",
        "# Create directory\n",
        "os.makedirs(root_dir, exist_ok=True)\n",
        "print(f\"Model save directory: {root_dir}\")\n",
        "print(f\"Trained model checkpoints will be saved to this directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI_mXO4Lr16y"
      },
      "source": [
        "## Setup average meter, fold reader, checkpoint saver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0grsT0pQr16z"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
        "\n",
        "\n",
        "def create_data_list_from_folder(data_dir, output_json_path, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Automatically create data list JSON file from folder\n",
        "    Data format: BraTS-GLI-XXXXX-XXX-{modality}.nii\n",
        "    Modality mapping: t2f -> flair, t1c -> t1ce, t1n -> t1, t2w -> t2, seg -> seg\n",
        "    \"\"\"\n",
        "    all_files = glob.glob(os.path.join(data_dir, \"*.nii\"))\n",
        "    patient_groups = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    # Modality mapping: your format -> BraTS standard format\n",
        "    modality_mapping = {\n",
        "        't2f': 'flair',\n",
        "        't1c': 't1ce',\n",
        "        't1n': 't1',\n",
        "        't2w': 't2',\n",
        "        'seg': 'seg'\n",
        "    }\n",
        "\n",
        "    # Parse filenames\n",
        "    for file_path in all_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        # Match format: BraTS-GLI-XXXXX-XXX-{modality}.nii\n",
        "        match = re.match(r'BraTS-GLI-(\\d+)-(\\d+)-(t1n|t2f|t2w|t1c|seg)\\.nii', filename)\n",
        "        if match:\n",
        "            patient_id = match.group(1)\n",
        "            sequence_id = match.group(2)\n",
        "            modality = match.group(3)\n",
        "            patient_groups[patient_id][sequence_id][modality] = file_path\n",
        "\n",
        "    # Build data list\n",
        "    data_list = []\n",
        "    for patient_id, sequences in patient_groups.items():\n",
        "        for seq_id, modalities in sequences.items():\n",
        "            # Check if all required modalities are included\n",
        "            if all(m in modalities for m in ['t2f', 't1c', 't1n', 't2w', 'seg']):\n",
        "                data_entry = {\n",
        "                    \"image\": [\n",
        "                        modalities['t2f'],  # flair\n",
        "                        modalities['t1c'],  # t1ce\n",
        "                        modalities['t1n'],  # t1\n",
        "                        modalities['t2w'],  # t2\n",
        "                    ],\n",
        "                    \"label\": modalities['seg']\n",
        "                }\n",
        "                data_list.append(data_entry)\n",
        "\n",
        "    # Random shuffle and split training/validation sets\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(data_list)\n",
        "    split_idx = int(len(data_list) * train_ratio)\n",
        "\n",
        "    training_data = data_list[:split_idx]\n",
        "    validation_data = data_list[split_idx:]\n",
        "\n",
        "    # Create fold structure (using fold 1)\n",
        "    folds_data = {\n",
        "        \"training\": [],\n",
        "        \"validation\": []\n",
        "    }\n",
        "\n",
        "    for i, item in enumerate(training_data):\n",
        "        item_with_fold = item.copy()\n",
        "        item_with_fold[\"fold\"] = 0 if i < len(training_data) * 0.8 else 1\n",
        "        folds_data[\"training\"].append(item_with_fold)\n",
        "\n",
        "    for item in validation_data:\n",
        "        item_with_fold = item.copy()\n",
        "        item_with_fold[\"fold\"] = 1\n",
        "        folds_data[\"validation\"].append(item_with_fold)\n",
        "\n",
        "    # Save JSON file\n",
        "    with open(output_json_path, 'w') as f:\n",
        "        json.dump(folds_data, f, indent=2)\n",
        "\n",
        "    print(f\"Data list creation completed!\")\n",
        "    print(f\"Training set: {len(training_data)} samples\")\n",
        "    print(f\"Validation set: {len(validation_data)} samples\")\n",
        "    print(f\"JSON file saved to: {output_json_path}\")\n",
        "\n",
        "    return output_json_path\n",
        "\n",
        "\n",
        "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
        "    with open(datalist) as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    json_data = json_data[key]\n",
        "\n",
        "    for d in json_data:\n",
        "        for k in d:\n",
        "            if isinstance(d[k], list):\n",
        "                d[k] = [os.path.join(basedir, iv) if not os.path.isabs(iv) else iv for iv in d[k]]\n",
        "            elif isinstance(d[k], str):\n",
        "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 and not os.path.isabs(d[k]) else d[k]\n",
        "\n",
        "    tr = []\n",
        "    val = []\n",
        "    for d in json_data:\n",
        "        if \"fold\" in d and d[\"fold\"] == fold:\n",
        "            val.append(d)\n",
        "        else:\n",
        "            tr.append(d)\n",
        "\n",
        "    return tr, val\n",
        "\n",
        "\n",
        "def save_checkpoint(model, epoch, filename=\"model.pt\", best_acc=0, dir_add=root_dir):\n",
        "    state_dict = model.state_dict()\n",
        "    save_dict = {\"epoch\": epoch, \"best_acc\": best_acc, \"state_dict\": state_dict}\n",
        "    filename = os.path.join(dir_add, filename)\n",
        "    torch.save(save_dict, filename)\n",
        "    print(\"Saving checkpoint\", filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkQne1yzr161"
      },
      "source": [
        "## Setup dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2PEx8hvr162"
      },
      "outputs": [],
      "source": [
        "def get_loader(batch_size, data_dir, json_list, fold, roi):\n",
        "    data_dir = data_dir\n",
        "    datalist_json = json_list\n",
        "    train_files, validation_files = datafold_read(datalist=datalist_json, basedir=data_dir, fold=fold)\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "            transforms.CropForegroundd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                source_key=\"image\",\n",
        "                k_divisible=[roi[0], roi[1], roi[2]],\n",
        "                allow_smaller=True,\n",
        "            ),\n",
        "            transforms.RandSpatialCropd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                roi_size=[roi[0], roi[1], roi[2]],\n",
        "                random_size=False,\n",
        "            ),\n",
        "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
        "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
        "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
        "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "            transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "            transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "        ]\n",
        "    )\n",
        "    val_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_ds = data.Dataset(data=train_files, transform=train_transform)\n",
        "\n",
        "    train_loader = data.DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_ds = data.Dataset(data=validation_files, transform=val_transform)\n",
        "    val_loader = data.DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlCej0zZbUX9"
      },
      "source": [
        "## High-Intensity Training Configuration (Target Dice >= 0.8)\n",
        "\n",
        "To achieve Dice score above 0.8, apply the following aggressive optimizations:\n",
        "\n",
        "1. **Model Capacity Increase**: feature_size 48 ‚Üí 96 (model parameters increase approximately 4x)\n",
        "2. **Input Size Increase**: ROI (128,128,128) ‚Üí (160,160,160) (larger receptive field)\n",
        "3. **Training Epochs Increase**: 300 ‚Üí 500 epochs\n",
        "4. **Learning Rate Optimization**: Longer warmup and slower decay\n",
        "5. **Loss Function Optimization**: Class weighting, more focus on ET class\n",
        "6. **Data Augmentation Enhancement**: Richer augmentation strategies\n",
        "7. **Batch Size Optimization**: If memory allows, try to increase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JCuycfbcwUq"
      },
      "source": [
        "## S3 Streaming Data Loading\n",
        "\n",
        "Use S3 streaming loading, no need to pre-download all 75GB data.\n",
        "\n",
        "**Features:**\n",
        "- Read data from S3 on demand\n",
        "- Intelligent local cache (approximately 5GB)\n",
        "- Small memory footprint (only current batch)\n",
        "- Suitable for datasets of any size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM9LJvE3cwUq",
        "outputId": "05c2cc75-e5ef-4a07-dcab-0c672f7de716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Ë∂ÖÂèÇÊï∞ÈÖçÁΩÆ\n",
            "============================================================\n",
            "ËÆ≠ÁªÉËΩÆÊï∞: 300\n",
            "È™åËØÅÈ¢ëÁéá: ÊØè10‰∏™epoch\n",
            "Batch Size: 2 (Ê¢ØÂ∫¶Á¥ØÁßØ: 2, Á≠âÊïà: 4)\n",
            "ROIÂ§ßÂ∞è: (128, 128, 128)\n",
            "ÊªëÂä®Á™óÂè£Batch: 4\n",
            "Êó©ÂÅúÈÖçÁΩÆ: patience=50, min_delta=0.0005\n",
            "S3ÁºìÂ≠òÈÖçÁΩÆ: 100‰∏™Êñá‰ª∂, num_workers=0 (ÂçïËøõÁ®ãÔºåÈÅøÂÖçSSLÈîôËØØ)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Hyperparameter Configuration (Important!)\n",
        "# ============================================================\n",
        "\n",
        "# Basic training parameters\n",
        "max_epochs = 300  # Maximum training epochs (doubled from 150 to 300)\n",
        "val_every = 10     # Validate every 10 epochs\n",
        "batch_size = 2    # Batch size\n",
        "roi = (128, 128, 128)  # ROI size\n",
        "sw_batch_size = 4  # Sliding window inference batch size\n",
        "infer_overlap = 0.5  # Overlap rate during inference\n",
        "\n",
        "# Gradient accumulation configuration\n",
        "gradient_accumulation_steps = 2  # Effective batch_size = 4\n",
        "\n",
        "# Early stopping configuration\n",
        "early_stop_patience = 50  # Increased to 50 epochs\n",
        "early_stop_min_delta = 0.0005  # More refined improvement threshold\n",
        "\n",
        "# S3 streaming loading configuration\n",
        "S3_STREAMING_CONFIG = {\n",
        "    'max_samples_train': None,  # None means use all data\n",
        "    'max_samples_val': None,\n",
        "    'num_workers': 0,  # Use 0 to avoid multi-process SSL errors (changed to single-process loading)\n",
        "    'cache_size': 100   # Increase local cache to 10GB, accelerate later epochs of long training\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Hyperparameter Configuration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training epochs: {max_epochs}\")\n",
        "print(f\"Validation frequency: every {val_every} epochs\")\n",
        "print(f\"Batch Size: {batch_size} (gradient accumulation: {gradient_accumulation_steps}, effective: {batch_size * gradient_accumulation_steps})\")\n",
        "print(f\"ROI size: {roi}\")\n",
        "print(f\"Sliding window Batch: {sw_batch_size}\")\n",
        "print(f\"Early stopping configuration: patience={early_stop_patience}, min_delta={early_stop_min_delta}\")\n",
        "print(f\"S3 cache configuration: {S3_STREAMING_CONFIG['cache_size']} files, num_workers={S3_STREAMING_CONFIG['num_workers']} (single process, avoid SSL errors)\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBeUaAEBcwUq",
        "outputId": "076c60c6-10ad-42b8-beda-14292fb59c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ê≠£Âú®ÂàõÂª∫S3ÊµÅÂºèDataLoader...\n",
            "\n",
            "============================================================\n",
            "ÂàõÂª∫S3ÊµÅÂºèDataLoader\n",
            "============================================================\n",
            "\n",
            "Ê≠•È™§1: ‰ªéS3ÂàõÂª∫Êï∞ÊçÆÂàóË°®...\n",
            "Ê≠£Âú®‰ªéS3ÂàõÂª∫Êï∞ÊçÆÂàóË°®...\n",
            "ÊâæÂà∞ 1000 ‰∏™Êñá‰ª∂\n",
            "ÂàõÂª∫ÂÆåÊàê:\n",
            "  ËÆ≠ÁªÉÈõÜ: 159 ‰∏™Ê†∑Êú¨\n",
            "  È™åËØÅÈõÜ: 40 ‰∏™Ê†∑Êú¨\n",
            "  ÊÄªÂ§ßÂ∞è: ~99.5 GBÔºà‰º∞ÁÆóÔºå‰∏çÂç†Áî®Êú¨Âú∞Á©∫Èó¥Ôºâ\n",
            "\n",
            "Ê≠•È™§2: ÂàõÂª∫S3ÊµÅÂºèDataset...\n",
            "Êú¨Âú∞ÁºìÂ≠òÁõÆÂΩï: /tmp/s3_cache_fac6c62j\n",
            "S3ÊµÅÂºèDatasetÂàùÂßãÂåñÂÆåÊàê:\n",
            "  Êï∞ÊçÆÊ†∑Êú¨Êï∞: 159\n",
            "  ÂÜÖÂ≠òÁºìÂ≠ò: 100 ‰∏™Êñá‰ª∂\n",
            "  Êú¨Âú∞ÁºìÂ≠ò: ÂêØÁî®\n",
            "Êú¨Âú∞ÁºìÂ≠òÁõÆÂΩï: /tmp/s3_cache_crf1rec4\n",
            "S3ÊµÅÂºèDatasetÂàùÂßãÂåñÂÆåÊàê:\n",
            "  Êï∞ÊçÆÊ†∑Êú¨Êï∞: 40\n",
            "  ÂÜÖÂ≠òÁºìÂ≠ò: 50 ‰∏™Êñá‰ª∂\n",
            "  Êú¨Âú∞ÁºìÂ≠ò: ÂêØÁî®\n",
            "\n",
            "Ê≠•È™§3: ÈÖçÁΩÆÊï∞ÊçÆÂ¢ûÂº∫...\n",
            "\n",
            "Ê≠•È™§4: ÂàõÂª∫MONAI Dataset...\n",
            "\n",
            "Ê≠•È™§5: ÂàõÂª∫DataLoader...\n",
            "\n",
            "============================================================\n",
            "S3ÊµÅÂºèDataLoaderÂàõÂª∫ÂÆåÊàê\n",
            "============================================================\n",
            "ËÆ≠ÁªÉÈõÜ: 159 ‰∏™Ê†∑Êú¨, 80 ‰∏™batches\n",
            "È™åËØÅÈõÜ: 40 ‰∏™Ê†∑Êú¨, 40 ‰∏™batches\n",
            "Batch Size: 2\n",
            "Num Workers: 0\n",
            "Êú¨Âú∞ÁºìÂ≠ò: 100 ‰∏™Êñá‰ª∂\n",
            "\n",
            "üí° ‰ºòÂäø:\n",
            "   - Êó†ÈúÄ‰∏ãËΩΩÂÖ®ÈÉ®75GBÊï∞ÊçÆ\n",
            "   - ÊåâÈúÄ‰ªéS3Âä†ËΩΩÔºåÁî®ÂÆåÂç≥ÈáäÊîæ\n",
            "   - Êô∫ËÉΩÁºìÂ≠òÊúÄËøë‰ΩøÁî®ÁöÑÊñá‰ª∂\n",
            "   - ÂÜÖÂ≠òÂç†Áî®: ~1.0GB (‰ªÖÂΩìÂâçbatch)\n",
            "============================================================\n",
            "\n",
            "‚úÖ S3ÊµÅÂºèDataLoaderÂàõÂª∫ÂÆåÊàê\n",
            "   ÊØè‰∏™epochÂ∞ÜÊåâÈúÄ‰ªéS3Âä†ËΩΩÊï∞ÊçÆ\n",
            "   Êú¨Âú∞ÁºìÂ≠òÁ∫¶Âç†Áî®5GBÁ©∫Èó¥\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Create S3 Streaming DataLoader\n",
        "# ============================================================\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nCreating S3 streaming DataLoader...\\n\")\n",
        "\n",
        "# Check S3 client\n",
        "if 's3_client' not in globals() or s3_client is None:\n",
        "    raise RuntimeError(\"S3 client not initialized, please run Cell 6 first\")\n",
        "\n",
        "# Create S3 streaming DataLoader\n",
        "train_loader, val_loader, train_s3_dataset, val_s3_dataset = get_s3_streaming_loader(\n",
        "    batch_size=batch_size,\n",
        "    s3_client=s3_client,\n",
        "    bucket_name=S3_BUCKET_NAME,\n",
        "    roi=roi,\n",
        "    **S3_STREAMING_CONFIG\n",
        ")\n",
        "\n",
        "print(f\"\\nS3 streaming DataLoader creation completed\")\n",
        "print(f\"   Each epoch will load data from S3 on demand\")\n",
        "print(f\"   Local cache will occupy approximately 5GB space\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "megmSmi1SLe8"
      },
      "source": [
        "## Training Configuration Optimization Notes\n",
        "\n",
        "Based on training result analysis, the following optimizations have been applied:\n",
        "\n",
        "### 1. Loss Function Optimization\n",
        "- **Combined Loss**: Dice Loss (70%) + Focal Loss (30%)\n",
        "- **Purpose**: Focal Loss handles class imbalance, especially improves ET class performance\n",
        "\n",
        "### 2. Learning Rate Strategy Optimization\n",
        "- **Initial Learning Rate**: 5e-5 (reduced from 1e-4)\n",
        "- **Warmup**: 10 epochs\n",
        "- **Scheduler**: Cosine Annealing with Warmup\n",
        "- **Purpose**: More stable training, avoid early oscillation\n",
        "\n",
        "### 3. Training Epochs Increase\n",
        "- **Maximum Epochs**: 150 (increased from 100)\n",
        "- **Purpose**: Give model more time to converge\n",
        "\n",
        "### 4. Validation Frequency Optimization\n",
        "- **Validation Frequency**: Every 5 epochs (reduced from 10)\n",
        "- **Purpose**: More frequent monitoring, detect issues in time\n",
        "\n",
        "### 5. Early Stopping Mechanism\n",
        "- **Patience**: 15 epochs\n",
        "- **Minimum Improvement**: 0.001\n",
        "- **Purpose**: Prevent overfitting, automatically stop training\n",
        "\n",
        "### 6. Memory Optimization (Medium Solution)\n",
        "- **ROI**: (128, 128, 128)\n",
        "- **Batch Size**: 2\n",
        "- **Gradient Accumulation**: 2 (effective batch_size=4)\n",
        "- **Gradient Checkpointing**: Enabled\n",
        "- **Purpose**: Balance memory and performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTfx7MG6r163"
      },
      "source": [
        "## Set dataset root directory and hyper-parameters\n",
        "\n",
        "The following hyper-parameters are set for the purpose of this tutorial. However, additional changes, as described below, maybe beneficial.\n",
        "\n",
        "If GPU memory is not sufficient, reduce sw_batch_size to 2 or batch_size to 1.\n",
        "\n",
        "Decrease val_every (validation frequency) to 1 for obtaining more accurate checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRLpGFzDr164"
      },
      "source": [
        "## Check data shape and visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khrL5Akjr164",
        "outputId": "547e60fd-16b8-4d83-a47e-03d8af2b2bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí° ‰ΩøÁî®S3ÊµÅÂºèÂä†ËΩΩÊó∂ÔºåÊ≠§Ê≠•È™§Ë∑≥Ëøá„ÄÇ\n",
            "   Êï∞ÊçÆÂ∞ÜÂú®ËÆ≠ÁªÉÊó∂Áõ¥Êé•‰ªéS3Âä†ËΩΩ„ÄÇ\n",
            "   Â¶ÇÈúÄÂèØËßÜÂåñÔºåÂèØ‰ª•Âú®ËÆ≠ÁªÉÂêéÂçïÁã¨‰∏ãËΩΩÊ†∑Êú¨„ÄÇ\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Data Visualization (Optional, skip when using S3 streaming loading)\n",
        "# ============================================================\n",
        "\n",
        "# When using S3 streaming loading, local data visualization is not needed\n",
        "# If visualization is needed, can download individual samples from S3 after training\n",
        "\n",
        "print(\"When using S3 streaming loading, this step is skipped.\")\n",
        "print(\"   Data will be loaded directly from S3 during training.\")\n",
        "print(\"   If visualization is needed, can download samples separately after training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6snnyljAr165"
      },
      "source": [
        "## Create Swin UNETR model\n",
        "\n",
        "In this section, we create Swin UNETR model for the 3-class brain tumor semantic segmentation. We use a feature size of 48. We also use gradient checkpointing (use_checkpoint) for more memory-efficient training. However, use_checkpoint for faster training if enough GPU memory is available.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA72OiN1r165",
        "outputId": "3c600b91-0436-40c7-8a9c-8f203d553d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ÂàõÂª∫Swin UNETRÊ®°ÂûãÔºàÈ´òÂº∫Â∫¶ÈÖçÁΩÆÔºâ...\n",
            "  ÁâπÂæÅÂ∞∫ÂØ∏: 96 (‰ªé48Â¢ûÂä†Âà∞96ÔºåÊ®°ÂûãÂÆπÈáèÂ¢ûÂä†Á∫¶4ÂÄç)\n",
            "  Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ: ÂºÄÂêØ\n",
            "Ê®°ÂûãÂèÇÊï∞: 248.10M\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if 'use_checkpoint' not in globals():\n",
        "    use_checkpoint = True\n",
        "\n",
        "# Use high-intensity configuration feature size\n",
        "if 'feature_size' not in globals():\n",
        "    feature_size = 96  # Increased from 48 to 96\n",
        "\n",
        "print(f\"Creating Swin UNETR model (high-intensity configuration)...\")\n",
        "print(f\"  Feature size: {feature_size} (increased from 48 to 96, model capacity increased approximately 4x)\")\n",
        "print(f\"  Gradient checkpointing: {'Enabled' if use_checkpoint else 'Disabled'}\")\n",
        "\n",
        "model = SwinUNETR(\n",
        "    in_channels=4,\n",
        "    out_channels=3,\n",
        "    feature_size=feature_size,\n",
        "    drop_rate=0.0,\n",
        "    attn_drop_rate=0.0,\n",
        "    dropout_path_rate=0.0,\n",
        "    use_checkpoint=use_checkpoint,\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {total_params/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aX-tFehr166"
      },
      "source": [
        "## Optimizer and loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woCSjq-8SLe-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1BFj3_rr166",
        "outputId": "7ab4acb9-2c0b-41a4-854f-2809cc8d0930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â∑≤ÂêØÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ (AMP)\n",
            "Â≠¶‰π†Áéá: 5e-05, Warmup: 20 epochs\n",
            "ÊçüÂ§±ÂáΩÊï∞: Dice (70%) + Focal (30%)\n"
          ]
        }
      ],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "from monai.losses import FocalLoss\n",
        "\n",
        "dice_loss = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
        "focal_loss = FocalLoss(to_onehot_y=False, gamma=2.0, weight=None)\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    dice = dice_loss(pred, target)\n",
        "    focal = focal_loss(pred, target)\n",
        "    return 0.7 * dice + 0.3 * focal\n",
        "\n",
        "loss_func = combined_loss\n",
        "\n",
        "post_sigmoid = Activations(sigmoid=True)\n",
        "post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
        "dice_acc = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
        "model_inferer = partial(\n",
        "    sliding_window_inference,\n",
        "    roi_size=[roi[0], roi[1], roi[2]],\n",
        "    sw_batch_size=sw_batch_size,\n",
        "    predictor=model,\n",
        "    overlap=infer_overlap,\n",
        ")\n",
        "\n",
        "initial_lr = 5e-5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=1e-5)\n",
        "\n",
        "warmup_epochs = 20\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < warmup_epochs:\n",
        "        return (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "use_amp = True\n",
        "print(\"Mixed precision training (AMP) enabled\")\n",
        "print(f\"Learning rate: {initial_lr}, Warmup: {warmup_epochs} epochs\")\n",
        "print(\"Loss function: Dice (70%) + Focal (30%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oSrOMeor167"
      },
      "source": [
        "## Define Train and Validation Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvmCbRMmr167",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, epoch, loss_func, scaler=None, use_amp=False, gradient_accumulation_steps=1):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    run_loss = AverageMeter()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for idx, batch_data in enumerate(loader):\n",
        "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "\n",
        "        if use_amp and scaler is not None:\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                logits = model(data)\n",
        "                loss = loss_func(logits, target)\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (idx + 1) % gradient_accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logits = model(data)\n",
        "            loss = loss_func(logits, target)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (idx + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        run_loss.update(loss.item() * gradient_accumulation_steps, n=batch_size)\n",
        "\n",
        "        if (idx + 1) % 10 == 0 or (idx + 1) == len(loader):\n",
        "            print(\n",
        "                \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, idx+1, len(loader)),\n",
        "                \"loss: {:.4f}\".format(run_loss.avg),\n",
        "                \"time {:.2f}s\".format(time.time() - start_time),\n",
        "            )\n",
        "        start_time = time.time()\n",
        "\n",
        "    if (idx + 1) % gradient_accumulation_steps != 0:\n",
        "        if use_amp and scaler is not None:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return run_loss.avg\n",
        "\n",
        "\n",
        "def val_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    epoch,\n",
        "    acc_func,\n",
        "    model_inferer=None,\n",
        "    post_sigmoid=None,\n",
        "    post_pred=None,\n",
        "):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    run_acc = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch_data in enumerate(loader):\n",
        "            data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "            logits = model_inferer(data)\n",
        "            val_labels_list = decollate_batch(target)\n",
        "            val_outputs_list = decollate_batch(logits)\n",
        "            val_output_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs_list]\n",
        "            acc_func.reset()\n",
        "            acc_func(y_pred=val_output_convert, y=val_labels_list)\n",
        "            acc, not_nans = acc_func.aggregate()\n",
        "            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
        "            dice_tc = run_acc.avg[0]\n",
        "            dice_wt = run_acc.avg[1]\n",
        "            dice_et = run_acc.avg[2]\n",
        "            print(\n",
        "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
        "                \", dice_tc:\",\n",
        "                dice_tc,\n",
        "                \", dice_wt:\",\n",
        "                dice_wt,\n",
        "                \", dice_et:\",\n",
        "                dice_et,\n",
        "                \", time {:.2f}s\".format(time.time() - start_time),\n",
        "            )\n",
        "            start_time = time.time()\n",
        "\n",
        "    return run_acc.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOqPb_5zr168"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akLinZksr168"
      },
      "outputs": [],
      "source": [
        "def trainer(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    loss_func,\n",
        "    acc_func,\n",
        "    scheduler,\n",
        "    model_inferer=None,\n",
        "    start_epoch=0,\n",
        "    post_sigmoid=None,\n",
        "    post_pred=None,\n",
        "    scaler=None,\n",
        "    use_amp=False,\n",
        "    gradient_accumulation_steps=1,\n",
        "    early_stop_patience=15,\n",
        "    early_stop_min_delta=0.001,\n",
        "):\n",
        "    val_acc_max = 0.0\n",
        "    dices_tc = []\n",
        "    dices_wt = []\n",
        "    dices_et = []\n",
        "    dices_avg = []\n",
        "    loss_epochs = []\n",
        "    trains_epoch = []\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Early stopping configuration: patience={early_stop_patience}, min_delta={early_stop_min_delta}\")\n",
        "\n",
        "    for epoch in range(start_epoch, max_epochs):\n",
        "        print(time.ctime(), \"Epoch:\", epoch)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        epoch_time = time.time()\n",
        "        train_loss = train_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            epoch=epoch,\n",
        "            loss_func=loss_func,\n",
        "            scaler=scaler,\n",
        "            use_amp=use_amp,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        )\n",
        "        print(\n",
        "            \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
        "            \"loss: {:.4f}\".format(train_loss),\n",
        "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
        "        )\n",
        "\n",
        "        if (epoch + 1) % val_every == 0 or epoch == 0:\n",
        "            loss_epochs.append(train_loss)\n",
        "            trains_epoch.append(int(epoch))\n",
        "            epoch_time = time.time()\n",
        "            val_acc = val_epoch(\n",
        "                model,\n",
        "                val_loader,\n",
        "                epoch=epoch,\n",
        "                acc_func=acc_func,\n",
        "                model_inferer=model_inferer,\n",
        "                post_sigmoid=post_sigmoid,\n",
        "                post_pred=post_pred,\n",
        "            )\n",
        "            dice_tc = val_acc[0]\n",
        "            dice_wt = val_acc[1]\n",
        "            dice_et = val_acc[2]\n",
        "            val_avg_acc = np.mean(val_acc)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(\n",
        "                \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
        "                \", dice_tc:\",\n",
        "                dice_tc,\n",
        "                \", dice_wt:\",\n",
        "                dice_wt,\n",
        "                \", dice_et:\",\n",
        "                dice_et,\n",
        "                \", Dice_Avg:\",\n",
        "                val_avg_acc,\n",
        "                \", LR: {:.6f}\".format(current_lr),\n",
        "                \", time {:.2f}s\".format(time.time() - epoch_time),\n",
        "            )\n",
        "            dices_tc.append(dice_tc)\n",
        "            dices_wt.append(dice_wt)\n",
        "            dices_et.append(dice_et)\n",
        "            dices_avg.append(val_avg_acc)\n",
        "\n",
        "            if val_avg_acc > val_acc_max + early_stop_min_delta:\n",
        "                print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_avg_acc))\n",
        "                val_acc_max = val_avg_acc\n",
        "                best_epoch = epoch\n",
        "                patience_counter = 0\n",
        "                save_checkpoint(\n",
        "                    model,\n",
        "                    epoch,\n",
        "                    best_acc=val_acc_max,\n",
        "                )\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= early_stop_patience:\n",
        "                    print(f\"\\nEarly stopping triggered: {early_stop_patience} epochs without improvement\")\n",
        "                    print(f\"Best result: {val_acc_max:.4f} (Epoch {best_epoch})\")\n",
        "                    break\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "    print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n",
        "    print(f\"Best Epoch: {best_epoch}\")\n",
        "    return (\n",
        "        val_acc_max,\n",
        "        dices_tc,\n",
        "        dices_wt,\n",
        "        dices_et,\n",
        "        dices_avg,\n",
        "        loss_epochs,\n",
        "        trains_epoch,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ueWA6uSbUYA"
      },
      "source": [
        "## Detailed Training Result Analysis Report (Fixed Version)\n",
        "\n",
        "Comprehensive analysis based on training output and performance metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebS921EQbUYA",
        "outputId": "d7d0859b-895f-4587-f72e-42a813fc7b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ËØ¶ÁªÜËÆ≠ÁªÉÁªìÊûúÂàÜÊûêÊä•Âëä\n",
            "======================================================================\n",
            "\n",
            "‚ö† Ë≠¶ÂëäÔºöËÆ≠ÁªÉÁªìÊûúÂèòÈáèÊú™ÊâæÂà∞\n",
            "ËØ∑ÂÖàËøêË°åËÆ≠ÁªÉcellÂÆåÊàêËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂÜçËøêË°åÊ≠§ÂàÜÊûêcell\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Detailed Training Result Analysis Report\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if training result variables exist\n",
        "if 'val_acc_max' not in globals() or 'dices_avg' not in globals():\n",
        "    print(\"\\nWarning: Training result variables not found\")\n",
        "    print(\"Please run the training cell to complete training first, then run this analysis cell\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    # 1. Overall Performance Metrics\n",
        "    print(\"\\n[1. Overall Performance Metrics]\")\n",
        "    print(f\"  Best Average Dice Score: {val_acc_max:.4f}\")\n",
        "    best_epoch_val = best_epoch if 'best_epoch' in globals() else 'N/A'\n",
        "    print(f\"  Best Epoch: {best_epoch_val}\")\n",
        "\n",
        "    if len(dices_avg) > 0:\n",
        "        trains_epoch_len = len(trains_epoch) if 'trains_epoch' in globals() and len(trains_epoch) > 0 else 0\n",
        "        print(f\"  Total Training Epochs: {trains_epoch_len} validation points\")\n",
        "        print(f\"  Initial Dice: {dices_avg[0]:.4f} (Epoch 0)\")\n",
        "        final_epoch = trains_epoch[-1] if 'trains_epoch' in globals() and len(trains_epoch) > 0 else 'N/A'\n",
        "        print(f\"  Final Dice: {dices_avg[-1]:.4f} (Epoch {final_epoch})\")\n",
        "    else:\n",
        "        print(\"  Training data is empty, please complete training first\")\n",
        "\n",
        "    # 2. Detailed Analysis by Class\n",
        "    if 'dices_tc' in globals() and 'dices_wt' in globals() and 'dices_et' in globals() and len(dices_tc) > 0 and len(dices_wt) > 0 and len(dices_et) > 0:\n",
        "        print(\"\\n[2. Performance Analysis by Class]\")\n",
        "        print(f\"\\n  TC (Tumor Core):\")\n",
        "        tc_best_idx = dices_tc.index(max(dices_tc)) if len(dices_tc) > 0 else 0\n",
        "        tc_best_epoch = trains_epoch[tc_best_idx] if 'trains_epoch' in globals() and len(trains_epoch) > tc_best_idx else 'N/A'\n",
        "        print(f\"    Best Dice: {max(dices_tc):.4f} (Epoch {tc_best_epoch})\")\n",
        "        print(f\"    Final Dice: {dices_tc[-1]:.4f}\")\n",
        "        print(f\"    Improvement: {(max(dices_tc) - dices_tc[0]) / dices_tc[0] * 100 if dices_tc[0] > 0 else 0:.1f}%\")\n",
        "\n",
        "        print(f\"\\n  WT (Whole Tumor):\")\n",
        "        wt_best_idx = dices_wt.index(max(dices_wt)) if len(dices_wt) > 0 else 0\n",
        "        wt_best_epoch = trains_epoch[wt_best_idx] if 'trains_epoch' in globals() and len(trains_epoch) > wt_best_idx else 'N/A'\n",
        "        print(f\"    Best Dice: {max(dices_wt):.4f} (Epoch {wt_best_epoch})\")\n",
        "        print(f\"    Final Dice: {dices_wt[-1]:.4f}\")\n",
        "        print(f\"    Improvement: {(max(dices_wt) - dices_wt[0]) / dices_wt[0] * 100 if dices_wt[0] > 0 else 0:.1f}%\")\n",
        "        print(f\"    Best performance, approaching clinically usable level (>=0.7)\")\n",
        "\n",
        "        print(f\"\\n  ET (Enhancing Tumor):\")\n",
        "        et_best_idx = dices_et.index(max(dices_et)) if len(dices_et) > 0 else 0\n",
        "        et_best_epoch = trains_epoch[et_best_idx] if 'trains_epoch' in globals() and len(trains_epoch) > et_best_idx else 'N/A'\n",
        "        print(f\"    Best Dice: {max(dices_et):.4f} (Epoch {et_best_epoch})\")\n",
        "        print(f\"    Final Dice: {dices_et[-1]:.4f}\")\n",
        "        print(f\"    Improvement: {(max(dices_et) - dices_et[0]) / dices_et[0] * 100 if dices_et[0] > 0 else 0:.1f}%\")\n",
        "        print(f\"    Warning: Worst performance, needs attention\")\n",
        "\n",
        "    # 3. Training Process Analysis\n",
        "    if len(dices_avg) > 0:\n",
        "        print(\"\\n[3. Training Process Analysis]\")\n",
        "        improvement = (val_acc_max - dices_avg[0]) / dices_avg[0] * 100 if dices_avg[0] > 0 else 0\n",
        "        print(f\"  Overall Improvement: {improvement:.1f}% (from {dices_avg[0]:.4f} to {val_acc_max:.4f})\")\n",
        "\n",
        "        # Check for overfitting\n",
        "        if len(dices_avg) > 1:\n",
        "            final_vs_best = (dices_avg[-1] / val_acc_max) * 100 if val_acc_max > 0 else 0\n",
        "            print(f\"  Final Performance vs Best Performance: {final_vs_best:.1f}%\")\n",
        "            if final_vs_best < 95:\n",
        "                print(f\"  Warning: Overfitting detected - final performance is {(100 - final_vs_best):.1f}% lower than best performance\")\n",
        "                print(f\"    Suggestion: Use model from Epoch {best_epoch_val} as final model\")\n",
        "\n",
        "        # Performance improvement stage analysis\n",
        "        if len(dices_avg) >= 3:\n",
        "            early_avg = np.mean(dices_avg[:len(dices_avg)//3])\n",
        "            mid_avg = np.mean(dices_avg[len(dices_avg)//3:2*len(dices_avg)//3])\n",
        "            late_avg = np.mean(dices_avg[2*len(dices_avg)//3:])\n",
        "            print(f\"\\n  Training Stage Analysis:\")\n",
        "            print(f\"    Early Average Dice: {early_avg:.4f}\")\n",
        "            print(f\"    Mid Average Dice: {mid_avg:.4f}\")\n",
        "            print(f\"    Late Average Dice: {late_avg:.4f}\")\n",
        "\n",
        "            if late_avg < mid_avg:\n",
        "                print(f\"    Warning: Performance decline in late stage, possible overfitting\")\n",
        "\n",
        "    # 4. Optimization Suggestions\n",
        "    print(\"\\n[4. Optimization Suggestions]\")\n",
        "    if 'dices_et' in globals() and len(dices_et) > 0 and max(dices_et) < 0.3:\n",
        "        print(\"  ‚Ä¢ ET class performance is poor (<0.3):\")\n",
        "        print(\"    - Consider increasing ET class loss weight\")\n",
        "        print(\"    - Use class-weighted Focal Loss\")\n",
        "        print(\"    - Increase data augmentation for ET\")\n",
        "        print(\"    - Consider using more complex post-processing\")\n",
        "\n",
        "    if 'dices_tc' in globals() and len(dices_tc) > 0 and max(dices_tc) < 0.5:\n",
        "        print(\"  ‚Ä¢ TC class has room for improvement (<0.5):\")\n",
        "        print(\"    - Adjust learning rate decay strategy\")\n",
        "        print(\"    - Increase training data or use data augmentation\")\n",
        "        print(\"    - Consider using pre-trained models\")\n",
        "\n",
        "    if len(dices_avg) > 0 and dices_avg[-1] < val_acc_max * 0.95:\n",
        "        print(\"  ‚Ä¢ Overfitting issue:\")\n",
        "        print(\"    - Early stopping mechanism has taken effect, use best model\")\n",
        "        print(\"    - Consider increasing Dropout or weight decay\")\n",
        "        print(\"    - Use data augmentation to reduce overfitting\")\n",
        "\n",
        "    print(\"\\n[5. Performance Evaluation]\")\n",
        "    if val_acc_max >= 0.4:\n",
        "        print(\"  Overall performance is good (Average Dice >= 0.4)\")\n",
        "    if 'dices_wt' in globals() and len(dices_wt) > 0 and max(dices_wt) >= 0.7:\n",
        "        print(\"  WT class reached clinically usable level (>=0.7)\")\n",
        "    if 'dices_tc' in globals() and len(dices_tc) > 0 and max(dices_tc) >= 0.5:\n",
        "        print(\"  TC class performance is acceptable (>=0.5)\")\n",
        "    if 'dices_et' in globals() and len(dices_et) > 0 and max(dices_et) >= 0.3:\n",
        "        print(\"  Warning: ET class needs improvement (<0.3)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeuABbB6r169"
      },
      "source": [
        "## Execute training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdbf9BZZr16-",
        "outputId": "6c3c6226-856a-4d1a-da0f-7d5c9c920d68",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ÂºÄÂßãËÆ≠ÁªÉ\n",
            "============================================================\n",
            "ÊúÄÂ§ßËΩÆÊï∞: 300\n",
            "È™åËØÅÈ¢ëÁéá: ÊØè 10 ‰∏™epoch\n",
            "Êó©ÂÅúËÄêÂøÉ: 50 ‰∏™epoch\n",
            "Êó©ÂÅúÊúÄÂ∞èÊîπËøõ: 0.0005\n",
            "============================================================\n",
            "\n",
            "Êó©ÂÅúÈÖçÁΩÆ: patience=50, min_delta=0.0005\n",
            "Thu Dec 18 03:16:13 2025 Epoch: 0\n",
            "Epoch 0/300 10/80 loss: 0.7891 time 19.41s\n",
            "Epoch 0/300 20/80 loss: 0.7866 time 29.60s\n",
            "Epoch 0/300 30/80 loss: 0.7850 time 22.43s\n",
            "Epoch 0/300 40/80 loss: 0.7833 time 16.18s\n",
            "Epoch 0/300 50/80 loss: 0.7816 time 22.51s\n",
            "Epoch 0/300 60/80 loss: 0.7791 time 21.37s\n",
            "Epoch 0/300 70/80 loss: 0.7770 time 18.43s\n",
            "Epoch 0/300 80/80 loss: 0.7749 time 16.24s\n",
            "Final training  0/299 loss: 0.7749 time 1927.79s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:226: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  win_data = torch.cat([inputs[win_slice] for win_slice in unravel_slice]).to(sw_device)\n",
            "/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:370: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  out[idx_zm] += p\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val 0/300 0/40 , dice_tc: 0.0025366198 , dice_wt: 0.07290706 , dice_et: 0.009891589 , time 28.24s\n",
            "Val 0/300 1/40 , dice_tc: 0.0015242207 , dice_wt: 0.11053904 , dice_et: 0.009891589 , time 16.36s\n",
            "Val 0/300 2/40 , dice_tc: 0.005028519 , dice_wt: 0.09816092 , dice_et: 0.024056863 , time 16.43s\n",
            "Val 0/300 3/40 , dice_tc: 0.0051304763 , dice_wt: 0.10465838 , dice_et: 0.021559134 , time 14.83s\n",
            "Val 0/300 4/40 , dice_tc: 0.004521882 , dice_wt: 0.09242536 , dice_et: 0.016423743 , time 10.81s\n",
            "Val 0/300 5/40 , dice_tc: 0.00495507 , dice_wt: 0.09793517 , dice_et: 0.017327772 , time 9.78s\n",
            "Val 0/300 6/40 , dice_tc: 0.004852208 , dice_wt: 0.090424955 , dice_et: 0.016455973 , time 15.68s\n",
            "Val 0/300 7/40 , dice_tc: 0.0045108153 , dice_wt: 0.08138382 , dice_et: 0.0146002155 , time 10.58s\n",
            "Val 0/300 8/40 , dice_tc: 0.004255048 , dice_wt: 0.079210296 , dice_et: 0.0136706205 , time 14.54s\n",
            "Val 0/300 9/40 , dice_tc: 0.0060765403 , dice_wt: 0.086762115 , dice_et: 0.016563449 , time 8.30s\n",
            "Val 0/300 10/40 , dice_tc: 0.005665357 , dice_wt: 0.08149088 , dice_et: 0.015538363 , time 8.72s\n",
            "Val 0/300 11/40 , dice_tc: 0.005274531 , dice_wt: 0.078842975 , dice_et: 0.014434561 , time 13.97s\n",
            "Val 0/300 12/40 , dice_tc: 0.0048727384 , dice_wt: 0.079680115 , dice_et: 0.014434561 , time 8.54s\n",
            "Val 0/300 13/40 , dice_tc: 0.00456364 , dice_wt: 0.07503327 , dice_et: 0.013358443 , time 8.66s\n",
            "Val 0/300 14/40 , dice_tc: 0.004304585 , dice_wt: 0.07555654 , dice_et: 0.012563332 , time 13.00s\n",
            "Val 0/300 15/40 , dice_tc: 0.004253643 , dice_wt: 0.07839894 , dice_et: 0.012244712 , time 14.41s\n",
            "Val 0/300 16/40 , dice_tc: 0.004648811 , dice_wt: 0.08328429 , dice_et: 0.013587675 , time 12.51s\n",
            "Val 0/300 17/40 , dice_tc: 0.004410185 , dice_wt: 0.083487004 , dice_et: 0.012810413 , time 8.85s\n",
            "Val 0/300 18/40 , dice_tc: 0.0049609207 , dice_wt: 0.08462413 , dice_et: 0.015382183 , time 14.24s\n",
            "Val 0/300 19/40 , dice_tc: 0.004840371 , dice_wt: 0.08290835 , dice_et: 0.015102171 , time 12.03s\n",
            "Val 0/300 20/40 , dice_tc: 0.0046552815 , dice_wt: 0.080962196 , dice_et: 0.014464835 , time 9.76s\n",
            "Val 0/300 21/40 , dice_tc: 0.0046627787 , dice_wt: 0.08057005 , dice_et: 0.014546998 , time 9.26s\n",
            "Val 0/300 22/40 , dice_tc: 0.0044789053 , dice_wt: 0.079721004 , dice_et: 0.013931255 , time 17.25s\n",
            "Val 0/300 23/40 , dice_tc: 0.0043458836 , dice_wt: 0.082558505 , dice_et: 0.013488156 , time 24.68s\n",
            "Val 0/300 24/40 , dice_tc: 0.00422508 , dice_wt: 0.082548305 , dice_et: 0.013140116 , time 17.56s\n",
            "Val 0/300 25/40 , dice_tc: 0.0040950966 , dice_wt: 0.084881656 , dice_et: 0.013140116 , time 17.01s\n",
            "Val 0/300 26/40 , dice_tc: 0.0041393517 , dice_wt: 0.08335422 , dice_et: 0.013279109 , time 14.18s\n",
            "Val 0/300 27/40 , dice_tc: 0.0040858733 , dice_wt: 0.084218256 , dice_et: 0.012833894 , time 14.16s\n",
            "Val 0/300 28/40 , dice_tc: 0.0039833677 , dice_wt: 0.083277754 , dice_et: 0.012493112 , time 13.60s\n",
            "Val 0/300 29/40 , dice_tc: 0.003855608 , dice_wt: 0.081467025 , dice_et: 0.0120534785 , time 10.18s\n",
            "Val 0/300 30/40 , dice_tc: 0.0037818511 , dice_wt: 0.08206809 , dice_et: 0.011653709 , time 15.10s\n",
            "Val 0/300 31/40 , dice_tc: 0.0038004345 , dice_wt: 0.08210822 , dice_et: 0.011663387 , time 14.61s\n",
            "Val 0/300 32/40 , dice_tc: 0.003686217 , dice_wt: 0.07999388 , dice_et: 0.011663387 , time 10.91s\n",
            "Val 0/300 33/40 , dice_tc: 0.0036881065 , dice_wt: 0.08019131 , dice_et: 0.011757013 , time 15.68s\n",
            "Val 0/300 34/40 , dice_tc: 0.003633327 , dice_wt: 0.078451596 , dice_et: 0.01154079 , time 9.71s\n",
            "Val 0/300 35/40 , dice_tc: 0.0036234828 , dice_wt: 0.077743396 , dice_et: 0.011479409 , time 12.90s\n",
            "Val 0/300 36/40 , dice_tc: 0.00406107 , dice_wt: 0.07696287 , dice_et: 0.01327452 , time 8.16s\n",
            "Val 0/300 37/40 , dice_tc: 0.0039654383 , dice_wt: 0.07796315 , dice_et: 0.012928951 , time 13.05s\n",
            "Val 0/300 38/40 , dice_tc: 0.0039974493 , dice_wt: 0.078815974 , dice_et: 0.01302656 , time 14.64s\n",
            "Val 0/300 39/40 , dice_tc: 0.0039005089 , dice_wt: 0.077724524 , dice_et: 0.01302656 , time 18.37s\n",
            "Final validation stats 0/299 , dice_tc: 0.0039005089 , dice_wt: 0.077724524 , dice_et: 0.01302656 , Dice_Avg: 0.03155053 , LR: 0.000003 , time 541.26s\n",
            "new best (0.000000 --> 0.031551). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 03:57:47 2025 Epoch: 1\n",
            "Epoch 1/300 10/80 loss: 0.7621 time 2.93s\n",
            "Epoch 1/300 20/80 loss: 0.7580 time 3.06s\n",
            "Epoch 1/300 30/80 loss: 0.7544 time 2.86s\n",
            "Epoch 1/300 40/80 loss: 0.7522 time 2.81s\n",
            "Epoch 1/300 50/80 loss: 0.7495 time 2.82s\n",
            "Epoch 1/300 60/80 loss: 0.7474 time 3.14s\n",
            "Epoch 1/300 70/80 loss: 0.7460 time 2.86s\n",
            "Epoch 1/300 80/80 loss: 0.7440 time 1.65s\n",
            "Final training  1/299 loss: 0.7440 time 227.83s\n",
            "Thu Dec 18 04:01:35 2025 Epoch: 2\n",
            "Epoch 2/300 10/80 loss: 0.7283 time 2.96s\n",
            "Epoch 2/300 20/80 loss: 0.7294 time 3.07s\n",
            "Epoch 2/300 30/80 loss: 0.7285 time 2.85s\n",
            "Epoch 2/300 40/80 loss: 0.7266 time 2.86s\n",
            "Epoch 2/300 50/80 loss: 0.7266 time 3.02s\n",
            "Epoch 2/300 60/80 loss: 0.7262 time 2.68s\n",
            "Epoch 2/300 70/80 loss: 0.7257 time 3.08s\n",
            "Epoch 2/300 80/80 loss: 0.7252 time 1.64s\n",
            "Final training  2/299 loss: 0.7252 time 226.79s\n",
            "Thu Dec 18 04:05:22 2025 Epoch: 3\n",
            "Epoch 3/300 10/80 loss: 0.7181 time 2.80s\n",
            "Epoch 3/300 20/80 loss: 0.7195 time 2.86s\n",
            "Epoch 3/300 30/80 loss: 0.7177 time 2.94s\n",
            "Epoch 3/300 40/80 loss: 0.7185 time 3.01s\n",
            "Epoch 3/300 50/80 loss: 0.7180 time 2.80s\n",
            "Epoch 3/300 60/80 loss: 0.7180 time 2.88s\n",
            "Epoch 3/300 70/80 loss: 0.7182 time 3.05s\n",
            "Epoch 3/300 80/80 loss: 0.7168 time 1.40s\n",
            "Final training  3/299 loss: 0.7168 time 226.27s\n",
            "Thu Dec 18 04:09:08 2025 Epoch: 4\n",
            "Epoch 4/300 10/80 loss: 0.7064 time 3.10s\n",
            "Epoch 4/300 20/80 loss: 0.7085 time 3.01s\n",
            "Epoch 4/300 30/80 loss: 0.7115 time 2.83s\n",
            "Epoch 4/300 40/80 loss: 0.7123 time 2.79s\n",
            "Epoch 4/300 50/80 loss: 0.7136 time 3.06s\n",
            "Epoch 4/300 60/80 loss: 0.7141 time 3.02s\n",
            "Epoch 4/300 70/80 loss: 0.7140 time 2.80s\n",
            "Epoch 4/300 80/80 loss: 0.7131 time 1.46s\n",
            "Final training  4/299 loss: 0.7131 time 225.09s\n",
            "Thu Dec 18 04:12:53 2025 Epoch: 5\n",
            "Epoch 5/300 10/80 loss: 0.7094 time 3.03s\n",
            "Epoch 5/300 20/80 loss: 0.7117 time 2.85s\n",
            "Epoch 5/300 30/80 loss: 0.7103 time 2.83s\n",
            "Epoch 5/300 40/80 loss: 0.7094 time 3.02s\n",
            "Epoch 5/300 50/80 loss: 0.7081 time 3.02s\n",
            "Epoch 5/300 60/80 loss: 0.7078 time 3.03s\n",
            "Epoch 5/300 70/80 loss: 0.7087 time 3.06s\n",
            "Epoch 5/300 80/80 loss: 0.7080 time 1.62s\n",
            "Final training  5/299 loss: 0.7080 time 225.56s\n",
            "Thu Dec 18 04:16:39 2025 Epoch: 6\n",
            "Epoch 6/300 10/80 loss: 0.7033 time 2.87s\n",
            "Epoch 6/300 20/80 loss: 0.7005 time 3.06s\n",
            "Epoch 6/300 30/80 loss: 0.7049 time 2.81s\n",
            "Epoch 6/300 40/80 loss: 0.7052 time 2.84s\n",
            "Epoch 6/300 50/80 loss: 0.7055 time 2.87s\n",
            "Epoch 6/300 60/80 loss: 0.7061 time 3.01s\n",
            "Epoch 6/300 70/80 loss: 0.7046 time 2.87s\n",
            "Epoch 6/300 80/80 loss: 0.7051 time 1.44s\n",
            "Final training  6/299 loss: 0.7051 time 226.45s\n",
            "Thu Dec 18 04:20:25 2025 Epoch: 7\n",
            "Epoch 7/300 10/80 loss: 0.7048 time 3.01s\n",
            "Epoch 7/300 20/80 loss: 0.7036 time 3.01s\n",
            "Epoch 7/300 30/80 loss: 0.7045 time 2.62s\n",
            "Epoch 7/300 40/80 loss: 0.7051 time 3.01s\n",
            "Epoch 7/300 50/80 loss: 0.7046 time 3.05s\n",
            "Epoch 7/300 60/80 loss: 0.7039 time 3.03s\n",
            "Epoch 7/300 70/80 loss: 0.7035 time 3.15s\n",
            "Epoch 7/300 80/80 loss: 0.7036 time 1.43s\n",
            "Final training  7/299 loss: 0.7036 time 227.47s\n",
            "Thu Dec 18 04:24:13 2025 Epoch: 8\n",
            "Epoch 8/300 10/80 loss: 0.6926 time 2.66s\n",
            "Epoch 8/300 20/80 loss: 0.6950 time 2.93s\n",
            "Epoch 8/300 30/80 loss: 0.6993 time 2.78s\n",
            "Epoch 8/300 40/80 loss: 0.7009 time 2.61s\n",
            "Epoch 8/300 50/80 loss: 0.7000 time 2.73s\n",
            "Epoch 8/300 60/80 loss: 0.6997 time 2.68s\n",
            "Epoch 8/300 70/80 loss: 0.6999 time 2.88s\n",
            "Epoch 8/300 80/80 loss: 0.7003 time 1.65s\n",
            "Final training  8/299 loss: 0.7003 time 227.41s\n",
            "Thu Dec 18 04:28:00 2025 Epoch: 9\n",
            "Epoch 9/300 10/80 loss: 0.6986 time 2.65s\n",
            "Epoch 9/300 20/80 loss: 0.6964 time 3.07s\n",
            "Epoch 9/300 30/80 loss: 0.6955 time 2.80s\n",
            "Epoch 9/300 40/80 loss: 0.6967 time 2.84s\n",
            "Epoch 9/300 50/80 loss: 0.6981 time 2.66s\n",
            "Epoch 9/300 60/80 loss: 0.6989 time 2.67s\n",
            "Epoch 9/300 70/80 loss: 0.6981 time 3.06s\n",
            "Epoch 9/300 80/80 loss: 0.6980 time 1.44s\n",
            "Final training  9/299 loss: 0.6980 time 227.66s\n",
            "Val 9/300 0/40 , dice_tc: 0.13720585 , dice_wt: 0.59729534 , dice_et: 0.23211043 , time 2.75s\n",
            "Val 9/300 1/40 , dice_tc: 0.08182532 , dice_wt: 0.69973135 , dice_et: 0.23211043 , time 2.77s\n",
            "Val 9/300 2/40 , dice_tc: 0.205737 , dice_wt: 0.68026495 , dice_et: 0.42104313 , time 2.73s\n",
            "Val 9/300 3/40 , dice_tc: 0.21665064 , dice_wt: 0.69462323 , dice_et: 0.3988334 , time 2.66s\n",
            "Val 9/300 4/40 , dice_tc: 0.1791814 , dice_wt: 0.61941826 , dice_et: 0.30036792 , time 2.54s\n",
            "Val 9/300 5/40 , dice_tc: 0.18049355 , dice_wt: 0.6231745 , dice_et: 0.29318017 , time 2.54s\n",
            "Val 9/300 6/40 , dice_tc: 0.18354678 , dice_wt: 0.58745635 , dice_et: 0.29419395 , time 2.77s\n",
            "Val 9/300 7/40 , dice_tc: 0.168922 , dice_wt: 0.5362102 , dice_et: 0.26757285 , time 2.56s\n",
            "Val 9/300 8/40 , dice_tc: 0.16317612 , dice_wt: 0.5293124 , dice_et: 0.25956818 , time 2.76s\n",
            "Val 9/300 9/40 , dice_tc: 0.18796813 , dice_wt: 0.5520427 , dice_et: 0.2744232 , time 2.57s\n",
            "Val 9/300 10/40 , dice_tc: 0.1787137 , dice_wt: 0.52921546 , dice_et: 0.25887623 , time 2.48s\n",
            "Val 9/300 11/40 , dice_tc: 0.170268 , dice_wt: 0.5185941 , dice_et: 0.24537793 , time 2.72s\n",
            "Val 9/300 12/40 , dice_tc: 0.15728992 , dice_wt: 0.5239798 , dice_et: 0.24537793 , time 2.53s\n",
            "Val 9/300 13/40 , dice_tc: 0.14800893 , dice_wt: 0.5003566 , dice_et: 0.22854531 , time 2.53s\n",
            "Val 9/300 14/40 , dice_tc: 0.14030676 , dice_wt: 0.5061139 , dice_et: 0.21205014 , time 2.64s\n",
            "Val 9/300 15/40 , dice_tc: 0.14165881 , dice_wt: 0.52015793 , dice_et: 0.21514659 , time 2.77s\n",
            "Val 9/300 16/40 , dice_tc: 0.15328139 , dice_wt: 0.53427094 , dice_et: 0.22915435 , time 2.73s\n",
            "Val 9/300 17/40 , dice_tc: 0.1454639 , dice_wt: 0.5376493 , dice_et: 0.21599527 , time 2.46s\n",
            "Val 9/300 18/40 , dice_tc: 0.16234846 , dice_wt: 0.54285413 , dice_et: 0.23681478 , time 2.70s\n",
            "Val 9/300 19/40 , dice_tc: 0.16136472 , dice_wt: 0.5366809 , dice_et: 0.23637658 , time 2.66s\n",
            "Val 9/300 20/40 , dice_tc: 0.15692584 , dice_wt: 0.5333128 , dice_et: 0.22948621 , time 2.53s\n",
            "Val 9/300 21/40 , dice_tc: 0.160455 , dice_wt: 0.53818274 , dice_et: 0.23290762 , time 2.49s\n",
            "Val 9/300 22/40 , dice_tc: 0.1544525 , dice_wt: 0.5362323 , dice_et: 0.22340003 , time 2.71s\n",
            "Val 9/300 23/40 , dice_tc: 0.15110306 , dice_wt: 0.5447636 , dice_et: 0.21801926 , time 2.76s\n",
            "Val 9/300 24/40 , dice_tc: 0.14780943 , dice_wt: 0.5485955 , dice_et: 0.21378131 , time 2.73s\n",
            "Val 9/300 25/40 , dice_tc: 0.14347053 , dice_wt: 0.554486 , dice_et: 0.21378131 , time 2.75s\n",
            "Val 9/300 26/40 , dice_tc: 0.14908598 , dice_wt: 0.5553367 , dice_et: 0.22199197 , time 2.75s\n",
            "Val 9/300 27/40 , dice_tc: 0.14837588 , dice_wt: 0.55824876 , dice_et: 0.21677016 , time 2.74s\n",
            "Val 9/300 28/40 , dice_tc: 0.14622281 , dice_wt: 0.553273 , dice_et: 0.21339718 , time 2.66s\n",
            "Val 9/300 29/40 , dice_tc: 0.14160436 , dice_wt: 0.5466617 , dice_et: 0.20600025 , time 2.60s\n",
            "Val 9/300 30/40 , dice_tc: 0.13989648 , dice_wt: 0.5490766 , dice_et: 0.199402 , time 2.69s\n",
            "Val 9/300 31/40 , dice_tc: 0.14392483 , dice_wt: 0.5523774 , dice_et: 0.20562391 , time 2.65s\n",
            "Val 9/300 32/40 , dice_tc: 0.1395697 , dice_wt: 0.5383437 , dice_et: 0.20562391 , time 2.54s\n",
            "Val 9/300 33/40 , dice_tc: 0.14057216 , dice_wt: 0.53865075 , dice_et: 0.20785181 , time 2.71s\n",
            "Val 9/300 34/40 , dice_tc: 0.1386769 , dice_wt: 0.52858955 , dice_et: 0.20382692 , time 2.58s\n",
            "Val 9/300 35/40 , dice_tc: 0.13911109 , dice_wt: 0.52398133 , dice_et: 0.20514181 , time 2.73s\n",
            "Val 9/300 36/40 , dice_tc: 0.15083505 , dice_wt: 0.52617085 , dice_et: 0.21835779 , time 2.53s\n",
            "Val 9/300 37/40 , dice_tc: 0.14740025 , dice_wt: 0.52894956 , dice_et: 0.21305726 , time 2.72s\n",
            "Val 9/300 38/40 , dice_tc: 0.15047216 , dice_wt: 0.53255916 , dice_et: 0.21833451 , time 2.74s\n",
            "Val 9/300 39/40 , dice_tc: 0.14683029 , dice_wt: 0.5271613 , dice_et: 0.21833451 , time 2.69s\n",
            "Final validation stats 9/299 , dice_tc: 0.14683029 , dice_wt: 0.5271613 , dice_et: 0.21833451 , Dice_Avg: 0.29744202 , LR: 0.000005 , time 106.18s\n",
            "new best (0.031551 --> 0.297442). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 04:33:36 2025 Epoch: 10\n",
            "Epoch 10/300 10/80 loss: 0.6946 time 3.14s\n",
            "Epoch 10/300 20/80 loss: 0.6983 time 3.17s\n",
            "Epoch 10/300 30/80 loss: 0.6994 time 3.07s\n",
            "Epoch 10/300 40/80 loss: 0.6991 time 2.79s\n",
            "Epoch 10/300 50/80 loss: 0.6988 time 3.06s\n",
            "Epoch 10/300 60/80 loss: 0.6982 time 2.87s\n",
            "Epoch 10/300 70/80 loss: 0.6986 time 3.02s\n",
            "Epoch 10/300 80/80 loss: 0.6974 time 1.48s\n",
            "Final training  10/299 loss: 0.6974 time 228.75s\n",
            "Thu Dec 18 04:37:25 2025 Epoch: 11\n",
            "Epoch 11/300 10/80 loss: 0.6914 time 2.65s\n",
            "Epoch 11/300 20/80 loss: 0.6894 time 3.06s\n",
            "Epoch 11/300 30/80 loss: 0.6922 time 2.90s\n",
            "Epoch 11/300 40/80 loss: 0.6928 time 2.81s\n",
            "Epoch 11/300 50/80 loss: 0.6919 time 2.83s\n",
            "Epoch 11/300 60/80 loss: 0.6931 time 2.68s\n",
            "Epoch 11/300 70/80 loss: 0.6928 time 2.89s\n",
            "Epoch 11/300 80/80 loss: 0.6926 time 1.66s\n",
            "Final training  11/299 loss: 0.6926 time 226.82s\n",
            "Thu Dec 18 04:41:12 2025 Epoch: 12\n",
            "Epoch 12/300 10/80 loss: 0.6892 time 2.80s\n",
            "Epoch 12/300 20/80 loss: 0.6922 time 2.92s\n",
            "Epoch 12/300 30/80 loss: 0.6911 time 3.10s\n",
            "Epoch 12/300 40/80 loss: 0.6928 time 3.09s\n",
            "Epoch 12/300 50/80 loss: 0.6934 time 3.07s\n",
            "Epoch 12/300 60/80 loss: 0.6947 time 3.01s\n",
            "Epoch 12/300 70/80 loss: 0.6938 time 2.84s\n",
            "Epoch 12/300 80/80 loss: 0.6913 time 1.67s\n",
            "Final training  12/299 loss: 0.6913 time 226.62s\n",
            "Thu Dec 18 04:44:58 2025 Epoch: 13\n",
            "Epoch 13/300 10/80 loss: 0.6929 time 2.82s\n",
            "Epoch 13/300 20/80 loss: 0.6866 time 2.99s\n",
            "Epoch 13/300 30/80 loss: 0.6876 time 2.75s\n",
            "Epoch 13/300 40/80 loss: 0.6910 time 2.68s\n",
            "Epoch 13/300 50/80 loss: 0.6917 time 2.82s\n",
            "Epoch 13/300 60/80 loss: 0.6906 time 2.93s\n",
            "Epoch 13/300 70/80 loss: 0.6897 time 3.00s\n",
            "Epoch 13/300 80/80 loss: 0.6902 time 1.44s\n",
            "Final training  13/299 loss: 0.6902 time 226.93s\n",
            "Thu Dec 18 04:48:45 2025 Epoch: 14\n",
            "Epoch 14/300 10/80 loss: 0.6784 time 2.82s\n",
            "Epoch 14/300 20/80 loss: 0.6854 time 2.86s\n",
            "Epoch 14/300 30/80 loss: 0.6891 time 3.09s\n",
            "Epoch 14/300 40/80 loss: 0.6914 time 2.76s\n",
            "Epoch 14/300 50/80 loss: 0.6891 time 2.66s\n",
            "Epoch 14/300 60/80 loss: 0.6894 time 3.06s\n",
            "Epoch 14/300 70/80 loss: 0.6889 time 2.83s\n",
            "Epoch 14/300 80/80 loss: 0.6880 time 1.66s\n",
            "Final training  14/299 loss: 0.6880 time 228.07s\n",
            "Thu Dec 18 04:52:33 2025 Epoch: 15\n",
            "Epoch 15/300 10/80 loss: 0.7011 time 3.05s\n",
            "Epoch 15/300 20/80 loss: 0.6989 time 2.68s\n",
            "Epoch 15/300 30/80 loss: 0.6960 time 2.82s\n",
            "Epoch 15/300 40/80 loss: 0.6942 time 2.81s\n",
            "Epoch 15/300 50/80 loss: 0.6924 time 2.98s\n",
            "Epoch 15/300 60/80 loss: 0.6913 time 2.94s\n",
            "Epoch 15/300 70/80 loss: 0.6899 time 2.85s\n",
            "Epoch 15/300 80/80 loss: 0.6894 time 1.69s\n",
            "Final training  15/299 loss: 0.6894 time 227.83s\n",
            "Thu Dec 18 04:56:21 2025 Epoch: 16\n",
            "Epoch 16/300 10/80 loss: 0.6905 time 2.86s\n",
            "Epoch 16/300 20/80 loss: 0.6921 time 2.78s\n",
            "Epoch 16/300 30/80 loss: 0.6863 time 3.14s\n",
            "Epoch 16/300 40/80 loss: 0.6864 time 2.91s\n",
            "Epoch 16/300 50/80 loss: 0.6861 time 2.84s\n",
            "Epoch 16/300 60/80 loss: 0.6857 time 3.08s\n",
            "Epoch 16/300 70/80 loss: 0.6869 time 2.90s\n",
            "Epoch 16/300 80/80 loss: 0.6861 time 1.67s\n",
            "Final training  16/299 loss: 0.6861 time 227.10s\n",
            "Thu Dec 18 05:00:08 2025 Epoch: 17\n",
            "Epoch 17/300 10/80 loss: 0.6882 time 2.69s\n",
            "Epoch 17/300 20/80 loss: 0.6886 time 3.12s\n",
            "Epoch 17/300 30/80 loss: 0.6872 time 2.66s\n",
            "Epoch 17/300 40/80 loss: 0.6897 time 3.02s\n",
            "Epoch 17/300 50/80 loss: 0.6895 time 2.84s\n",
            "Epoch 17/300 60/80 loss: 0.6889 time 3.07s\n",
            "Epoch 17/300 70/80 loss: 0.6868 time 2.63s\n",
            "Epoch 17/300 80/80 loss: 0.6848 time 1.58s\n",
            "Final training  17/299 loss: 0.6848 time 227.69s\n",
            "Thu Dec 18 05:03:56 2025 Epoch: 18\n",
            "Epoch 18/300 10/80 loss: 0.6804 time 2.91s\n",
            "Epoch 18/300 20/80 loss: 0.6831 time 2.83s\n",
            "Epoch 18/300 30/80 loss: 0.6826 time 3.03s\n",
            "Epoch 18/300 40/80 loss: 0.6864 time 2.75s\n",
            "Epoch 18/300 50/80 loss: 0.6863 time 2.93s\n",
            "Epoch 18/300 60/80 loss: 0.6847 time 2.89s\n",
            "Epoch 18/300 70/80 loss: 0.6834 time 3.14s\n",
            "Epoch 18/300 80/80 loss: 0.6825 time 1.64s\n",
            "Final training  18/299 loss: 0.6825 time 227.78s\n",
            "Thu Dec 18 05:07:44 2025 Epoch: 19\n",
            "Epoch 19/300 10/80 loss: 0.6961 time 3.02s\n",
            "Epoch 19/300 20/80 loss: 0.6825 time 3.14s\n",
            "Epoch 19/300 30/80 loss: 0.6831 time 2.79s\n",
            "Epoch 19/300 40/80 loss: 0.6845 time 3.00s\n",
            "Epoch 19/300 50/80 loss: 0.6850 time 2.99s\n",
            "Epoch 19/300 60/80 loss: 0.6847 time 3.09s\n",
            "Epoch 19/300 70/80 loss: 0.6840 time 2.84s\n",
            "Epoch 19/300 80/80 loss: 0.6827 time 1.48s\n",
            "Final training  19/299 loss: 0.6827 time 227.07s\n",
            "Val 19/300 0/40 , dice_tc: 0.38826492 , dice_wt: 0.7295457 , dice_et: 0.5069127 , time 2.76s\n",
            "Val 19/300 1/40 , dice_tc: 0.2922032 , dice_wt: 0.8006846 , dice_et: 0.5069127 , time 2.71s\n",
            "Val 19/300 2/40 , dice_tc: 0.46345386 , dice_wt: 0.8081687 , dice_et: 0.6911417 , time 2.69s\n",
            "Val 19/300 3/40 , dice_tc: 0.5090984 , dice_wt: 0.8198869 , dice_et: 0.71237224 , time 2.69s\n",
            "Val 19/300 4/40 , dice_tc: 0.4416131 , dice_wt: 0.74382776 , dice_et: 0.5505769 , time 2.58s\n",
            "Val 19/300 5/40 , dice_tc: 0.4504521 , dice_wt: 0.74639434 , dice_et: 0.55643016 , time 2.57s\n",
            "Val 19/300 6/40 , dice_tc: 0.47545052 , dice_wt: 0.7426227 , dice_et: 0.58818394 , time 2.71s\n",
            "Val 19/300 7/40 , dice_tc: 0.4530634 , dice_wt: 0.71492565 , dice_et: 0.57180256 , time 2.54s\n",
            "Val 19/300 8/40 , dice_tc: 0.45160356 , dice_wt: 0.7158649 , dice_et: 0.57913697 , time 2.76s\n",
            "Val 19/300 9/40 , dice_tc: 0.4734908 , dice_wt: 0.7265338 , dice_et: 0.59372777 , time 2.62s\n",
            "Val 19/300 10/40 , dice_tc: 0.46319902 , dice_wt: 0.716595 , dice_et: 0.5850052 , time 2.50s\n",
            "Val 19/300 11/40 , dice_tc: 0.4468609 , dice_wt: 0.70922 , dice_et: 0.56443346 , time 2.69s\n",
            "Val 19/300 12/40 , dice_tc: 0.41315496 , dice_wt: 0.7194034 , dice_et: 0.56443346 , time 2.53s\n",
            "Val 19/300 13/40 , dice_tc: 0.39953536 , dice_wt: 0.6993834 , dice_et: 0.54895574 , time 2.53s\n",
            "Val 19/300 14/40 , dice_tc: 0.3817929 , dice_wt: 0.7036868 , dice_et: 0.5130521 , time 2.67s\n",
            "Val 19/300 15/40 , dice_tc: 0.38211447 , dice_wt: 0.708214 , dice_et: 0.51253456 , time 2.73s\n",
            "Val 19/300 16/40 , dice_tc: 0.39451706 , dice_wt: 0.71402276 , dice_et: 0.5198624 , time 2.70s\n",
            "Val 19/300 17/40 , dice_tc: 0.37759632 , dice_wt: 0.716938 , dice_et: 0.49830112 , time 2.55s\n",
            "Val 19/300 18/40 , dice_tc: 0.39959568 , dice_wt: 0.7195445 , dice_et: 0.5208951 , time 2.81s\n",
            "Val 19/300 19/40 , dice_tc: 0.40120086 , dice_wt: 0.7157546 , dice_et: 0.52357686 , time 2.71s\n",
            "Val 19/300 20/40 , dice_tc: 0.39595324 , dice_wt: 0.7153976 , dice_et: 0.5186922 , time 2.46s\n",
            "Val 19/300 21/40 , dice_tc: 0.41065553 , dice_wt: 0.7191333 , dice_et: 0.5342188 , time 2.49s\n",
            "Val 19/300 22/40 , dice_tc: 0.39658406 , dice_wt: 0.71916837 , dice_et: 0.5146576 , time 2.67s\n",
            "Val 19/300 23/40 , dice_tc: 0.38788262 , dice_wt: 0.7233429 , dice_et: 0.50335217 , time 2.81s\n",
            "Val 19/300 24/40 , dice_tc: 0.38061637 , dice_wt: 0.7264508 , dice_et: 0.4928984 , time 2.74s\n",
            "Val 19/300 25/40 , dice_tc: 0.36887297 , dice_wt: 0.7285659 , dice_et: 0.4928984 , time 2.73s\n",
            "Val 19/300 26/40 , dice_tc: 0.3808657 , dice_wt: 0.7302524 , dice_et: 0.50557333 , time 2.73s\n",
            "Val 19/300 27/40 , dice_tc: 0.3757818 , dice_wt: 0.72934246 , dice_et: 0.49218562 , time 2.77s\n",
            "Val 19/300 28/40 , dice_tc: 0.3732546 , dice_wt: 0.7294014 , dice_et: 0.48940066 , time 2.66s\n",
            "Val 19/300 29/40 , dice_tc: 0.36190414 , dice_wt: 0.7285586 , dice_et: 0.4741914 , time 2.48s\n",
            "Val 19/300 30/40 , dice_tc: 0.35788462 , dice_wt: 0.72938985 , dice_et: 0.4597265 , time 2.68s\n",
            "Val 19/300 31/40 , dice_tc: 0.36605516 , dice_wt: 0.731893 , dice_et: 0.46848047 , time 2.69s\n",
            "Val 19/300 32/40 , dice_tc: 0.3549647 , dice_wt: 0.7246237 , dice_et: 0.46848047 , time 2.62s\n",
            "Val 19/300 33/40 , dice_tc: 0.35920677 , dice_wt: 0.7238997 , dice_et: 0.4748761 , time 2.70s\n",
            "Val 19/300 34/40 , dice_tc: 0.36149248 , dice_wt: 0.71994257 , dice_et: 0.4761381 , time 2.46s\n",
            "Val 19/300 35/40 , dice_tc: 0.3659617 , dice_wt: 0.7185908 , dice_et: 0.48290542 , time 2.71s\n",
            "Val 19/300 36/40 , dice_tc: 0.38013592 , dice_wt: 0.7202451 , dice_et: 0.49619433 , time 2.53s\n",
            "Val 19/300 37/40 , dice_tc: 0.37361073 , dice_wt: 0.72264427 , dice_et: 0.48958582 , time 2.77s\n",
            "Val 19/300 38/40 , dice_tc: 0.3810473 , dice_wt: 0.72421443 , dice_et: 0.4973582 , time 2.67s\n",
            "Val 19/300 39/40 , dice_tc: 0.37213722 , dice_wt: 0.7211319 , dice_et: 0.4973582 , time 2.69s\n",
            "Final validation stats 19/299 , dice_tc: 0.37213722 , dice_wt: 0.7211319 , dice_et: 0.4973582 , Dice_Avg: 0.5302091 , LR: 0.000008 , time 106.11s\n",
            "new best (0.297442 --> 0.530209). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 05:13:19 2025 Epoch: 20\n",
            "Epoch 20/300 10/80 loss: 0.6933 time 3.18s\n",
            "Epoch 20/300 20/80 loss: 0.6909 time 3.04s\n",
            "Epoch 20/300 30/80 loss: 0.6846 time 3.07s\n",
            "Epoch 20/300 40/80 loss: 0.6817 time 3.07s\n",
            "Epoch 20/300 50/80 loss: 0.6830 time 2.99s\n",
            "Epoch 20/300 60/80 loss: 0.6826 time 2.85s\n",
            "Epoch 20/300 70/80 loss: 0.6842 time 3.12s\n",
            "Epoch 20/300 80/80 loss: 0.6839 time 1.63s\n",
            "Final training  20/299 loss: 0.6839 time 227.32s\n",
            "Thu Dec 18 05:17:07 2025 Epoch: 21\n",
            "Epoch 21/300 10/80 loss: 0.6845 time 3.08s\n",
            "Epoch 21/300 20/80 loss: 0.6838 time 2.82s\n",
            "Epoch 21/300 30/80 loss: 0.6818 time 3.07s\n",
            "Epoch 21/300 40/80 loss: 0.6791 time 3.02s\n",
            "Epoch 21/300 50/80 loss: 0.6806 time 2.95s\n",
            "Epoch 21/300 60/80 loss: 0.6797 time 2.81s\n",
            "Epoch 21/300 70/80 loss: 0.6798 time 2.83s\n",
            "Epoch 21/300 80/80 loss: 0.6809 time 1.63s\n",
            "Final training  21/299 loss: 0.6809 time 226.81s\n",
            "Thu Dec 18 05:20:54 2025 Epoch: 22\n",
            "Epoch 22/300 10/80 loss: 0.6794 time 2.72s\n",
            "Epoch 22/300 20/80 loss: 0.6787 time 2.86s\n",
            "Epoch 22/300 30/80 loss: 0.6790 time 2.80s\n",
            "Epoch 22/300 40/80 loss: 0.6782 time 2.76s\n",
            "Epoch 22/300 50/80 loss: 0.6785 time 3.01s\n",
            "Epoch 22/300 60/80 loss: 0.6778 time 3.01s\n",
            "Epoch 22/300 70/80 loss: 0.6788 time 3.11s\n",
            "Epoch 22/300 80/80 loss: 0.6771 time 1.41s\n",
            "Final training  22/299 loss: 0.6771 time 226.49s\n",
            "Thu Dec 18 05:24:40 2025 Epoch: 23\n",
            "Epoch 23/300 10/80 loss: 0.6677 time 2.90s\n",
            "Epoch 23/300 20/80 loss: 0.6734 time 2.81s\n",
            "Epoch 23/300 30/80 loss: 0.6722 time 3.03s\n",
            "Epoch 23/300 40/80 loss: 0.6748 time 2.81s\n",
            "Epoch 23/300 50/80 loss: 0.6758 time 2.86s\n",
            "Epoch 23/300 60/80 loss: 0.6778 time 2.93s\n",
            "Epoch 23/300 70/80 loss: 0.6755 time 2.98s\n",
            "Epoch 23/300 80/80 loss: 0.6733 time 1.67s\n",
            "Final training  23/299 loss: 0.6733 time 225.82s\n",
            "Thu Dec 18 05:28:26 2025 Epoch: 24\n",
            "Epoch 24/300 10/80 loss: 0.6816 time 3.07s\n",
            "Epoch 24/300 20/80 loss: 0.6754 time 2.84s\n",
            "Epoch 24/300 30/80 loss: 0.6752 time 2.86s\n",
            "Epoch 24/300 40/80 loss: 0.6745 time 2.67s\n",
            "Epoch 24/300 50/80 loss: 0.6719 time 3.03s\n",
            "Epoch 24/300 60/80 loss: 0.6731 time 2.59s\n",
            "Epoch 24/300 70/80 loss: 0.6722 time 2.87s\n",
            "Epoch 24/300 80/80 loss: 0.6731 time 1.45s\n",
            "Final training  24/299 loss: 0.6731 time 226.33s\n",
            "Thu Dec 18 05:32:12 2025 Epoch: 25\n",
            "Epoch 25/300 10/80 loss: 0.6532 time 2.99s\n",
            "Epoch 25/300 20/80 loss: 0.6607 time 3.05s\n",
            "Epoch 25/300 30/80 loss: 0.6657 time 3.14s\n",
            "Epoch 25/300 40/80 loss: 0.6652 time 2.80s\n",
            "Epoch 25/300 50/80 loss: 0.6670 time 3.10s\n",
            "Epoch 25/300 60/80 loss: 0.6683 time 3.07s\n",
            "Epoch 25/300 70/80 loss: 0.6693 time 2.83s\n",
            "Epoch 25/300 80/80 loss: 0.6698 time 1.59s\n",
            "Final training  25/299 loss: 0.6698 time 226.05s\n",
            "Thu Dec 18 05:35:58 2025 Epoch: 26\n",
            "Epoch 26/300 10/80 loss: 0.6759 time 3.09s\n",
            "Epoch 26/300 20/80 loss: 0.6783 time 2.83s\n",
            "Epoch 26/300 30/80 loss: 0.6774 time 2.80s\n",
            "Epoch 26/300 40/80 loss: 0.6756 time 2.59s\n",
            "Epoch 26/300 50/80 loss: 0.6758 time 3.05s\n",
            "Epoch 26/300 60/80 loss: 0.6758 time 3.02s\n",
            "Epoch 26/300 70/80 loss: 0.6716 time 2.84s\n",
            "Epoch 26/300 80/80 loss: 0.6707 time 1.65s\n",
            "Final training  26/299 loss: 0.6707 time 225.52s\n",
            "Thu Dec 18 05:39:44 2025 Epoch: 27\n",
            "Epoch 27/300 10/80 loss: 0.6717 time 2.83s\n",
            "Epoch 27/300 20/80 loss: 0.6703 time 2.88s\n",
            "Epoch 27/300 30/80 loss: 0.6709 time 2.86s\n",
            "Epoch 27/300 40/80 loss: 0.6707 time 2.84s\n",
            "Epoch 27/300 50/80 loss: 0.6679 time 2.64s\n",
            "Epoch 27/300 60/80 loss: 0.6674 time 2.85s\n",
            "Epoch 27/300 70/80 loss: 0.6676 time 3.11s\n",
            "Epoch 27/300 80/80 loss: 0.6676 time 1.57s\n",
            "Final training  27/299 loss: 0.6676 time 226.22s\n",
            "Thu Dec 18 05:43:30 2025 Epoch: 28\n",
            "Epoch 28/300 10/80 loss: 0.6649 time 2.83s\n",
            "Epoch 28/300 20/80 loss: 0.6735 time 2.87s\n",
            "Epoch 28/300 30/80 loss: 0.6705 time 2.61s\n",
            "Epoch 28/300 40/80 loss: 0.6704 time 2.86s\n",
            "Epoch 28/300 50/80 loss: 0.6695 time 2.89s\n",
            "Epoch 28/300 60/80 loss: 0.6691 time 2.62s\n",
            "Epoch 28/300 70/80 loss: 0.6695 time 3.10s\n",
            "Epoch 28/300 80/80 loss: 0.6706 time 1.67s\n",
            "Final training  28/299 loss: 0.6706 time 226.33s\n",
            "Thu Dec 18 05:47:16 2025 Epoch: 29\n",
            "Epoch 29/300 10/80 loss: 0.6534 time 2.84s\n",
            "Epoch 29/300 20/80 loss: 0.6625 time 2.91s\n",
            "Epoch 29/300 30/80 loss: 0.6663 time 2.78s\n",
            "Epoch 29/300 40/80 loss: 0.6646 time 2.82s\n",
            "Epoch 29/300 50/80 loss: 0.6657 time 2.94s\n",
            "Epoch 29/300 60/80 loss: 0.6645 time 2.90s\n",
            "Epoch 29/300 70/80 loss: 0.6625 time 2.72s\n",
            "Epoch 29/300 80/80 loss: 0.6600 time 1.41s\n",
            "Final training  29/299 loss: 0.6600 time 226.63s\n",
            "Val 29/300 0/40 , dice_tc: 0.51863056 , dice_wt: 0.8327316 , dice_et: 0.6049128 , time 2.74s\n",
            "Val 29/300 1/40 , dice_tc: 0.36135498 , dice_wt: 0.86947787 , dice_et: 0.6049128 , time 2.68s\n",
            "Val 29/300 2/40 , dice_tc: 0.52344245 , dice_wt: 0.87200874 , dice_et: 0.74192476 , time 2.78s\n",
            "Val 29/300 3/40 , dice_tc: 0.5707629 , dice_wt: 0.87770975 , dice_et: 0.7552137 , time 2.68s\n",
            "Val 29/300 4/40 , dice_tc: 0.49510175 , dice_wt: 0.7933275 , dice_et: 0.5801762 , time 2.53s\n",
            "Val 29/300 5/40 , dice_tc: 0.50774544 , dice_wt: 0.79501367 , dice_et: 0.58501214 , time 2.55s\n",
            "Val 29/300 6/40 , dice_tc: 0.5500397 , dice_wt: 0.8049035 , dice_et: 0.62643397 , time 2.71s\n",
            "Val 29/300 7/40 , dice_tc: 0.5567492 , dice_wt: 0.79085076 , dice_et: 0.6357433 , time 2.62s\n",
            "Val 29/300 8/40 , dice_tc: 0.55947244 , dice_wt: 0.7936234 , dice_et: 0.6459515 , time 2.74s\n",
            "Val 29/300 9/40 , dice_tc: 0.57835335 , dice_wt: 0.80077755 , dice_et: 0.65563226 , time 2.57s\n",
            "Val 29/300 10/40 , dice_tc: 0.5688144 , dice_wt: 0.79228914 , dice_et: 0.6460291 , time 2.47s\n",
            "Val 29/300 11/40 , dice_tc: 0.55094373 , dice_wt: 0.7885595 , dice_et: 0.6249407 , time 2.74s\n",
            "Val 29/300 12/40 , dice_tc: 0.509501 , dice_wt: 0.79526746 , dice_et: 0.6249407 , time 2.54s\n",
            "Val 29/300 13/40 , dice_tc: 0.50902826 , dice_wt: 0.7811137 , dice_et: 0.61996156 , time 2.53s\n",
            "Val 29/300 14/40 , dice_tc: 0.49817753 , dice_wt: 0.78561467 , dice_et: 0.5815434 , time 2.63s\n",
            "Val 29/300 15/40 , dice_tc: 0.49441963 , dice_wt: 0.78790456 , dice_et: 0.5796363 , time 2.74s\n",
            "Val 29/300 16/40 , dice_tc: 0.49074274 , dice_wt: 0.7894742 , dice_et: 0.57181513 , time 2.76s\n",
            "Val 29/300 17/40 , dice_tc: 0.4761316 , dice_wt: 0.79211396 , dice_et: 0.55332005 , time 2.54s\n",
            "Val 29/300 18/40 , dice_tc: 0.49606615 , dice_wt: 0.7941975 , dice_et: 0.5739909 , time 2.75s\n",
            "Val 29/300 19/40 , dice_tc: 0.49928468 , dice_wt: 0.7924458 , dice_et: 0.5770161 , time 2.70s\n",
            "Val 29/300 20/40 , dice_tc: 0.49708948 , dice_wt: 0.7931413 , dice_et: 0.5755954 , time 2.46s\n",
            "Val 29/300 21/40 , dice_tc: 0.5099878 , dice_wt: 0.7951874 , dice_et: 0.5894523 , time 2.57s\n",
            "Val 29/300 22/40 , dice_tc: 0.49384034 , dice_wt: 0.7971191 , dice_et: 0.57032335 , time 2.74s\n",
            "Val 29/300 23/40 , dice_tc: 0.48317942 , dice_wt: 0.8004027 , dice_et: 0.5592859 , time 2.78s\n",
            "Val 29/300 24/40 , dice_tc: 0.473838 , dice_wt: 0.80339605 , dice_et: 0.54927844 , time 2.70s\n",
            "Val 29/300 25/40 , dice_tc: 0.45880476 , dice_wt: 0.80481917 , dice_et: 0.54927844 , time 2.77s\n",
            "Val 29/300 26/40 , dice_tc: 0.46962407 , dice_wt: 0.80586517 , dice_et: 0.55965346 , time 2.72s\n",
            "Val 29/300 27/40 , dice_tc: 0.4669935 , dice_wt: 0.80640423 , dice_et: 0.5506033 , time 2.69s\n",
            "Val 29/300 28/40 , dice_tc: 0.46386668 , dice_wt: 0.80670166 , dice_et: 0.5470596 , time 2.64s\n",
            "Val 29/300 29/40 , dice_tc: 0.4506904 , dice_wt: 0.8065254 , dice_et: 0.5314273 , time 2.49s\n",
            "Val 29/300 30/40 , dice_tc: 0.4453482 , dice_wt: 0.8068659 , dice_et: 0.5152998 , time 2.73s\n",
            "Val 29/300 31/40 , dice_tc: 0.45286307 , dice_wt: 0.80860424 , dice_et: 0.5230167 , time 2.70s\n",
            "Val 29/300 32/40 , dice_tc: 0.43913993 , dice_wt: 0.8057162 , dice_et: 0.5230167 , time 2.56s\n",
            "Val 29/300 33/40 , dice_tc: 0.44273195 , dice_wt: 0.8044218 , dice_et: 0.5276179 , time 2.70s\n",
            "Val 29/300 34/40 , dice_tc: 0.44916216 , dice_wt: 0.80066794 , dice_et: 0.5314554 , time 2.53s\n",
            "Val 29/300 35/40 , dice_tc: 0.4557731 , dice_wt: 0.80213064 , dice_et: 0.5383462 , time 2.70s\n",
            "Val 29/300 36/40 , dice_tc: 0.46805027 , dice_wt: 0.8030792 , dice_et: 0.5504619 , time 2.52s\n",
            "Val 29/300 37/40 , dice_tc: 0.46176243 , dice_wt: 0.8050698 , dice_et: 0.5450579 , time 2.70s\n",
            "Val 29/300 38/40 , dice_tc: 0.46892 , dice_wt: 0.8061814 , dice_et: 0.5520444 , time 2.68s\n",
            "Val 29/300 39/40 , dice_tc: 0.45913935 , dice_wt: 0.8050275 , dice_et: 0.5520444 , time 2.75s\n",
            "Final validation stats 29/299 , dice_tc: 0.45913935 , dice_wt: 0.8050275 , dice_et: 0.5520444 , Dice_Avg: 0.6054037 , LR: 0.000010 , time 106.13s\n",
            "new best (0.530209 --> 0.605404). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 05:52:51 2025 Epoch: 30\n",
            "Epoch 30/300 10/80 loss: 0.6625 time 2.96s\n",
            "Epoch 30/300 20/80 loss: 0.6695 time 2.83s\n",
            "Epoch 30/300 30/80 loss: 0.6688 time 2.84s\n",
            "Epoch 30/300 40/80 loss: 0.6692 time 2.85s\n",
            "Epoch 30/300 50/80 loss: 0.6671 time 3.06s\n",
            "Epoch 30/300 60/80 loss: 0.6680 time 3.01s\n",
            "Epoch 30/300 70/80 loss: 0.6659 time 2.80s\n",
            "Epoch 30/300 80/80 loss: 0.6644 time 1.62s\n",
            "Final training  30/299 loss: 0.6644 time 226.89s\n",
            "Thu Dec 18 05:56:38 2025 Epoch: 31\n",
            "Epoch 31/300 10/80 loss: 0.6677 time 2.68s\n",
            "Epoch 31/300 20/80 loss: 0.6657 time 3.05s\n",
            "Epoch 31/300 30/80 loss: 0.6669 time 3.04s\n",
            "Epoch 31/300 40/80 loss: 0.6660 time 2.69s\n",
            "Epoch 31/300 50/80 loss: 0.6637 time 2.99s\n",
            "Epoch 31/300 60/80 loss: 0.6634 time 2.76s\n",
            "Epoch 31/300 70/80 loss: 0.6637 time 2.87s\n",
            "Epoch 31/300 80/80 loss: 0.6633 time 1.68s\n",
            "Final training  31/299 loss: 0.6633 time 225.97s\n",
            "Thu Dec 18 06:00:24 2025 Epoch: 32\n",
            "Epoch 32/300 10/80 loss: 0.6401 time 2.88s\n",
            "Epoch 32/300 20/80 loss: 0.6413 time 2.99s\n",
            "Epoch 32/300 30/80 loss: 0.6451 time 2.68s\n",
            "Epoch 32/300 40/80 loss: 0.6494 time 2.85s\n",
            "Epoch 32/300 50/80 loss: 0.6491 time 2.97s\n",
            "Epoch 32/300 60/80 loss: 0.6517 time 2.86s\n",
            "Epoch 32/300 70/80 loss: 0.6525 time 2.65s\n",
            "Epoch 32/300 80/80 loss: 0.6542 time 1.43s\n",
            "Final training  32/299 loss: 0.6542 time 225.86s\n",
            "Thu Dec 18 06:04:10 2025 Epoch: 33\n",
            "Epoch 33/300 10/80 loss: 0.6479 time 2.68s\n",
            "Epoch 33/300 20/80 loss: 0.6500 time 3.05s\n",
            "Epoch 33/300 30/80 loss: 0.6494 time 3.04s\n",
            "Epoch 33/300 40/80 loss: 0.6483 time 3.16s\n",
            "Epoch 33/300 50/80 loss: 0.6499 time 2.76s\n",
            "Epoch 33/300 60/80 loss: 0.6515 time 2.84s\n",
            "Epoch 33/300 70/80 loss: 0.6517 time 2.85s\n",
            "Epoch 33/300 80/80 loss: 0.6501 time 1.67s\n",
            "Final training  33/299 loss: 0.6501 time 226.51s\n",
            "Thu Dec 18 06:07:57 2025 Epoch: 34\n",
            "Epoch 34/300 10/80 loss: 0.6659 time 3.04s\n",
            "Epoch 34/300 20/80 loss: 0.6646 time 2.57s\n",
            "Epoch 34/300 30/80 loss: 0.6621 time 2.99s\n",
            "Epoch 34/300 40/80 loss: 0.6607 time 2.67s\n",
            "Epoch 34/300 50/80 loss: 0.6610 time 2.86s\n",
            "Epoch 34/300 60/80 loss: 0.6581 time 2.72s\n",
            "Epoch 34/300 70/80 loss: 0.6558 time 2.79s\n",
            "Epoch 34/300 80/80 loss: 0.6540 time 1.43s\n",
            "Final training  34/299 loss: 0.6540 time 226.19s\n",
            "Thu Dec 18 06:11:43 2025 Epoch: 35\n",
            "Epoch 35/300 10/80 loss: 0.6419 time 2.89s\n",
            "Epoch 35/300 20/80 loss: 0.6467 time 2.71s\n",
            "Epoch 35/300 30/80 loss: 0.6439 time 3.05s\n",
            "Epoch 35/300 40/80 loss: 0.6516 time 2.86s\n",
            "Epoch 35/300 50/80 loss: 0.6499 time 2.80s\n",
            "Epoch 35/300 60/80 loss: 0.6507 time 3.00s\n",
            "Epoch 35/300 70/80 loss: 0.6500 time 3.02s\n",
            "Epoch 35/300 80/80 loss: 0.6505 time 1.67s\n",
            "Final training  35/299 loss: 0.6505 time 226.99s\n",
            "Thu Dec 18 06:15:30 2025 Epoch: 36\n",
            "Epoch 36/300 10/80 loss: 0.6422 time 2.85s\n",
            "Epoch 36/300 20/80 loss: 0.6469 time 2.65s\n",
            "Epoch 36/300 30/80 loss: 0.6393 time 2.94s\n",
            "Epoch 36/300 40/80 loss: 0.6425 time 3.01s\n",
            "Epoch 36/300 50/80 loss: 0.6420 time 2.84s\n",
            "Epoch 36/300 60/80 loss: 0.6464 time 2.90s\n",
            "Epoch 36/300 70/80 loss: 0.6438 time 2.78s\n",
            "Epoch 36/300 80/80 loss: 0.6434 time 1.57s\n",
            "Final training  36/299 loss: 0.6434 time 226.64s\n",
            "Thu Dec 18 06:19:17 2025 Epoch: 37\n",
            "Epoch 37/300 10/80 loss: 0.6570 time 2.63s\n",
            "Epoch 37/300 20/80 loss: 0.6491 time 2.81s\n",
            "Epoch 37/300 30/80 loss: 0.6495 time 2.67s\n",
            "Epoch 37/300 40/80 loss: 0.6490 time 2.88s\n",
            "Epoch 37/300 50/80 loss: 0.6506 time 2.87s\n",
            "Epoch 37/300 60/80 loss: 0.6506 time 2.80s\n",
            "Epoch 37/300 70/80 loss: 0.6495 time 3.03s\n",
            "Epoch 37/300 80/80 loss: 0.6485 time 1.68s\n",
            "Final training  37/299 loss: 0.6485 time 226.16s\n",
            "Thu Dec 18 06:23:03 2025 Epoch: 38\n",
            "Epoch 38/300 10/80 loss: 0.6364 time 3.03s\n",
            "Epoch 38/300 20/80 loss: 0.6409 time 2.60s\n",
            "Epoch 38/300 30/80 loss: 0.6431 time 3.09s\n",
            "Epoch 38/300 40/80 loss: 0.6425 time 3.03s\n",
            "Epoch 38/300 50/80 loss: 0.6415 time 2.87s\n",
            "Epoch 38/300 60/80 loss: 0.6375 time 2.93s\n",
            "Epoch 38/300 70/80 loss: 0.6397 time 3.03s\n",
            "Epoch 38/300 80/80 loss: 0.6412 time 1.43s\n",
            "Final training  38/299 loss: 0.6412 time 226.23s\n",
            "Thu Dec 18 06:26:49 2025 Epoch: 39\n",
            "Epoch 39/300 10/80 loss: 0.6328 time 2.80s\n",
            "Epoch 39/300 20/80 loss: 0.6284 time 2.89s\n",
            "Epoch 39/300 30/80 loss: 0.6306 time 3.09s\n",
            "Epoch 39/300 40/80 loss: 0.6342 time 3.05s\n",
            "Epoch 39/300 50/80 loss: 0.6389 time 2.63s\n",
            "Epoch 39/300 60/80 loss: 0.6414 time 3.07s\n",
            "Epoch 39/300 70/80 loss: 0.6431 time 2.82s\n",
            "Epoch 39/300 80/80 loss: 0.6443 time 1.50s\n",
            "Final training  39/299 loss: 0.6443 time 226.47s\n",
            "Val 39/300 0/40 , dice_tc: 0.54260814 , dice_wt: 0.882564 , dice_et: 0.6303149 , time 2.73s\n",
            "Val 39/300 1/40 , dice_tc: 0.37206483 , dice_wt: 0.91026795 , dice_et: 0.6303149 , time 2.67s\n",
            "Val 39/300 2/40 , dice_tc: 0.5349658 , dice_wt: 0.90801245 , dice_et: 0.74915797 , time 2.71s\n",
            "Val 39/300 3/40 , dice_tc: 0.57749116 , dice_wt: 0.90875936 , dice_et: 0.7503833 , time 2.72s\n",
            "Val 39/300 4/40 , dice_tc: 0.50459117 , dice_wt: 0.82692134 , dice_et: 0.5753613 , time 2.61s\n",
            "Val 39/300 5/40 , dice_tc: 0.5250092 , dice_wt: 0.82817036 , dice_et: 0.59099174 , time 2.56s\n",
            "Val 39/300 6/40 , dice_tc: 0.5631458 , dice_wt: 0.8365239 , dice_et: 0.6233588 , time 2.71s\n",
            "Val 39/300 7/40 , dice_tc: 0.56478125 , dice_wt: 0.8225436 , dice_et: 0.6231486 , time 2.54s\n",
            "Val 39/300 8/40 , dice_tc: 0.57052916 , dice_wt: 0.8273189 , dice_et: 0.63684654 , time 2.79s\n",
            "Val 39/300 9/40 , dice_tc: 0.5927547 , dice_wt: 0.83334196 , dice_et: 0.6516571 , time 2.56s\n",
            "Val 39/300 10/40 , dice_tc: 0.5859795 , dice_wt: 0.8268495 , dice_et: 0.6430784 , time 2.48s\n",
            "Val 39/300 11/40 , dice_tc: 0.57432467 , dice_wt: 0.8299737 , dice_et: 0.62618536 , time 2.66s\n",
            "Val 39/300 12/40 , dice_tc: 0.53126633 , dice_wt: 0.8364573 , dice_et: 0.62618536 , time 2.54s\n",
            "Val 39/300 13/40 , dice_tc: 0.5265021 , dice_wt: 0.82122743 , dice_et: 0.6170345 , time 2.50s\n",
            "Val 39/300 14/40 , dice_tc: 0.51559407 , dice_wt: 0.82434964 , dice_et: 0.596899 , time 2.64s\n",
            "Val 39/300 15/40 , dice_tc: 0.5165587 , dice_wt: 0.82631874 , dice_et: 0.60026294 , time 2.68s\n",
            "Val 39/300 16/40 , dice_tc: 0.51066345 , dice_wt: 0.8258611 , dice_et: 0.58886796 , time 2.70s\n",
            "Val 39/300 17/40 , dice_tc: 0.5062142 , dice_wt: 0.8279992 , dice_et: 0.5884889 , time 2.54s\n",
            "Val 39/300 18/40 , dice_tc: 0.52524126 , dice_wt: 0.82815766 , dice_et: 0.606591 , time 2.85s\n",
            "Val 39/300 19/40 , dice_tc: 0.528932 , dice_wt: 0.8273404 , dice_et: 0.6081799 , time 2.72s\n",
            "Val 39/300 20/40 , dice_tc: 0.526779 , dice_wt: 0.8283469 , dice_et: 0.60274744 , time 2.46s\n",
            "Val 39/300 21/40 , dice_tc: 0.5402664 , dice_wt: 0.8300003 , dice_et: 0.6154372 , time 2.51s\n",
            "Val 39/300 22/40 , dice_tc: 0.5241478 , dice_wt: 0.83207655 , dice_et: 0.5965755 , time 2.80s\n",
            "Val 39/300 23/40 , dice_tc: 0.51414925 , dice_wt: 0.8348022 , dice_et: 0.5863926 , time 2.77s\n",
            "Val 39/300 24/40 , dice_tc: 0.50443846 , dice_wt: 0.83775747 , dice_et: 0.5760506 , time 2.74s\n",
            "Val 39/300 25/40 , dice_tc: 0.48954722 , dice_wt: 0.8394807 , dice_et: 0.5760506 , time 2.72s\n",
            "Val 39/300 26/40 , dice_tc: 0.4992249 , dice_wt: 0.8398497 , dice_et: 0.5839256 , time 2.72s\n",
            "Val 39/300 27/40 , dice_tc: 0.49697545 , dice_wt: 0.83948135 , dice_et: 0.57296205 , time 2.78s\n",
            "Val 39/300 28/40 , dice_tc: 0.4960045 , dice_wt: 0.84129214 , dice_et: 0.5702728 , time 2.64s\n",
            "Val 39/300 29/40 , dice_tc: 0.48232 , dice_wt: 0.8416821 , dice_et: 0.55341715 , time 2.55s\n",
            "Val 39/300 30/40 , dice_tc: 0.47798985 , dice_wt: 0.8426246 , dice_et: 0.5367493 , time 2.66s\n",
            "Val 39/300 31/40 , dice_tc: 0.48596498 , dice_wt: 0.84419125 , dice_et: 0.54489696 , time 2.74s\n",
            "Val 39/300 32/40 , dice_tc: 0.47123876 , dice_wt: 0.84092873 , dice_et: 0.54489696 , time 2.55s\n",
            "Val 39/300 33/40 , dice_tc: 0.47499028 , dice_wt: 0.83998686 , dice_et: 0.54983157 , time 2.71s\n",
            "Val 39/300 34/40 , dice_tc: 0.4809725 , dice_wt: 0.83607566 , dice_et: 0.5533758 , time 2.48s\n",
            "Val 39/300 35/40 , dice_tc: 0.4868444 , dice_wt: 0.83690536 , dice_et: 0.5584626 , time 2.70s\n",
            "Val 39/300 36/40 , dice_tc: 0.49828273 , dice_wt: 0.83738726 , dice_et: 0.5695779 , time 2.59s\n",
            "Val 39/300 37/40 , dice_tc: 0.49183002 , dice_wt: 0.8392744 , dice_et: 0.56721663 , time 2.73s\n",
            "Val 39/300 38/40 , dice_tc: 0.4986634 , dice_wt: 0.8403642 , dice_et: 0.573488 , time 2.67s\n",
            "Val 39/300 39/40 , dice_tc: 0.48845273 , dice_wt: 0.8391123 , dice_et: 0.573488 , time 2.67s\n",
            "Final validation stats 39/299 , dice_tc: 0.48845273 , dice_wt: 0.8391123 , dice_et: 0.573488 , Dice_Avg: 0.63368434 , LR: 0.000013 , time 106.10s\n",
            "new best (0.605404 --> 0.633684). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 06:32:24 2025 Epoch: 40\n",
            "Epoch 40/300 10/80 loss: 0.6406 time 3.10s\n",
            "Epoch 40/300 20/80 loss: 0.6398 time 2.99s\n",
            "Epoch 40/300 30/80 loss: 0.6371 time 2.85s\n",
            "Epoch 40/300 40/80 loss: 0.6381 time 3.02s\n",
            "Epoch 40/300 50/80 loss: 0.6354 time 2.78s\n",
            "Epoch 40/300 60/80 loss: 0.6371 time 2.65s\n",
            "Epoch 40/300 70/80 loss: 0.6384 time 2.88s\n",
            "Epoch 40/300 80/80 loss: 0.6396 time 1.63s\n",
            "Final training  40/299 loss: 0.6396 time 227.24s\n",
            "Thu Dec 18 06:36:11 2025 Epoch: 41\n",
            "Epoch 41/300 10/80 loss: 0.6295 time 2.82s\n",
            "Epoch 41/300 20/80 loss: 0.6297 time 3.12s\n",
            "Epoch 41/300 30/80 loss: 0.6325 time 3.07s\n",
            "Epoch 41/300 40/80 loss: 0.6347 time 3.03s\n",
            "Epoch 41/300 50/80 loss: 0.6379 time 3.07s\n",
            "Epoch 41/300 60/80 loss: 0.6356 time 2.61s\n",
            "Epoch 41/300 70/80 loss: 0.6359 time 2.88s\n",
            "Epoch 41/300 80/80 loss: 0.6389 time 1.41s\n",
            "Final training  41/299 loss: 0.6389 time 226.91s\n",
            "Thu Dec 18 06:39:58 2025 Epoch: 42\n",
            "Epoch 42/300 10/80 loss: 0.6435 time 2.88s\n",
            "Epoch 42/300 20/80 loss: 0.6353 time 2.81s\n",
            "Epoch 42/300 30/80 loss: 0.6338 time 2.88s\n",
            "Epoch 42/300 40/80 loss: 0.6314 time 2.90s\n",
            "Epoch 42/300 50/80 loss: 0.6296 time 2.86s\n",
            "Epoch 42/300 60/80 loss: 0.6282 time 3.08s\n",
            "Epoch 42/300 70/80 loss: 0.6276 time 2.87s\n",
            "Epoch 42/300 80/80 loss: 0.6307 time 1.44s\n",
            "Final training  42/299 loss: 0.6307 time 226.45s\n",
            "Thu Dec 18 06:43:45 2025 Epoch: 43\n",
            "Epoch 43/300 10/80 loss: 0.6272 time 2.59s\n",
            "Epoch 43/300 20/80 loss: 0.6263 time 3.02s\n",
            "Epoch 43/300 30/80 loss: 0.6276 time 2.87s\n",
            "Epoch 43/300 40/80 loss: 0.6299 time 2.83s\n",
            "Epoch 43/300 50/80 loss: 0.6294 time 2.97s\n",
            "Epoch 43/300 60/80 loss: 0.6291 time 3.03s\n",
            "Epoch 43/300 70/80 loss: 0.6271 time 2.90s\n",
            "Epoch 43/300 80/80 loss: 0.6273 time 1.69s\n",
            "Final training  43/299 loss: 0.6273 time 226.56s\n",
            "Thu Dec 18 06:47:31 2025 Epoch: 44\n",
            "Epoch 44/300 10/80 loss: 0.6291 time 2.86s\n",
            "Epoch 44/300 20/80 loss: 0.6363 time 2.82s\n",
            "Epoch 44/300 30/80 loss: 0.6367 time 2.87s\n",
            "Epoch 44/300 40/80 loss: 0.6340 time 2.66s\n",
            "Epoch 44/300 50/80 loss: 0.6334 time 2.87s\n",
            "Epoch 44/300 60/80 loss: 0.6332 time 3.08s\n",
            "Epoch 44/300 70/80 loss: 0.6341 time 3.09s\n",
            "Epoch 44/300 80/80 loss: 0.6337 time 1.42s\n",
            "Final training  44/299 loss: 0.6337 time 226.90s\n",
            "Thu Dec 18 06:51:18 2025 Epoch: 45\n",
            "Epoch 45/300 10/80 loss: 0.6411 time 2.84s\n",
            "Epoch 45/300 20/80 loss: 0.6411 time 2.72s\n",
            "Epoch 45/300 30/80 loss: 0.6362 time 2.66s\n",
            "Epoch 45/300 40/80 loss: 0.6329 time 2.87s\n",
            "Epoch 45/300 50/80 loss: 0.6281 time 2.88s\n",
            "Epoch 45/300 60/80 loss: 0.6290 time 2.92s\n",
            "Epoch 45/300 70/80 loss: 0.6311 time 2.86s\n",
            "Epoch 45/300 80/80 loss: 0.6351 time 1.59s\n",
            "Final training  45/299 loss: 0.6351 time 226.46s\n",
            "Thu Dec 18 06:55:04 2025 Epoch: 46\n",
            "Epoch 46/300 10/80 loss: 0.6395 time 3.03s\n",
            "Epoch 46/300 20/80 loss: 0.6358 time 2.83s\n",
            "Epoch 46/300 30/80 loss: 0.6324 time 2.89s\n",
            "Epoch 46/300 40/80 loss: 0.6320 time 3.06s\n",
            "Epoch 46/300 50/80 loss: 0.6322 time 2.88s\n",
            "Epoch 46/300 60/80 loss: 0.6309 time 2.86s\n",
            "Epoch 46/300 70/80 loss: 0.6294 time 2.88s\n",
            "Epoch 46/300 80/80 loss: 0.6313 time 1.43s\n",
            "Final training  46/299 loss: 0.6313 time 226.55s\n",
            "Thu Dec 18 06:58:51 2025 Epoch: 47\n",
            "Epoch 47/300 10/80 loss: 0.6273 time 2.86s\n",
            "Epoch 47/300 20/80 loss: 0.6361 time 3.14s\n",
            "Epoch 47/300 30/80 loss: 0.6331 time 2.65s\n",
            "Epoch 47/300 40/80 loss: 0.6311 time 2.94s\n",
            "Epoch 47/300 50/80 loss: 0.6334 time 2.91s\n",
            "Epoch 47/300 60/80 loss: 0.6340 time 2.86s\n",
            "Epoch 47/300 70/80 loss: 0.6301 time 2.84s\n",
            "Epoch 47/300 80/80 loss: 0.6312 time 1.66s\n",
            "Final training  47/299 loss: 0.6312 time 226.88s\n",
            "Thu Dec 18 07:02:38 2025 Epoch: 48\n",
            "Epoch 48/300 10/80 loss: 0.6218 time 2.94s\n",
            "Epoch 48/300 20/80 loss: 0.6153 time 2.84s\n",
            "Epoch 48/300 30/80 loss: 0.6280 time 2.81s\n",
            "Epoch 48/300 40/80 loss: 0.6270 time 2.85s\n",
            "Epoch 48/300 50/80 loss: 0.6276 time 3.03s\n",
            "Epoch 48/300 60/80 loss: 0.6251 time 2.84s\n",
            "Epoch 48/300 70/80 loss: 0.6260 time 2.81s\n",
            "Epoch 48/300 80/80 loss: 0.6267 time 1.44s\n",
            "Final training  48/299 loss: 0.6267 time 226.11s\n",
            "Thu Dec 18 07:06:24 2025 Epoch: 49\n",
            "Epoch 49/300 10/80 loss: 0.6168 time 2.81s\n",
            "Epoch 49/300 20/80 loss: 0.6328 time 2.96s\n",
            "Epoch 49/300 30/80 loss: 0.6276 time 2.69s\n",
            "Epoch 49/300 40/80 loss: 0.6319 time 3.02s\n",
            "Epoch 49/300 50/80 loss: 0.6272 time 3.11s\n",
            "Epoch 49/300 60/80 loss: 0.6287 time 3.09s\n",
            "Epoch 49/300 70/80 loss: 0.6258 time 3.07s\n",
            "Epoch 49/300 80/80 loss: 0.6228 time 1.49s\n",
            "Final training  49/299 loss: 0.6228 time 227.02s\n",
            "Val 49/300 0/40 , dice_tc: 0.49697185 , dice_wt: 0.8967263 , dice_et: 0.6068757 , time 2.80s\n",
            "Val 49/300 1/40 , dice_tc: 0.30818355 , dice_wt: 0.9194484 , dice_et: 0.6068757 , time 2.71s\n",
            "Val 49/300 2/40 , dice_tc: 0.49143216 , dice_wt: 0.91277105 , dice_et: 0.73846775 , time 2.70s\n",
            "Val 49/300 3/40 , dice_tc: 0.53627676 , dice_wt: 0.91106164 , dice_et: 0.73441666 , time 2.67s\n",
            "Val 49/300 4/40 , dice_tc: 0.49112934 , dice_wt: 0.8320395 , dice_et: 0.5689613 , time 2.57s\n",
            "Val 49/300 5/40 , dice_tc: 0.5069967 , dice_wt: 0.8302326 , dice_et: 0.5793074 , time 2.54s\n",
            "Val 49/300 6/40 , dice_tc: 0.5468849 , dice_wt: 0.83673 , dice_et: 0.61386883 , time 2.70s\n",
            "Val 49/300 7/40 , dice_tc: 0.5581086 , dice_wt: 0.8267012 , dice_et: 0.6209255 , time 2.54s\n",
            "Val 49/300 8/40 , dice_tc: 0.5576331 , dice_wt: 0.8329733 , dice_et: 0.6292852 , time 2.74s\n",
            "Val 49/300 9/40 , dice_tc: 0.5795225 , dice_wt: 0.8387909 , dice_et: 0.63826966 , time 2.63s\n",
            "Val 49/300 10/40 , dice_tc: 0.5769541 , dice_wt: 0.8358082 , dice_et: 0.6334899 , time 2.48s\n",
            "Val 49/300 11/40 , dice_tc: 0.5564614 , dice_wt: 0.8290588 , dice_et: 0.6072007 , time 2.67s\n",
            "Val 49/300 12/40 , dice_tc: 0.5145997 , dice_wt: 0.83615303 , dice_et: 0.6072007 , time 2.45s\n",
            "Val 49/300 13/40 , dice_tc: 0.51136076 , dice_wt: 0.82558995 , dice_et: 0.59895694 , time 2.53s\n",
            "Val 49/300 14/40 , dice_tc: 0.50105965 , dice_wt: 0.83030695 , dice_et: 0.59416175 , time 2.68s\n",
            "Val 49/300 15/40 , dice_tc: 0.4973214 , dice_wt: 0.8309129 , dice_et: 0.59308964 , time 2.68s\n",
            "Val 49/300 16/40 , dice_tc: 0.4964892 , dice_wt: 0.8313234 , dice_et: 0.5885593 , time 2.70s\n",
            "Val 49/300 17/40 , dice_tc: 0.48281273 , dice_wt: 0.8335902 , dice_et: 0.5774977 , time 2.54s\n",
            "Val 49/300 18/40 , dice_tc: 0.5022489 , dice_wt: 0.83215696 , dice_et: 0.59495974 , time 2.81s\n",
            "Val 49/300 19/40 , dice_tc: 0.50653344 , dice_wt: 0.83187217 , dice_et: 0.5966165 , time 2.70s\n",
            "Val 49/300 20/40 , dice_tc: 0.5024482 , dice_wt: 0.83295804 , dice_et: 0.5894268 , time 2.47s\n",
            "Val 49/300 21/40 , dice_tc: 0.5173498 , dice_wt: 0.83572936 , dice_et: 0.60315794 , time 2.53s\n",
            "Val 49/300 22/40 , dice_tc: 0.50290996 , dice_wt: 0.8391683 , dice_et: 0.5867349 , time 2.74s\n",
            "Val 49/300 23/40 , dice_tc: 0.49217808 , dice_wt: 0.8417363 , dice_et: 0.57537794 , time 2.79s\n",
            "Val 49/300 24/40 , dice_tc: 0.48315266 , dice_wt: 0.84553444 , dice_et: 0.56530875 , time 2.74s\n",
            "Val 49/300 25/40 , dice_tc: 0.46922505 , dice_wt: 0.8476918 , dice_et: 0.56530875 , time 2.72s\n",
            "Val 49/300 26/40 , dice_tc: 0.4777858 , dice_wt: 0.8478766 , dice_et: 0.57191354 , time 2.72s\n",
            "Val 49/300 27/40 , dice_tc: 0.47280768 , dice_wt: 0.84639347 , dice_et: 0.5565167 , time 2.80s\n",
            "Val 49/300 28/40 , dice_tc: 0.46916553 , dice_wt: 0.8467701 , dice_et: 0.55062854 , time 2.66s\n",
            "Val 49/300 29/40 , dice_tc: 0.45611337 , dice_wt: 0.8471397 , dice_et: 0.5338601 , time 2.49s\n",
            "Val 49/300 30/40 , dice_tc: 0.45250124 , dice_wt: 0.84844506 , dice_et: 0.5173303 , time 2.67s\n",
            "Val 49/300 31/40 , dice_tc: 0.46030864 , dice_wt: 0.85018647 , dice_et: 0.5257181 , time 2.68s\n",
            "Val 49/300 32/40 , dice_tc: 0.44719997 , dice_wt: 0.8450051 , dice_et: 0.5257181 , time 2.59s\n",
            "Val 49/300 33/40 , dice_tc: 0.45064345 , dice_wt: 0.8442014 , dice_et: 0.5297167 , time 2.71s\n",
            "Val 49/300 34/40 , dice_tc: 0.4610847 , dice_wt: 0.8429475 , dice_et: 0.53867024 , time 2.48s\n",
            "Val 49/300 35/40 , dice_tc: 0.46812427 , dice_wt: 0.84437376 , dice_et: 0.54498893 , time 2.70s\n",
            "Val 49/300 36/40 , dice_tc: 0.4800093 , dice_wt: 0.84523183 , dice_et: 0.55637014 , time 2.52s\n",
            "Val 49/300 37/40 , dice_tc: 0.47152635 , dice_wt: 0.84718746 , dice_et: 0.55114293 , time 2.79s\n",
            "Val 49/300 38/40 , dice_tc: 0.47853372 , dice_wt: 0.8484775 , dice_et: 0.5579935 , time 2.67s\n",
            "Val 49/300 39/40 , dice_tc: 0.46936488 , dice_wt: 0.84831536 , dice_et: 0.5579935 , time 2.70s\n",
            "Final validation stats 49/299 , dice_tc: 0.46936488 , dice_wt: 0.84831536 , dice_et: 0.5579935 , Dice_Avg: 0.6252246 , LR: 0.000015 , time 106.02s\n",
            "Thu Dec 18 07:11:57 2025 Epoch: 50\n",
            "Epoch 50/300 10/80 loss: 0.6226 time 2.86s\n",
            "Epoch 50/300 20/80 loss: 0.6219 time 2.89s\n",
            "Epoch 50/300 30/80 loss: 0.6202 time 2.57s\n",
            "Epoch 50/300 40/80 loss: 0.6209 time 2.86s\n",
            "Epoch 50/300 50/80 loss: 0.6200 time 2.85s\n",
            "Epoch 50/300 60/80 loss: 0.6173 time 2.91s\n",
            "Epoch 50/300 70/80 loss: 0.6148 time 2.82s\n",
            "Epoch 50/300 80/80 loss: 0.6166 time 1.43s\n",
            "Final training  50/299 loss: 0.6166 time 226.84s\n",
            "Thu Dec 18 07:15:44 2025 Epoch: 51\n",
            "Epoch 51/300 10/80 loss: 0.6307 time 2.85s\n",
            "Epoch 51/300 20/80 loss: 0.6231 time 2.81s\n",
            "Epoch 51/300 30/80 loss: 0.6190 time 2.79s\n",
            "Epoch 51/300 40/80 loss: 0.6156 time 2.87s\n",
            "Epoch 51/300 50/80 loss: 0.6148 time 3.08s\n",
            "Epoch 51/300 60/80 loss: 0.6194 time 2.76s\n",
            "Epoch 51/300 70/80 loss: 0.6195 time 3.04s\n",
            "Epoch 51/300 80/80 loss: 0.6207 time 1.45s\n",
            "Final training  51/299 loss: 0.6207 time 227.74s\n",
            "Thu Dec 18 07:19:32 2025 Epoch: 52\n",
            "Epoch 52/300 10/80 loss: 0.6200 time 2.69s\n",
            "Epoch 52/300 20/80 loss: 0.6207 time 3.04s\n",
            "Epoch 52/300 30/80 loss: 0.6161 time 3.03s\n",
            "Epoch 52/300 40/80 loss: 0.6121 time 2.96s\n",
            "Epoch 52/300 50/80 loss: 0.6141 time 2.85s\n",
            "Epoch 52/300 60/80 loss: 0.6150 time 2.81s\n",
            "Epoch 52/300 70/80 loss: 0.6175 time 2.70s\n",
            "Epoch 52/300 80/80 loss: 0.6197 time 1.41s\n",
            "Final training  52/299 loss: 0.6197 time 227.33s\n",
            "Thu Dec 18 07:23:19 2025 Epoch: 53\n",
            "Epoch 53/300 10/80 loss: 0.5899 time 2.84s\n",
            "Epoch 53/300 20/80 loss: 0.6039 time 2.82s\n",
            "Epoch 53/300 30/80 loss: 0.6128 time 3.00s\n",
            "Epoch 53/300 40/80 loss: 0.6183 time 2.63s\n",
            "Epoch 53/300 50/80 loss: 0.6170 time 3.08s\n",
            "Epoch 53/300 60/80 loss: 0.6204 time 3.14s\n",
            "Epoch 53/300 70/80 loss: 0.6205 time 2.61s\n",
            "Epoch 53/300 80/80 loss: 0.6212 time 1.43s\n",
            "Final training  53/299 loss: 0.6212 time 226.86s\n",
            "Thu Dec 18 07:27:06 2025 Epoch: 54\n",
            "Epoch 54/300 10/80 loss: 0.6098 time 2.93s\n",
            "Epoch 54/300 20/80 loss: 0.6128 time 2.86s\n",
            "Epoch 54/300 30/80 loss: 0.6053 time 3.00s\n",
            "Epoch 54/300 40/80 loss: 0.6064 time 3.08s\n",
            "Epoch 54/300 50/80 loss: 0.6065 time 3.06s\n",
            "Epoch 54/300 60/80 loss: 0.6104 time 3.03s\n",
            "Epoch 54/300 70/80 loss: 0.6111 time 3.11s\n",
            "Epoch 54/300 80/80 loss: 0.6123 time 1.66s\n",
            "Final training  54/299 loss: 0.6123 time 227.35s\n",
            "Thu Dec 18 07:30:53 2025 Epoch: 55\n",
            "Epoch 55/300 10/80 loss: 0.6033 time 2.97s\n",
            "Epoch 55/300 20/80 loss: 0.6041 time 3.07s\n",
            "Epoch 55/300 30/80 loss: 0.5999 time 2.86s\n",
            "Epoch 55/300 40/80 loss: 0.6010 time 2.90s\n",
            "Epoch 55/300 50/80 loss: 0.6074 time 2.89s\n",
            "Epoch 55/300 60/80 loss: 0.6086 time 3.00s\n",
            "Epoch 55/300 70/80 loss: 0.6105 time 2.63s\n",
            "Epoch 55/300 80/80 loss: 0.6063 time 1.62s\n",
            "Final training  55/299 loss: 0.6063 time 227.25s\n",
            "Thu Dec 18 07:34:40 2025 Epoch: 56\n",
            "Epoch 56/300 10/80 loss: 0.6382 time 3.14s\n",
            "Epoch 56/300 20/80 loss: 0.6163 time 2.65s\n",
            "Epoch 56/300 30/80 loss: 0.6194 time 2.87s\n",
            "Epoch 56/300 40/80 loss: 0.6139 time 2.96s\n",
            "Epoch 56/300 50/80 loss: 0.6099 time 2.80s\n",
            "Epoch 56/300 60/80 loss: 0.6134 time 2.98s\n",
            "Epoch 56/300 70/80 loss: 0.6111 time 3.12s\n",
            "Epoch 56/300 80/80 loss: 0.6117 time 1.62s\n",
            "Final training  56/299 loss: 0.6117 time 226.73s\n",
            "Thu Dec 18 07:38:27 2025 Epoch: 57\n",
            "Epoch 57/300 10/80 loss: 0.5875 time 3.06s\n",
            "Epoch 57/300 20/80 loss: 0.6017 time 3.04s\n",
            "Epoch 57/300 30/80 loss: 0.6001 time 2.79s\n",
            "Epoch 57/300 40/80 loss: 0.6034 time 2.89s\n",
            "Epoch 57/300 50/80 loss: 0.6030 time 2.82s\n",
            "Epoch 57/300 60/80 loss: 0.6017 time 2.86s\n",
            "Epoch 57/300 70/80 loss: 0.6023 time 2.85s\n",
            "Epoch 57/300 80/80 loss: 0.6010 time 1.43s\n",
            "Final training  57/299 loss: 0.6010 time 226.45s\n",
            "Thu Dec 18 07:42:14 2025 Epoch: 58\n",
            "Epoch 58/300 10/80 loss: 0.5958 time 2.91s\n",
            "Epoch 58/300 20/80 loss: 0.6064 time 2.84s\n",
            "Epoch 58/300 30/80 loss: 0.6137 time 2.84s\n",
            "Epoch 58/300 40/80 loss: 0.6142 time 3.02s\n",
            "Epoch 58/300 50/80 loss: 0.6147 time 2.65s\n",
            "Epoch 58/300 60/80 loss: 0.6167 time 2.75s\n",
            "Epoch 58/300 70/80 loss: 0.6131 time 2.85s\n",
            "Epoch 58/300 80/80 loss: 0.6161 time 1.49s\n",
            "Final training  58/299 loss: 0.6161 time 225.49s\n",
            "Thu Dec 18 07:45:59 2025 Epoch: 59\n",
            "Epoch 59/300 10/80 loss: 0.6135 time 2.63s\n",
            "Epoch 59/300 20/80 loss: 0.5949 time 2.83s\n",
            "Epoch 59/300 30/80 loss: 0.5970 time 2.93s\n",
            "Epoch 59/300 40/80 loss: 0.5985 time 2.96s\n",
            "Epoch 59/300 50/80 loss: 0.5970 time 3.01s\n",
            "Epoch 59/300 60/80 loss: 0.5988 time 2.92s\n",
            "Epoch 59/300 70/80 loss: 0.5985 time 2.64s\n",
            "Epoch 59/300 80/80 loss: 0.5987 time 1.43s\n",
            "Final training  59/299 loss: 0.5987 time 226.37s\n",
            "Val 59/300 0/40 , dice_tc: 0.73601764 , dice_wt: 0.9391913 , dice_et: 0.80412465 , time 2.73s\n",
            "Val 59/300 1/40 , dice_tc: 0.6217092 , dice_wt: 0.953532 , dice_et: 0.80412465 , time 2.72s\n",
            "Val 59/300 2/40 , dice_tc: 0.71560735 , dice_wt: 0.9448991 , dice_et: 0.85960704 , time 2.70s\n",
            "Val 59/300 3/40 , dice_tc: 0.7226879 , dice_wt: 0.9292748 , dice_et: 0.8443422 , time 2.71s\n",
            "Val 59/300 4/40 , dice_tc: 0.6440208 , dice_wt: 0.836166 , dice_et: 0.65208733 , time 2.52s\n",
            "Val 59/300 5/40 , dice_tc: 0.6715221 , dice_wt: 0.83912116 , dice_et: 0.6865803 , time 2.55s\n",
            "Val 59/300 6/40 , dice_tc: 0.6835047 , dice_wt: 0.8314536 , dice_et: 0.69737345 , time 2.76s\n",
            "Val 59/300 7/40 , dice_tc: 0.6887798 , dice_wt: 0.82889485 , dice_et: 0.7052614 , time 2.54s\n",
            "Val 59/300 8/40 , dice_tc: 0.68846905 , dice_wt: 0.8385306 , dice_et: 0.71868604 , time 2.77s\n",
            "Val 59/300 9/40 , dice_tc: 0.7015244 , dice_wt: 0.8433112 , dice_et: 0.73032767 , time 2.51s\n",
            "Val 59/300 10/40 , dice_tc: 0.71308786 , dice_wt: 0.84778994 , dice_et: 0.74295634 , time 2.46s\n",
            "Val 59/300 11/40 , dice_tc: 0.7020121 , dice_wt: 0.845434 , dice_et: 0.7330585 , time 2.73s\n",
            "Val 59/300 12/40 , dice_tc: 0.65029776 , dice_wt: 0.8516813 , dice_et: 0.7330585 , time 2.54s\n",
            "Val 59/300 13/40 , dice_tc: 0.6445912 , dice_wt: 0.8461836 , dice_et: 0.7210681 , time 2.50s\n",
            "Val 59/300 14/40 , dice_tc: 0.63070464 , dice_wt: 0.84981793 , dice_et: 0.68076324 , time 2.63s\n",
            "Val 59/300 15/40 , dice_tc: 0.6352533 , dice_wt: 0.85079134 , dice_et: 0.68788177 , time 2.68s\n",
            "Val 59/300 16/40 , dice_tc: 0.6165536 , dice_wt: 0.8498709 , dice_et: 0.6644619 , time 2.68s\n",
            "Val 59/300 17/40 , dice_tc: 0.6178562 , dice_wt: 0.85310113 , dice_et: 0.6614031 , time 2.46s\n",
            "Val 59/300 18/40 , dice_tc: 0.6322332 , dice_wt: 0.852205 , dice_et: 0.67598736 , time 2.67s\n",
            "Val 59/300 19/40 , dice_tc: 0.63604516 , dice_wt: 0.8529965 , dice_et: 0.6801846 , time 2.69s\n",
            "Val 59/300 20/40 , dice_tc: 0.63428485 , dice_wt: 0.85504305 , dice_et: 0.6802097 , time 2.51s\n",
            "Val 59/300 21/40 , dice_tc: 0.6446071 , dice_wt: 0.857082 , dice_et: 0.6895188 , time 2.52s\n",
            "Val 59/300 22/40 , dice_tc: 0.6323233 , dice_wt: 0.8609592 , dice_et: 0.6794476 , time 2.77s\n",
            "Val 59/300 23/40 , dice_tc: 0.62527794 , dice_wt: 0.8643492 , dice_et: 0.675375 , time 2.74s\n",
            "Val 59/300 24/40 , dice_tc: 0.6179284 , dice_wt: 0.8681339 , dice_et: 0.6718884 , time 2.74s\n",
            "Val 59/300 25/40 , dice_tc: 0.6045511 , dice_wt: 0.87063366 , dice_et: 0.6718884 , time 2.82s\n",
            "Val 59/300 26/40 , dice_tc: 0.6125796 , dice_wt: 0.8716229 , dice_et: 0.67908734 , time 2.75s\n",
            "Val 59/300 27/40 , dice_tc: 0.61374146 , dice_wt: 0.8710486 , dice_et: 0.6732106 , time 2.68s\n",
            "Val 59/300 28/40 , dice_tc: 0.6120018 , dice_wt: 0.8717481 , dice_et: 0.67265666 , time 2.65s\n",
            "Val 59/300 29/40 , dice_tc: 0.59583396 , dice_wt: 0.8721798 , dice_et: 0.6543238 , time 2.62s\n",
            "Val 59/300 30/40 , dice_tc: 0.59446955 , dice_wt: 0.8735257 , dice_et: 0.6366234 , time 2.68s\n",
            "Val 59/300 31/40 , dice_tc: 0.6016554 , dice_wt: 0.8752227 , dice_et: 0.644556 , time 2.63s\n",
            "Val 59/300 32/40 , dice_tc: 0.60486054 , dice_wt: 0.87225014 , dice_et: 0.644556 , time 2.55s\n",
            "Val 59/300 33/40 , dice_tc: 0.6075425 , dice_wt: 0.870699 , dice_et: 0.649321 , time 2.70s\n",
            "Val 59/300 34/40 , dice_tc: 0.60879 , dice_wt: 0.86575633 , dice_et: 0.6487395 , time 2.52s\n",
            "Val 59/300 35/40 , dice_tc: 0.608486 , dice_wt: 0.8650136 , dice_et: 0.64732933 , time 2.64s\n",
            "Val 59/300 36/40 , dice_tc: 0.6175001 , dice_wt: 0.8661266 , dice_et: 0.6565157 , time 2.53s\n",
            "Val 59/300 37/40 , dice_tc: 0.6105606 , dice_wt: 0.86796 , dice_et: 0.65413886 , time 2.72s\n",
            "Val 59/300 38/40 , dice_tc: 0.6163211 , dice_wt: 0.8693018 , dice_et: 0.6599682 , time 2.67s\n",
            "Val 59/300 39/40 , dice_tc: 0.6048876 , dice_wt: 0.8690405 , dice_et: 0.6599682 , time 2.69s\n",
            "Final validation stats 59/299 , dice_tc: 0.6048876 , dice_wt: 0.8690405 , dice_et: 0.6599682 , Dice_Avg: 0.71129876 , LR: 0.000017 , time 105.65s\n",
            "new best (0.633684 --> 0.711299). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 07:51:33 2025 Epoch: 60\n",
            "Epoch 60/300 10/80 loss: 0.5982 time 3.06s\n",
            "Epoch 60/300 20/80 loss: 0.6027 time 2.96s\n",
            "Epoch 60/300 30/80 loss: 0.6035 time 3.08s\n",
            "Epoch 60/300 40/80 loss: 0.6035 time 2.81s\n",
            "Epoch 60/300 50/80 loss: 0.6014 time 3.14s\n",
            "Epoch 60/300 60/80 loss: 0.5989 time 2.86s\n",
            "Epoch 60/300 70/80 loss: 0.5968 time 2.78s\n",
            "Epoch 60/300 80/80 loss: 0.5988 time 1.44s\n",
            "Final training  60/299 loss: 0.5988 time 227.11s\n",
            "Thu Dec 18 07:55:21 2025 Epoch: 61\n",
            "Epoch 61/300 10/80 loss: 0.5800 time 2.66s\n",
            "Epoch 61/300 20/80 loss: 0.5880 time 3.10s\n",
            "Epoch 61/300 30/80 loss: 0.5907 time 2.88s\n",
            "Epoch 61/300 40/80 loss: 0.5908 time 2.84s\n",
            "Epoch 61/300 50/80 loss: 0.5888 time 3.03s\n",
            "Epoch 61/300 60/80 loss: 0.5941 time 2.84s\n",
            "Epoch 61/300 70/80 loss: 0.5965 time 2.81s\n",
            "Epoch 61/300 80/80 loss: 0.5935 time 1.63s\n",
            "Final training  61/299 loss: 0.5935 time 227.58s\n",
            "Thu Dec 18 07:59:08 2025 Epoch: 62\n",
            "Epoch 62/300 10/80 loss: 0.6006 time 2.65s\n",
            "Epoch 62/300 20/80 loss: 0.5983 time 2.99s\n",
            "Epoch 62/300 30/80 loss: 0.6076 time 2.88s\n",
            "Epoch 62/300 40/80 loss: 0.6134 time 2.67s\n",
            "Epoch 62/300 50/80 loss: 0.6061 time 2.96s\n",
            "Epoch 62/300 60/80 loss: 0.6042 time 2.87s\n",
            "Epoch 62/300 70/80 loss: 0.6047 time 3.04s\n",
            "Epoch 62/300 80/80 loss: 0.6031 time 1.51s\n",
            "Final training  62/299 loss: 0.6031 time 226.88s\n",
            "Thu Dec 18 08:02:55 2025 Epoch: 63\n",
            "Epoch 63/300 10/80 loss: 0.6111 time 3.05s\n",
            "Epoch 63/300 20/80 loss: 0.6106 time 3.06s\n",
            "Epoch 63/300 30/80 loss: 0.6106 time 3.10s\n",
            "Epoch 63/300 40/80 loss: 0.6070 time 2.90s\n",
            "Epoch 63/300 50/80 loss: 0.6017 time 3.07s\n",
            "Epoch 63/300 60/80 loss: 0.6008 time 3.05s\n",
            "Epoch 63/300 70/80 loss: 0.6031 time 3.04s\n",
            "Epoch 63/300 80/80 loss: 0.6037 time 1.56s\n",
            "Final training  63/299 loss: 0.6037 time 226.09s\n",
            "Thu Dec 18 08:06:41 2025 Epoch: 64\n",
            "Epoch 64/300 10/80 loss: 0.5793 time 3.02s\n",
            "Epoch 64/300 20/80 loss: 0.5862 time 2.97s\n",
            "Epoch 64/300 30/80 loss: 0.5905 time 2.78s\n",
            "Epoch 64/300 40/80 loss: 0.5905 time 3.00s\n",
            "Epoch 64/300 50/80 loss: 0.5902 time 2.81s\n",
            "Epoch 64/300 60/80 loss: 0.5923 time 3.05s\n",
            "Epoch 64/300 70/80 loss: 0.5910 time 2.81s\n",
            "Epoch 64/300 80/80 loss: 0.5916 time 1.42s\n",
            "Final training  64/299 loss: 0.5916 time 226.18s\n",
            "Thu Dec 18 08:10:27 2025 Epoch: 65\n",
            "Epoch 65/300 10/80 loss: 0.5908 time 3.07s\n",
            "Epoch 65/300 20/80 loss: 0.6032 time 2.82s\n",
            "Epoch 65/300 30/80 loss: 0.6006 time 2.86s\n",
            "Epoch 65/300 40/80 loss: 0.5988 time 2.77s\n",
            "Epoch 65/300 50/80 loss: 0.5960 time 2.95s\n",
            "Epoch 65/300 60/80 loss: 0.5921 time 2.89s\n",
            "Epoch 65/300 70/80 loss: 0.5935 time 2.92s\n",
            "Epoch 65/300 80/80 loss: 0.5930 time 1.40s\n",
            "Final training  65/299 loss: 0.5930 time 226.11s\n",
            "Thu Dec 18 08:14:13 2025 Epoch: 66\n",
            "Epoch 66/300 10/80 loss: 0.5756 time 2.84s\n",
            "Epoch 66/300 20/80 loss: 0.5884 time 2.84s\n",
            "Epoch 66/300 30/80 loss: 0.5904 time 2.96s\n",
            "Epoch 66/300 40/80 loss: 0.5902 time 2.85s\n",
            "Epoch 66/300 50/80 loss: 0.5923 time 3.07s\n",
            "Epoch 66/300 60/80 loss: 0.5881 time 2.69s\n",
            "Epoch 66/300 70/80 loss: 0.5898 time 2.82s\n",
            "Epoch 66/300 80/80 loss: 0.5912 time 1.44s\n",
            "Final training  66/299 loss: 0.5912 time 225.66s\n",
            "Thu Dec 18 08:17:59 2025 Epoch: 67\n",
            "Epoch 67/300 10/80 loss: 0.5760 time 3.07s\n",
            "Epoch 67/300 20/80 loss: 0.5796 time 2.79s\n",
            "Epoch 67/300 30/80 loss: 0.5812 time 2.70s\n",
            "Epoch 67/300 40/80 loss: 0.5818 time 3.08s\n",
            "Epoch 67/300 50/80 loss: 0.5798 time 2.87s\n",
            "Epoch 67/300 60/80 loss: 0.5807 time 3.10s\n",
            "Epoch 67/300 70/80 loss: 0.5821 time 2.87s\n",
            "Epoch 67/300 80/80 loss: 0.5827 time 1.37s\n",
            "Final training  67/299 loss: 0.5827 time 226.22s\n",
            "Thu Dec 18 08:21:45 2025 Epoch: 68\n",
            "Epoch 68/300 10/80 loss: 0.5762 time 2.85s\n",
            "Epoch 68/300 20/80 loss: 0.5780 time 2.82s\n",
            "Epoch 68/300 30/80 loss: 0.5814 time 3.05s\n",
            "Epoch 68/300 40/80 loss: 0.5887 time 2.68s\n",
            "Epoch 68/300 50/80 loss: 0.5889 time 2.65s\n",
            "Epoch 68/300 60/80 loss: 0.5897 time 2.76s\n",
            "Epoch 68/300 70/80 loss: 0.5891 time 2.88s\n",
            "Epoch 68/300 80/80 loss: 0.5876 time 1.62s\n",
            "Final training  68/299 loss: 0.5876 time 225.78s\n",
            "Thu Dec 18 08:25:31 2025 Epoch: 69\n",
            "Epoch 69/300 10/80 loss: 0.5906 time 3.13s\n",
            "Epoch 69/300 20/80 loss: 0.5929 time 3.09s\n",
            "Epoch 69/300 30/80 loss: 0.6002 time 2.68s\n",
            "Epoch 69/300 40/80 loss: 0.5901 time 2.92s\n",
            "Epoch 69/300 50/80 loss: 0.5894 time 2.86s\n",
            "Epoch 69/300 60/80 loss: 0.5931 time 2.65s\n",
            "Epoch 69/300 70/80 loss: 0.5947 time 3.03s\n",
            "Epoch 69/300 80/80 loss: 0.5943 time 1.66s\n",
            "Final training  69/299 loss: 0.5943 time 225.96s\n",
            "Val 69/300 0/40 , dice_tc: 0.6123118 , dice_wt: 0.904254 , dice_et: 0.7115605 , time 2.70s\n",
            "Val 69/300 1/40 , dice_tc: 0.3972405 , dice_wt: 0.92944604 , dice_et: 0.7115605 , time 2.66s\n",
            "Val 69/300 2/40 , dice_tc: 0.5659147 , dice_wt: 0.93033 , dice_et: 0.812055 , time 2.69s\n",
            "Val 69/300 3/40 , dice_tc: 0.59655374 , dice_wt: 0.9253516 , dice_et: 0.7907993 , time 2.73s\n",
            "Val 69/300 4/40 , dice_tc: 0.56427467 , dice_wt: 0.85317695 , dice_et: 0.61745036 , time 2.56s\n",
            "Val 69/300 5/40 , dice_tc: 0.5854733 , dice_wt: 0.85461456 , dice_et: 0.63734424 , time 2.50s\n",
            "Val 69/300 6/40 , dice_tc: 0.61334366 , dice_wt: 0.8604142 , dice_et: 0.6627496 , time 2.69s\n",
            "Val 69/300 7/40 , dice_tc: 0.62108815 , dice_wt: 0.8509898 , dice_et: 0.66712093 , time 2.55s\n",
            "Val 69/300 8/40 , dice_tc: 0.6181371 , dice_wt: 0.85615456 , dice_et: 0.6699827 , time 2.81s\n",
            "Val 69/300 9/40 , dice_tc: 0.6429001 , dice_wt: 0.8607828 , dice_et: 0.6796219 , time 2.57s\n",
            "Val 69/300 10/40 , dice_tc: 0.64385426 , dice_wt: 0.8613297 , dice_et: 0.68131894 , time 2.46s\n",
            "Val 69/300 11/40 , dice_tc: 0.62738603 , dice_wt: 0.8637299 , dice_et: 0.66284555 , time 2.67s\n",
            "Val 69/300 12/40 , dice_tc: 0.5803377 , dice_wt: 0.86921036 , dice_et: 0.66284555 , time 2.49s\n",
            "Val 69/300 13/40 , dice_tc: 0.56840384 , dice_wt: 0.85758054 , dice_et: 0.6476354 , time 2.61s\n",
            "Val 69/300 14/40 , dice_tc: 0.55301386 , dice_wt: 0.86051065 , dice_et: 0.63110507 , time 2.66s\n",
            "Val 69/300 15/40 , dice_tc: 0.5592736 , dice_wt: 0.8613486 , dice_et: 0.6375646 , time 2.70s\n",
            "Val 69/300 16/40 , dice_tc: 0.5464366 , dice_wt: 0.8591548 , dice_et: 0.6196108 , time 2.71s\n",
            "Val 69/300 17/40 , dice_tc: 0.5416582 , dice_wt: 0.860901 , dice_et: 0.6161472 , time 2.59s\n",
            "Val 69/300 18/40 , dice_tc: 0.55929565 , dice_wt: 0.8603094 , dice_et: 0.63249075 , time 2.74s\n",
            "Val 69/300 19/40 , dice_tc: 0.5622152 , dice_wt: 0.85915345 , dice_et: 0.6343485 , time 2.67s\n",
            "Val 69/300 20/40 , dice_tc: 0.5553762 , dice_wt: 0.8600016 , dice_et: 0.62552905 , time 2.46s\n",
            "Val 69/300 21/40 , dice_tc: 0.57007295 , dice_wt: 0.8623881 , dice_et: 0.639025 , time 2.51s\n",
            "Val 69/300 22/40 , dice_tc: 0.5550745 , dice_wt: 0.865085 , dice_et: 0.62308836 , time 2.80s\n",
            "Val 69/300 23/40 , dice_tc: 0.54560035 , dice_wt: 0.8676246 , dice_et: 0.614132 , time 2.75s\n",
            "Val 69/300 24/40 , dice_tc: 0.53640974 , dice_wt: 0.8705829 , dice_et: 0.6040451 , time 2.78s\n",
            "Val 69/300 25/40 , dice_tc: 0.5225521 , dice_wt: 0.87252516 , dice_et: 0.6040451 , time 2.72s\n",
            "Val 69/300 26/40 , dice_tc: 0.53137255 , dice_wt: 0.8728528 , dice_et: 0.6114714 , time 2.77s\n",
            "Val 69/300 27/40 , dice_tc: 0.5329624 , dice_wt: 0.8730046 , dice_et: 0.60047793 , time 2.72s\n",
            "Val 69/300 28/40 , dice_tc: 0.53039104 , dice_wt: 0.87432504 , dice_et: 0.5966454 , time 2.64s\n",
            "Val 69/300 29/40 , dice_tc: 0.5159377 , dice_wt: 0.87456125 , dice_et: 0.5788548 , time 2.54s\n",
            "Val 69/300 30/40 , dice_tc: 0.51218903 , dice_wt: 0.8755437 , dice_et: 0.5609118 , time 2.67s\n",
            "Val 69/300 31/40 , dice_tc: 0.52067846 , dice_wt: 0.87701267 , dice_et: 0.5700908 , time 2.70s\n",
            "Val 69/300 32/40 , dice_tc: 0.51878923 , dice_wt: 0.87479115 , dice_et: 0.5700908 , time 2.55s\n",
            "Val 69/300 33/40 , dice_tc: 0.5224289 , dice_wt: 0.87429094 , dice_et: 0.57454073 , time 2.72s\n",
            "Val 69/300 34/40 , dice_tc: 0.5299821 , dice_wt: 0.8732273 , dice_et: 0.5815843 , time 2.48s\n",
            "Val 69/300 35/40 , dice_tc: 0.5318946 , dice_wt: 0.8726292 , dice_et: 0.5828217 , time 2.70s\n",
            "Val 69/300 36/40 , dice_tc: 0.54287654 , dice_wt: 0.8730045 , dice_et: 0.59384483 , time 2.60s\n",
            "Val 69/300 37/40 , dice_tc: 0.5334773 , dice_wt: 0.8745273 , dice_et: 0.5856385 , time 2.72s\n",
            "Val 69/300 38/40 , dice_tc: 0.54031426 , dice_wt: 0.87523013 , dice_et: 0.5928674 , time 2.67s\n",
            "Val 69/300 39/40 , dice_tc: 0.52967095 , dice_wt: 0.8750079 , dice_et: 0.5928674 , time 2.68s\n",
            "Final validation stats 69/299 , dice_tc: 0.52967095 , dice_wt: 0.8750079 , dice_et: 0.5928674 , Dice_Avg: 0.66584873 , LR: 0.000020 , time 105.95s\n",
            "Thu Dec 18 08:31:03 2025 Epoch: 70\n",
            "Epoch 70/300 10/80 loss: 0.5547 time 2.82s\n",
            "Epoch 70/300 20/80 loss: 0.5691 time 3.06s\n",
            "Epoch 70/300 30/80 loss: 0.5668 time 2.87s\n",
            "Epoch 70/300 40/80 loss: 0.5689 time 2.86s\n",
            "Epoch 70/300 50/80 loss: 0.5724 time 2.83s\n",
            "Epoch 70/300 60/80 loss: 0.5758 time 2.69s\n",
            "Epoch 70/300 70/80 loss: 0.5783 time 3.08s\n",
            "Epoch 70/300 80/80 loss: 0.5793 time 1.42s\n",
            "Final training  70/299 loss: 0.5793 time 226.41s\n",
            "Thu Dec 18 08:34:50 2025 Epoch: 71\n",
            "Epoch 71/300 10/80 loss: 0.5951 time 3.02s\n",
            "Epoch 71/300 20/80 loss: 0.5974 time 2.68s\n",
            "Epoch 71/300 30/80 loss: 0.5928 time 3.08s\n",
            "Epoch 71/300 40/80 loss: 0.5873 time 2.82s\n",
            "Epoch 71/300 50/80 loss: 0.5827 time 3.09s\n",
            "Epoch 71/300 60/80 loss: 0.5813 time 2.71s\n",
            "Epoch 71/300 70/80 loss: 0.5831 time 2.68s\n",
            "Epoch 71/300 80/80 loss: 0.5840 time 1.40s\n",
            "Final training  71/299 loss: 0.5840 time 225.53s\n",
            "Thu Dec 18 08:38:35 2025 Epoch: 72\n",
            "Epoch 72/300 10/80 loss: 0.5914 time 2.92s\n",
            "Epoch 72/300 20/80 loss: 0.5842 time 2.97s\n",
            "Epoch 72/300 30/80 loss: 0.5895 time 2.89s\n",
            "Epoch 72/300 40/80 loss: 0.5861 time 3.09s\n",
            "Epoch 72/300 50/80 loss: 0.5855 time 2.89s\n",
            "Epoch 72/300 60/80 loss: 0.5784 time 2.85s\n",
            "Epoch 72/300 70/80 loss: 0.5771 time 2.97s\n",
            "Epoch 72/300 80/80 loss: 0.5796 time 1.60s\n",
            "Final training  72/299 loss: 0.5796 time 227.02s\n",
            "Thu Dec 18 08:42:22 2025 Epoch: 73\n",
            "Epoch 73/300 10/80 loss: 0.5882 time 3.02s\n",
            "Epoch 73/300 20/80 loss: 0.6012 time 2.91s\n",
            "Epoch 73/300 30/80 loss: 0.5947 time 3.00s\n",
            "Epoch 73/300 40/80 loss: 0.5888 time 3.05s\n",
            "Epoch 73/300 50/80 loss: 0.5858 time 2.91s\n",
            "Epoch 73/300 60/80 loss: 0.5880 time 2.78s\n",
            "Epoch 73/300 70/80 loss: 0.5840 time 2.65s\n",
            "Epoch 73/300 80/80 loss: 0.5823 time 1.65s\n",
            "Final training  73/299 loss: 0.5823 time 226.30s\n",
            "Thu Dec 18 08:46:08 2025 Epoch: 74\n",
            "Epoch 74/300 10/80 loss: 0.5703 time 2.91s\n",
            "Epoch 74/300 20/80 loss: 0.5739 time 3.07s\n",
            "Epoch 74/300 30/80 loss: 0.5734 time 2.96s\n",
            "Epoch 74/300 40/80 loss: 0.5769 time 3.03s\n",
            "Epoch 74/300 50/80 loss: 0.5723 time 2.81s\n",
            "Epoch 74/300 60/80 loss: 0.5743 time 2.89s\n",
            "Epoch 74/300 70/80 loss: 0.5762 time 2.62s\n",
            "Epoch 74/300 80/80 loss: 0.5746 time 1.61s\n",
            "Final training  74/299 loss: 0.5746 time 226.37s\n",
            "Thu Dec 18 08:49:55 2025 Epoch: 75\n",
            "Epoch 75/300 10/80 loss: 0.5866 time 2.65s\n",
            "Epoch 75/300 20/80 loss: 0.5789 time 2.63s\n",
            "Epoch 75/300 30/80 loss: 0.5714 time 2.64s\n",
            "Epoch 75/300 40/80 loss: 0.5725 time 3.12s\n",
            "Epoch 75/300 50/80 loss: 0.5736 time 3.05s\n",
            "Epoch 75/300 60/80 loss: 0.5690 time 3.01s\n",
            "Epoch 75/300 70/80 loss: 0.5653 time 2.69s\n",
            "Epoch 75/300 80/80 loss: 0.5666 time 1.55s\n",
            "Final training  75/299 loss: 0.5666 time 225.29s\n",
            "Thu Dec 18 08:53:40 2025 Epoch: 76\n",
            "Epoch 76/300 10/80 loss: 0.5633 time 2.84s\n",
            "Epoch 76/300 20/80 loss: 0.5635 time 2.89s\n",
            "Epoch 76/300 30/80 loss: 0.5579 time 2.91s\n",
            "Epoch 76/300 40/80 loss: 0.5677 time 2.85s\n",
            "Epoch 76/300 50/80 loss: 0.5704 time 2.72s\n",
            "Epoch 76/300 60/80 loss: 0.5664 time 3.06s\n",
            "Epoch 76/300 70/80 loss: 0.5683 time 2.85s\n",
            "Epoch 76/300 80/80 loss: 0.5689 time 1.63s\n",
            "Final training  76/299 loss: 0.5689 time 226.64s\n",
            "Thu Dec 18 08:57:27 2025 Epoch: 77\n",
            "Epoch 77/300 10/80 loss: 0.5711 time 2.81s\n",
            "Epoch 77/300 20/80 loss: 0.5672 time 2.81s\n",
            "Epoch 77/300 30/80 loss: 0.5684 time 3.10s\n",
            "Epoch 77/300 40/80 loss: 0.5653 time 2.61s\n",
            "Epoch 77/300 50/80 loss: 0.5703 time 2.69s\n",
            "Epoch 77/300 60/80 loss: 0.5718 time 3.06s\n",
            "Epoch 77/300 70/80 loss: 0.5722 time 3.02s\n",
            "Epoch 77/300 80/80 loss: 0.5745 time 1.43s\n",
            "Final training  77/299 loss: 0.5745 time 225.80s\n",
            "Thu Dec 18 09:01:13 2025 Epoch: 78\n",
            "Epoch 78/300 10/80 loss: 0.5696 time 3.01s\n",
            "Epoch 78/300 20/80 loss: 0.5768 time 2.65s\n",
            "Epoch 78/300 30/80 loss: 0.5760 time 2.95s\n",
            "Epoch 78/300 40/80 loss: 0.5696 time 2.85s\n",
            "Epoch 78/300 50/80 loss: 0.5640 time 2.87s\n",
            "Epoch 78/300 60/80 loss: 0.5686 time 2.83s\n",
            "Epoch 78/300 70/80 loss: 0.5670 time 3.02s\n",
            "Epoch 78/300 80/80 loss: 0.5705 time 1.42s\n",
            "Final training  78/299 loss: 0.5705 time 226.39s\n",
            "Thu Dec 18 09:04:59 2025 Epoch: 79\n",
            "Epoch 79/300 10/80 loss: 0.5846 time 2.61s\n",
            "Epoch 79/300 20/80 loss: 0.5728 time 3.12s\n",
            "Epoch 79/300 30/80 loss: 0.5678 time 2.59s\n",
            "Epoch 79/300 40/80 loss: 0.5672 time 2.90s\n",
            "Epoch 79/300 50/80 loss: 0.5759 time 2.80s\n",
            "Epoch 79/300 60/80 loss: 0.5702 time 2.83s\n",
            "Epoch 79/300 70/80 loss: 0.5664 time 2.65s\n",
            "Epoch 79/300 80/80 loss: 0.5672 time 1.66s\n",
            "Final training  79/299 loss: 0.5672 time 227.03s\n",
            "Val 79/300 0/40 , dice_tc: 0.42261803 , dice_wt: 0.9003835 , dice_et: 0.4859902 , time 2.75s\n",
            "Val 79/300 1/40 , dice_tc: 0.27131099 , dice_wt: 0.936486 , dice_et: 0.4859902 , time 2.70s\n",
            "Val 79/300 2/40 , dice_tc: 0.46531662 , dice_wt: 0.9185951 , dice_et: 0.67663616 , time 2.70s\n",
            "Val 79/300 3/40 , dice_tc: 0.5100318 , dice_wt: 0.9068368 , dice_et: 0.68233514 , time 2.74s\n",
            "Val 79/300 4/40 , dice_tc: 0.48059922 , dice_wt: 0.8223422 , dice_et: 0.53274417 , time 2.54s\n",
            "Val 79/300 5/40 , dice_tc: 0.4977332 , dice_wt: 0.82267505 , dice_et: 0.54660064 , time 2.54s\n",
            "Val 79/300 6/40 , dice_tc: 0.52750605 , dice_wt: 0.8262385 , dice_et: 0.5776952 , time 2.72s\n",
            "Val 79/300 7/40 , dice_tc: 0.52037257 , dice_wt: 0.81915545 , dice_et: 0.5669878 , time 2.62s\n",
            "Val 79/300 8/40 , dice_tc: 0.5164015 , dice_wt: 0.83098596 , dice_et: 0.56887376 , time 2.75s\n",
            "Val 79/300 9/40 , dice_tc: 0.54637617 , dice_wt: 0.8391919 , dice_et: 0.5852398 , time 2.56s\n",
            "Val 79/300 10/40 , dice_tc: 0.53713757 , dice_wt: 0.83926815 , dice_et: 0.5762428 , time 2.47s\n",
            "Val 79/300 11/40 , dice_tc: 0.5220954 , dice_wt: 0.8257475 , dice_et: 0.55875975 , time 2.68s\n",
            "Val 79/300 12/40 , dice_tc: 0.48286664 , dice_wt: 0.83402455 , dice_et: 0.55875975 , time 2.62s\n",
            "Val 79/300 13/40 , dice_tc: 0.4681561 , dice_wt: 0.83028203 , dice_et: 0.53976053 , time 2.54s\n",
            "Val 79/300 14/40 , dice_tc: 0.4515887 , dice_wt: 0.835436 , dice_et: 0.520619 , time 2.65s\n",
            "Val 79/300 15/40 , dice_tc: 0.45336133 , dice_wt: 0.83879805 , dice_et: 0.52348316 , time 2.73s\n",
            "Val 79/300 16/40 , dice_tc: 0.46136102 , dice_wt: 0.84126896 , dice_et: 0.53028387 , time 2.73s\n",
            "Val 79/300 17/40 , dice_tc: 0.4458677 , dice_wt: 0.8449284 , dice_et: 0.5121005 , time 2.55s\n",
            "Val 79/300 18/40 , dice_tc: 0.46690634 , dice_wt: 0.8436957 , dice_et: 0.533016 , time 2.76s\n",
            "Val 79/300 19/40 , dice_tc: 0.4702824 , dice_wt: 0.8460392 , dice_et: 0.53680706 , time 2.70s\n",
            "Val 79/300 20/40 , dice_tc: 0.46405792 , dice_wt: 0.8496338 , dice_et: 0.52888644 , time 2.48s\n",
            "Val 79/300 21/40 , dice_tc: 0.4791721 , dice_wt: 0.852483 , dice_et: 0.54359174 , time 2.49s\n",
            "Val 79/300 22/40 , dice_tc: 0.46452394 , dice_wt: 0.85654557 , dice_et: 0.52597976 , time 2.76s\n",
            "Val 79/300 23/40 , dice_tc: 0.4524678 , dice_wt: 0.8582228 , dice_et: 0.51150256 , time 2.74s\n",
            "Val 79/300 24/40 , dice_tc: 0.44345462 , dice_wt: 0.8618298 , dice_et: 0.5007986 , time 2.74s\n",
            "Val 79/300 25/40 , dice_tc: 0.4313528 , dice_wt: 0.86496294 , dice_et: 0.5007986 , time 2.72s\n",
            "Val 79/300 26/40 , dice_tc: 0.44327742 , dice_wt: 0.8637516 , dice_et: 0.5123535 , time 2.73s\n",
            "Val 79/300 27/40 , dice_tc: 0.4438005 , dice_wt: 0.8627757 , dice_et: 0.50205237 , time 2.77s\n",
            "Val 79/300 28/40 , dice_tc: 0.44106376 , dice_wt: 0.8619461 , dice_et: 0.4981987 , time 2.64s\n",
            "Val 79/300 29/40 , dice_tc: 0.42806256 , dice_wt: 0.8618518 , dice_et: 0.4820007 , time 2.46s\n",
            "Val 79/300 30/40 , dice_tc: 0.42503124 , dice_wt: 0.8631005 , dice_et: 0.46688542 , time 2.72s\n",
            "Val 79/300 31/40 , dice_tc: 0.43372077 , dice_wt: 0.8645769 , dice_et: 0.47672153 , time 2.68s\n",
            "Val 79/300 32/40 , dice_tc: 0.4209824 , dice_wt: 0.8619435 , dice_et: 0.47672153 , time 2.54s\n",
            "Val 79/300 33/40 , dice_tc: 0.42599916 , dice_wt: 0.8614243 , dice_et: 0.48192224 , time 2.70s\n",
            "Val 79/300 34/40 , dice_tc: 0.433917 , dice_wt: 0.8607702 , dice_et: 0.48990628 , time 2.46s\n",
            "Val 79/300 35/40 , dice_tc: 0.44098192 , dice_wt: 0.8616284 , dice_et: 0.4972768 , time 2.74s\n",
            "Val 79/300 36/40 , dice_tc: 0.4534337 , dice_wt: 0.86338973 , dice_et: 0.509918 , time 2.48s\n",
            "Val 79/300 37/40 , dice_tc: 0.4449086 , dice_wt: 0.86523896 , dice_et: 0.5003958 , time 2.67s\n",
            "Val 79/300 38/40 , dice_tc: 0.45130628 , dice_wt: 0.8666182 , dice_et: 0.507549 , time 2.67s\n",
            "Val 79/300 39/40 , dice_tc: 0.4417359 , dice_wt: 0.86717874 , dice_et: 0.507549 , time 2.73s\n",
            "Final validation stats 79/299 , dice_tc: 0.4417359 , dice_wt: 0.86717874 , dice_et: 0.507549 , Dice_Avg: 0.6054879 , LR: 0.000023 , time 105.98s\n",
            "Thu Dec 18 09:10:32 2025 Epoch: 80\n",
            "Epoch 80/300 10/80 loss: 0.5509 time 3.05s\n",
            "Epoch 80/300 20/80 loss: 0.5622 time 2.81s\n",
            "Epoch 80/300 30/80 loss: 0.5662 time 2.87s\n",
            "Epoch 80/300 40/80 loss: 0.5629 time 2.86s\n",
            "Epoch 80/300 50/80 loss: 0.5724 time 2.61s\n",
            "Epoch 80/300 60/80 loss: 0.5696 time 2.83s\n",
            "Epoch 80/300 70/80 loss: 0.5664 time 2.89s\n",
            "Epoch 80/300 80/80 loss: 0.5656 time 1.42s\n",
            "Final training  80/299 loss: 0.5656 time 226.33s\n",
            "Thu Dec 18 09:14:18 2025 Epoch: 81\n",
            "Epoch 81/300 10/80 loss: 0.5661 time 2.90s\n",
            "Epoch 81/300 20/80 loss: 0.5594 time 3.04s\n",
            "Epoch 81/300 30/80 loss: 0.5591 time 2.86s\n",
            "Epoch 81/300 40/80 loss: 0.5653 time 2.71s\n",
            "Epoch 81/300 50/80 loss: 0.5638 time 2.66s\n",
            "Epoch 81/300 60/80 loss: 0.5626 time 3.02s\n",
            "Epoch 81/300 70/80 loss: 0.5623 time 3.06s\n",
            "Epoch 81/300 80/80 loss: 0.5636 time 1.40s\n",
            "Final training  81/299 loss: 0.5636 time 226.23s\n",
            "Thu Dec 18 09:18:05 2025 Epoch: 82\n",
            "Epoch 82/300 10/80 loss: 0.5881 time 2.82s\n",
            "Epoch 82/300 20/80 loss: 0.5751 time 2.71s\n",
            "Epoch 82/300 30/80 loss: 0.5728 time 2.79s\n",
            "Epoch 82/300 40/80 loss: 0.5787 time 2.62s\n",
            "Epoch 82/300 50/80 loss: 0.5806 time 2.81s\n",
            "Epoch 82/300 60/80 loss: 0.5725 time 3.05s\n",
            "Epoch 82/300 70/80 loss: 0.5722 time 2.81s\n",
            "Epoch 82/300 80/80 loss: 0.5660 time 1.63s\n",
            "Final training  82/299 loss: 0.5660 time 226.03s\n",
            "Thu Dec 18 09:21:51 2025 Epoch: 83\n",
            "Epoch 83/300 10/80 loss: 0.5321 time 3.06s\n",
            "Epoch 83/300 20/80 loss: 0.5550 time 2.83s\n",
            "Epoch 83/300 30/80 loss: 0.5573 time 2.87s\n",
            "Epoch 83/300 40/80 loss: 0.5565 time 3.00s\n",
            "Epoch 83/300 50/80 loss: 0.5600 time 3.00s\n",
            "Epoch 83/300 60/80 loss: 0.5551 time 2.77s\n",
            "Epoch 83/300 70/80 loss: 0.5601 time 3.03s\n",
            "Epoch 83/300 80/80 loss: 0.5614 time 1.45s\n",
            "Final training  83/299 loss: 0.5614 time 226.34s\n",
            "Thu Dec 18 09:25:37 2025 Epoch: 84\n",
            "Epoch 84/300 10/80 loss: 0.5612 time 2.85s\n",
            "Epoch 84/300 20/80 loss: 0.5507 time 2.87s\n",
            "Epoch 84/300 30/80 loss: 0.5611 time 2.83s\n",
            "Epoch 84/300 40/80 loss: 0.5586 time 3.02s\n",
            "Epoch 84/300 50/80 loss: 0.5532 time 2.90s\n",
            "Epoch 84/300 60/80 loss: 0.5542 time 2.95s\n",
            "Epoch 84/300 70/80 loss: 0.5557 time 2.88s\n",
            "Epoch 84/300 80/80 loss: 0.5564 time 1.41s\n",
            "Final training  84/299 loss: 0.5564 time 225.57s\n",
            "Thu Dec 18 09:29:22 2025 Epoch: 85\n",
            "Epoch 85/300 10/80 loss: 0.5853 time 3.10s\n",
            "Epoch 85/300 20/80 loss: 0.5686 time 2.96s\n",
            "Epoch 85/300 30/80 loss: 0.5703 time 3.02s\n",
            "Epoch 85/300 40/80 loss: 0.5577 time 2.90s\n",
            "Epoch 85/300 50/80 loss: 0.5579 time 2.82s\n",
            "Epoch 85/300 60/80 loss: 0.5580 time 2.71s\n",
            "Epoch 85/300 70/80 loss: 0.5541 time 3.08s\n",
            "Epoch 85/300 80/80 loss: 0.5535 time 1.41s\n",
            "Final training  85/299 loss: 0.5535 time 226.20s\n",
            "Thu Dec 18 09:33:09 2025 Epoch: 86\n",
            "Epoch 86/300 10/80 loss: 0.5448 time 3.07s\n",
            "Epoch 86/300 20/80 loss: 0.5532 time 3.06s\n",
            "Epoch 86/300 30/80 loss: 0.5543 time 2.80s\n",
            "Epoch 86/300 40/80 loss: 0.5577 time 2.85s\n",
            "Epoch 86/300 50/80 loss: 0.5608 time 2.93s\n",
            "Epoch 86/300 60/80 loss: 0.5591 time 2.82s\n",
            "Epoch 86/300 70/80 loss: 0.5589 time 2.66s\n",
            "Epoch 86/300 80/80 loss: 0.5557 time 1.68s\n",
            "Final training  86/299 loss: 0.5557 time 228.31s\n",
            "Thu Dec 18 09:36:57 2025 Epoch: 87\n",
            "Epoch 87/300 10/80 loss: 0.5615 time 3.05s\n",
            "Epoch 87/300 20/80 loss: 0.5574 time 3.11s\n",
            "Epoch 87/300 30/80 loss: 0.5558 time 2.86s\n",
            "Epoch 87/300 40/80 loss: 0.5548 time 3.04s\n",
            "Epoch 87/300 50/80 loss: 0.5536 time 2.84s\n",
            "Epoch 87/300 60/80 loss: 0.5603 time 2.87s\n",
            "Epoch 87/300 70/80 loss: 0.5580 time 2.85s\n",
            "Epoch 87/300 80/80 loss: 0.5575 time 1.48s\n",
            "Final training  87/299 loss: 0.5575 time 227.20s\n",
            "Thu Dec 18 09:40:44 2025 Epoch: 88\n",
            "Epoch 88/300 10/80 loss: 0.5324 time 3.03s\n",
            "Epoch 88/300 20/80 loss: 0.5230 time 2.82s\n",
            "Epoch 88/300 30/80 loss: 0.5315 time 2.74s\n",
            "Epoch 88/300 40/80 loss: 0.5415 time 2.82s\n",
            "Epoch 88/300 50/80 loss: 0.5374 time 3.04s\n",
            "Epoch 88/300 60/80 loss: 0.5414 time 3.06s\n",
            "Epoch 88/300 70/80 loss: 0.5382 time 2.65s\n",
            "Epoch 88/300 80/80 loss: 0.5407 time 1.62s\n",
            "Final training  88/299 loss: 0.5407 time 226.51s\n",
            "Thu Dec 18 09:44:31 2025 Epoch: 89\n",
            "Epoch 89/300 10/80 loss: 0.5554 time 2.88s\n",
            "Epoch 89/300 20/80 loss: 0.5385 time 3.10s\n",
            "Epoch 89/300 30/80 loss: 0.5365 time 2.72s\n",
            "Epoch 89/300 40/80 loss: 0.5363 time 2.89s\n",
            "Epoch 89/300 50/80 loss: 0.5425 time 2.85s\n",
            "Epoch 89/300 60/80 loss: 0.5406 time 3.07s\n",
            "Epoch 89/300 70/80 loss: 0.5411 time 2.62s\n",
            "Epoch 89/300 80/80 loss: 0.5410 time 1.60s\n",
            "Final training  89/299 loss: 0.5410 time 226.76s\n",
            "Val 89/300 0/40 , dice_tc: 0.6580154 , dice_wt: 0.9482843 , dice_et: 0.76495796 , time 2.75s\n",
            "Val 89/300 1/40 , dice_tc: 0.42472085 , dice_wt: 0.96245277 , dice_et: 0.76495796 , time 2.76s\n",
            "Val 89/300 2/40 , dice_tc: 0.57950133 , dice_wt: 0.9462077 , dice_et: 0.8358723 , time 2.64s\n",
            "Val 89/300 3/40 , dice_tc: 0.6153015 , dice_wt: 0.9304323 , dice_et: 0.8302787 , time 2.66s\n",
            "Val 89/300 4/40 , dice_tc: 0.5636276 , dice_wt: 0.84273607 , dice_et: 0.6614347 , time 2.54s\n",
            "Val 89/300 5/40 , dice_tc: 0.5901093 , dice_wt: 0.8471763 , dice_et: 0.683395 , time 2.54s\n",
            "Val 89/300 6/40 , dice_tc: 0.6186973 , dice_wt: 0.8453228 , dice_et: 0.7067425 , time 2.79s\n",
            "Val 89/300 7/40 , dice_tc: 0.6232337 , dice_wt: 0.84283495 , dice_et: 0.7053237 , time 2.57s\n",
            "Val 89/300 8/40 , dice_tc: 0.6176155 , dice_wt: 0.85124546 , dice_et: 0.7090278 , time 2.75s\n",
            "Val 89/300 9/40 , dice_tc: 0.6392875 , dice_wt: 0.855322 , dice_et: 0.72028905 , time 2.56s\n",
            "Val 89/300 10/40 , dice_tc: 0.643187 , dice_wt: 0.8592741 , dice_et: 0.72204334 , time 2.48s\n",
            "Val 89/300 11/40 , dice_tc: 0.6252213 , dice_wt: 0.858227 , dice_et: 0.7012766 , time 2.67s\n",
            "Val 89/300 12/40 , dice_tc: 0.5786574 , dice_wt: 0.8644642 , dice_et: 0.7012766 , time 2.54s\n",
            "Val 89/300 13/40 , dice_tc: 0.56920534 , dice_wt: 0.8597197 , dice_et: 0.6869285 , time 2.48s\n",
            "Val 89/300 14/40 , dice_tc: 0.5524319 , dice_wt: 0.86262286 , dice_et: 0.6660735 , time 2.63s\n",
            "Val 89/300 15/40 , dice_tc: 0.55433595 , dice_wt: 0.86509424 , dice_et: 0.6705038 , time 2.72s\n",
            "Val 89/300 16/40 , dice_tc: 0.53967226 , dice_wt: 0.86342835 , dice_et: 0.6477148 , time 2.64s\n",
            "Val 89/300 17/40 , dice_tc: 0.5361942 , dice_wt: 0.8664787 , dice_et: 0.6436595 , time 2.47s\n",
            "Val 89/300 18/40 , dice_tc: 0.5522258 , dice_wt: 0.8653245 , dice_et: 0.6578795 , time 2.69s\n",
            "Val 89/300 19/40 , dice_tc: 0.5561401 , dice_wt: 0.8666123 , dice_et: 0.66042894 , time 2.63s\n",
            "Val 89/300 20/40 , dice_tc: 0.5489854 , dice_wt: 0.86900586 , dice_et: 0.6532246 , time 2.49s\n",
            "Val 89/300 21/40 , dice_tc: 0.56339204 , dice_wt: 0.87196755 , dice_et: 0.66532624 , time 2.47s\n",
            "Val 89/300 22/40 , dice_tc: 0.5477218 , dice_wt: 0.8753942 , dice_et: 0.64801013 , time 2.65s\n",
            "Val 89/300 23/40 , dice_tc: 0.5413704 , dice_wt: 0.8781107 , dice_et: 0.6425055 , time 2.66s\n",
            "Val 89/300 24/40 , dice_tc: 0.5322915 , dice_wt: 0.8811946 , dice_et: 0.6332432 , time 2.67s\n",
            "Val 89/300 25/40 , dice_tc: 0.51780283 , dice_wt: 0.883319 , dice_et: 0.6332432 , time 2.66s\n",
            "Val 89/300 26/40 , dice_tc: 0.52761537 , dice_wt: 0.88400406 , dice_et: 0.6410548 , time 2.64s\n",
            "Val 89/300 27/40 , dice_tc: 0.52785385 , dice_wt: 0.8829524 , dice_et: 0.6312692 , time 2.71s\n",
            "Val 89/300 28/40 , dice_tc: 0.5247337 , dice_wt: 0.8838603 , dice_et: 0.6274516 , time 2.62s\n",
            "Val 89/300 29/40 , dice_tc: 0.50983644 , dice_wt: 0.8837634 , dice_et: 0.6082493 , time 2.53s\n",
            "Val 89/300 30/40 , dice_tc: 0.5060511 , dice_wt: 0.8845718 , dice_et: 0.5900398 , time 2.67s\n",
            "Val 89/300 31/40 , dice_tc: 0.5154379 , dice_wt: 0.88616097 , dice_et: 0.59930015 , time 2.63s\n",
            "Val 89/300 32/40 , dice_tc: 0.50404453 , dice_wt: 0.88259226 , dice_et: 0.59930015 , time 2.50s\n",
            "Val 89/300 33/40 , dice_tc: 0.5078376 , dice_wt: 0.88068557 , dice_et: 0.60366714 , time 2.64s\n",
            "Val 89/300 34/40 , dice_tc: 0.51540875 , dice_wt: 0.8792052 , dice_et: 0.6110291 , time 2.47s\n",
            "Val 89/300 35/40 , dice_tc: 0.5194141 , dice_wt: 0.87876105 , dice_et: 0.61391735 , time 2.63s\n",
            "Val 89/300 36/40 , dice_tc: 0.53054583 , dice_wt: 0.88003755 , dice_et: 0.6239485 , time 2.45s\n",
            "Val 89/300 37/40 , dice_tc: 0.5213596 , dice_wt: 0.8814577 , dice_et: 0.61785966 , time 2.69s\n",
            "Val 89/300 38/40 , dice_tc: 0.5283717 , dice_wt: 0.8827545 , dice_et: 0.62435484 , time 2.71s\n",
            "Val 89/300 39/40 , dice_tc: 0.51750773 , dice_wt: 0.882287 , dice_et: 0.62435484 , time 2.66s\n",
            "Final validation stats 89/299 , dice_tc: 0.51750773 , dice_wt: 0.882287 , dice_et: 0.62435484 , Dice_Avg: 0.67471653 , LR: 0.000025 , time 104.68s\n",
            "Thu Dec 18 09:50:02 2025 Epoch: 90\n",
            "Epoch 90/300 10/80 loss: 0.5784 time 2.82s\n",
            "Epoch 90/300 20/80 loss: 0.5825 time 2.85s\n",
            "Epoch 90/300 30/80 loss: 0.5615 time 3.11s\n",
            "Epoch 90/300 40/80 loss: 0.5461 time 2.83s\n",
            "Epoch 90/300 50/80 loss: 0.5529 time 3.12s\n",
            "Epoch 90/300 60/80 loss: 0.5540 time 3.09s\n",
            "Epoch 90/300 70/80 loss: 0.5541 time 2.82s\n",
            "Epoch 90/300 80/80 loss: 0.5516 time 1.67s\n",
            "Final training  90/299 loss: 0.5516 time 226.17s\n",
            "Thu Dec 18 09:53:48 2025 Epoch: 91\n",
            "Epoch 91/300 10/80 loss: 0.5857 time 2.88s\n",
            "Epoch 91/300 20/80 loss: 0.5567 time 2.98s\n",
            "Epoch 91/300 30/80 loss: 0.5659 time 3.14s\n",
            "Epoch 91/300 40/80 loss: 0.5683 time 2.70s\n",
            "Epoch 91/300 50/80 loss: 0.5646 time 2.69s\n",
            "Epoch 91/300 60/80 loss: 0.5647 time 3.08s\n",
            "Epoch 91/300 70/80 loss: 0.5621 time 2.93s\n",
            "Epoch 91/300 80/80 loss: 0.5546 time 1.41s\n",
            "Final training  91/299 loss: 0.5546 time 227.41s\n",
            "Thu Dec 18 09:57:36 2025 Epoch: 92\n",
            "Epoch 92/300 10/80 loss: 0.5388 time 2.94s\n",
            "Epoch 92/300 20/80 loss: 0.5406 time 2.83s\n",
            "Epoch 92/300 30/80 loss: 0.5277 time 3.03s\n",
            "Epoch 92/300 40/80 loss: 0.5366 time 2.78s\n",
            "Epoch 92/300 50/80 loss: 0.5450 time 2.87s\n",
            "Epoch 92/300 60/80 loss: 0.5495 time 2.99s\n",
            "Epoch 92/300 70/80 loss: 0.5449 time 2.70s\n",
            "Epoch 92/300 80/80 loss: 0.5408 time 1.63s\n",
            "Final training  92/299 loss: 0.5408 time 227.17s\n",
            "Thu Dec 18 10:01:23 2025 Epoch: 93\n",
            "Epoch 93/300 10/80 loss: 0.5279 time 3.14s\n",
            "Epoch 93/300 20/80 loss: 0.5327 time 3.00s\n",
            "Epoch 93/300 30/80 loss: 0.5426 time 3.01s\n",
            "Epoch 93/300 40/80 loss: 0.5516 time 3.14s\n",
            "Epoch 93/300 50/80 loss: 0.5491 time 2.82s\n",
            "Epoch 93/300 60/80 loss: 0.5478 time 3.12s\n",
            "Epoch 93/300 70/80 loss: 0.5552 time 3.08s\n",
            "Epoch 93/300 80/80 loss: 0.5534 time 1.60s\n",
            "Final training  93/299 loss: 0.5534 time 227.82s\n",
            "Thu Dec 18 10:05:11 2025 Epoch: 94\n",
            "Epoch 94/300 10/80 loss: 0.5564 time 2.86s\n",
            "Epoch 94/300 20/80 loss: 0.5470 time 2.68s\n",
            "Epoch 94/300 30/80 loss: 0.5314 time 2.92s\n",
            "Epoch 94/300 40/80 loss: 0.5276 time 3.18s\n",
            "Epoch 94/300 50/80 loss: 0.5365 time 2.83s\n",
            "Epoch 94/300 60/80 loss: 0.5311 time 2.83s\n",
            "Epoch 94/300 70/80 loss: 0.5352 time 2.93s\n",
            "Epoch 94/300 80/80 loss: 0.5353 time 1.45s\n",
            "Final training  94/299 loss: 0.5353 time 228.45s\n",
            "Thu Dec 18 10:08:59 2025 Epoch: 95\n",
            "Epoch 95/300 10/80 loss: 0.5344 time 2.85s\n",
            "Epoch 95/300 20/80 loss: 0.5391 time 3.14s\n",
            "Epoch 95/300 30/80 loss: 0.5277 time 2.85s\n",
            "Epoch 95/300 40/80 loss: 0.5272 time 3.05s\n",
            "Epoch 95/300 50/80 loss: 0.5269 time 3.12s\n",
            "Epoch 95/300 60/80 loss: 0.5283 time 3.09s\n",
            "Epoch 95/300 70/80 loss: 0.5271 time 2.71s\n",
            "Epoch 95/300 80/80 loss: 0.5235 time 1.58s\n",
            "Final training  95/299 loss: 0.5235 time 228.26s\n",
            "Thu Dec 18 10:12:48 2025 Epoch: 96\n",
            "Epoch 96/300 10/80 loss: 0.5292 time 2.59s\n",
            "Epoch 96/300 20/80 loss: 0.5098 time 3.18s\n",
            "Epoch 96/300 30/80 loss: 0.5208 time 3.11s\n",
            "Epoch 96/300 40/80 loss: 0.5240 time 3.02s\n",
            "Epoch 96/300 50/80 loss: 0.5291 time 2.88s\n",
            "Epoch 96/300 60/80 loss: 0.5288 time 3.06s\n",
            "Epoch 96/300 70/80 loss: 0.5265 time 2.88s\n",
            "Epoch 96/300 80/80 loss: 0.5256 time 1.49s\n",
            "Final training  96/299 loss: 0.5256 time 228.79s\n",
            "Thu Dec 18 10:16:36 2025 Epoch: 97\n",
            "Epoch 97/300 10/80 loss: 0.5676 time 2.85s\n",
            "Epoch 97/300 20/80 loss: 0.5462 time 3.06s\n",
            "Epoch 97/300 30/80 loss: 0.5397 time 3.09s\n",
            "Epoch 97/300 40/80 loss: 0.5374 time 2.64s\n",
            "Epoch 97/300 50/80 loss: 0.5374 time 2.86s\n",
            "Epoch 97/300 60/80 loss: 0.5325 time 3.07s\n",
            "Epoch 97/300 70/80 loss: 0.5311 time 3.02s\n",
            "Epoch 97/300 80/80 loss: 0.5280 time 1.43s\n",
            "Final training  97/299 loss: 0.5280 time 227.78s\n",
            "Thu Dec 18 10:20:24 2025 Epoch: 98\n",
            "Epoch 98/300 10/80 loss: 0.5426 time 2.64s\n",
            "Epoch 98/300 20/80 loss: 0.5338 time 3.05s\n",
            "Epoch 98/300 30/80 loss: 0.5389 time 3.16s\n",
            "Epoch 98/300 40/80 loss: 0.5274 time 3.08s\n",
            "Epoch 98/300 50/80 loss: 0.5278 time 3.05s\n",
            "Epoch 98/300 60/80 loss: 0.5296 time 2.80s\n",
            "Epoch 98/300 70/80 loss: 0.5304 time 2.87s\n",
            "Epoch 98/300 80/80 loss: 0.5326 time 1.42s\n",
            "Final training  98/299 loss: 0.5326 time 228.36s\n",
            "Thu Dec 18 10:24:12 2025 Epoch: 99\n",
            "Epoch 99/300 10/80 loss: 0.5353 time 2.87s\n",
            "Epoch 99/300 20/80 loss: 0.5382 time 2.76s\n",
            "Epoch 99/300 30/80 loss: 0.5307 time 3.10s\n",
            "Epoch 99/300 40/80 loss: 0.5278 time 2.60s\n",
            "Epoch 99/300 50/80 loss: 0.5338 time 3.04s\n",
            "Epoch 99/300 60/80 loss: 0.5372 time 2.94s\n",
            "Epoch 99/300 70/80 loss: 0.5330 time 2.89s\n",
            "Epoch 99/300 80/80 loss: 0.5313 time 1.66s\n",
            "Final training  99/299 loss: 0.5313 time 228.54s\n",
            "Val 99/300 0/40 , dice_tc: 0.6614585 , dice_wt: 0.9447303 , dice_et: 0.7676832 , time 2.82s\n",
            "Val 99/300 1/40 , dice_tc: 0.40185976 , dice_wt: 0.95816696 , dice_et: 0.7676832 , time 2.73s\n",
            "Val 99/300 2/40 , dice_tc: 0.5697891 , dice_wt: 0.93693566 , dice_et: 0.84308153 , time 2.69s\n",
            "Val 99/300 3/40 , dice_tc: 0.60174143 , dice_wt: 0.92855984 , dice_et: 0.82608676 , time 2.69s\n",
            "Val 99/300 4/40 , dice_tc: 0.5574328 , dice_wt: 0.8438697 , dice_et: 0.6595233 , time 2.54s\n",
            "Val 99/300 5/40 , dice_tc: 0.53840494 , dice_wt: 0.83730775 , dice_et: 0.6226899 , time 2.63s\n",
            "Val 99/300 6/40 , dice_tc: 0.56948924 , dice_wt: 0.8409726 , dice_et: 0.6508612 , time 2.69s\n",
            "Val 99/300 7/40 , dice_tc: 0.58919805 , dice_wt: 0.84108466 , dice_et: 0.67102736 , time 2.56s\n",
            "Val 99/300 8/40 , dice_tc: 0.58946896 , dice_wt: 0.8516444 , dice_et: 0.68161845 , time 2.74s\n",
            "Val 99/300 9/40 , dice_tc: 0.6096774 , dice_wt: 0.8523434 , dice_et: 0.6783852 , time 2.67s\n",
            "Val 99/300 10/40 , dice_tc: 0.61229324 , dice_wt: 0.8568029 , dice_et: 0.682424 , time 2.49s\n",
            "Val 99/300 11/40 , dice_tc: 0.6083786 , dice_wt: 0.85430056 , dice_et: 0.6802895 , time 2.66s\n",
            "Val 99/300 12/40 , dice_tc: 0.5626533 , dice_wt: 0.86080116 , dice_et: 0.6802895 , time 2.48s\n",
            "Val 99/300 13/40 , dice_tc: 0.54992765 , dice_wt: 0.85740757 , dice_et: 0.67222595 , time 2.54s\n",
            "Val 99/300 14/40 , dice_tc: 0.53510267 , dice_wt: 0.86175746 , dice_et: 0.66088486 , time 2.69s\n",
            "Val 99/300 15/40 , dice_tc: 0.54106647 , dice_wt: 0.865358 , dice_et: 0.66919535 , time 2.75s\n",
            "Val 99/300 16/40 , dice_tc: 0.5260709 , dice_wt: 0.8634842 , dice_et: 0.6458113 , time 2.67s\n",
            "Val 99/300 17/40 , dice_tc: 0.51920605 , dice_wt: 0.8667981 , dice_et: 0.64683735 , time 2.54s\n",
            "Val 99/300 18/40 , dice_tc: 0.53307736 , dice_wt: 0.8624084 , dice_et: 0.655666 , time 2.77s\n",
            "Val 99/300 19/40 , dice_tc: 0.5387355 , dice_wt: 0.86469424 , dice_et: 0.6608724 , time 2.68s\n",
            "Val 99/300 20/40 , dice_tc: 0.5317022 , dice_wt: 0.86716413 , dice_et: 0.65368676 , time 2.48s\n",
            "Val 99/300 21/40 , dice_tc: 0.54785657 , dice_wt: 0.86976874 , dice_et: 0.66604984 , time 2.46s\n",
            "Val 99/300 22/40 , dice_tc: 0.5355821 , dice_wt: 0.87346274 , dice_et: 0.65326774 , time 2.67s\n",
            "Val 99/300 23/40 , dice_tc: 0.5281999 , dice_wt: 0.8765755 , dice_et: 0.6484713 , time 2.81s\n",
            "Val 99/300 24/40 , dice_tc: 0.5202299 , dice_wt: 0.87964606 , dice_et: 0.6404866 , time 2.76s\n",
            "Val 99/300 25/40 , dice_tc: 0.5068337 , dice_wt: 0.8823074 , dice_et: 0.6404866 , time 2.74s\n",
            "Val 99/300 26/40 , dice_tc: 0.51765627 , dice_wt: 0.88294715 , dice_et: 0.64876133 , time 2.68s\n",
            "Val 99/300 27/40 , dice_tc: 0.51866615 , dice_wt: 0.88259834 , dice_et: 0.6374708 , time 2.71s\n",
            "Val 99/300 28/40 , dice_tc: 0.51696837 , dice_wt: 0.8829021 , dice_et: 0.6359381 , time 2.63s\n",
            "Val 99/300 29/40 , dice_tc: 0.5024362 , dice_wt: 0.8830391 , dice_et: 0.6171259 , time 2.54s\n",
            "Val 99/300 30/40 , dice_tc: 0.50039256 , dice_wt: 0.88366854 , dice_et: 0.5997423 , time 2.73s\n",
            "Val 99/300 31/40 , dice_tc: 0.5108575 , dice_wt: 0.8850771 , dice_et: 0.6096052 , time 2.70s\n",
            "Val 99/300 32/40 , dice_tc: 0.51722336 , dice_wt: 0.881387 , dice_et: 0.6096052 , time 2.65s\n",
            "Val 99/300 33/40 , dice_tc: 0.521629 , dice_wt: 0.88054156 , dice_et: 0.6149504 , time 2.66s\n",
            "Val 99/300 34/40 , dice_tc: 0.52795655 , dice_wt: 0.8794137 , dice_et: 0.62156934 , time 2.49s\n",
            "Val 99/300 35/40 , dice_tc: 0.5290025 , dice_wt: 0.8787469 , dice_et: 0.62139094 , time 2.71s\n",
            "Val 99/300 36/40 , dice_tc: 0.540244 , dice_wt: 0.87993 , dice_et: 0.63159657 , time 2.53s\n",
            "Val 99/300 37/40 , dice_tc: 0.5304441 , dice_wt: 0.8814696 , dice_et: 0.6254619 , time 2.74s\n",
            "Val 99/300 38/40 , dice_tc: 0.5376295 , dice_wt: 0.8827133 , dice_et: 0.6324037 , time 2.70s\n",
            "Val 99/300 39/40 , dice_tc: 0.5265825 , dice_wt: 0.883399 , dice_et: 0.6324037 , time 2.68s\n",
            "Final validation stats 99/299 , dice_tc: 0.5265825 , dice_wt: 0.883399 , dice_et: 0.6324037 , Dice_Avg: 0.680795 , LR: 0.000028 , time 106.13s\n",
            "Thu Dec 18 10:29:47 2025 Epoch: 100\n",
            "Epoch 100/300 10/80 loss: 0.5463 time 2.97s\n",
            "Epoch 100/300 20/80 loss: 0.5424 time 2.94s\n",
            "Epoch 100/300 30/80 loss: 0.5330 time 3.04s\n",
            "Epoch 100/300 40/80 loss: 0.5305 time 2.99s\n",
            "Epoch 100/300 50/80 loss: 0.5286 time 3.05s\n",
            "Epoch 100/300 60/80 loss: 0.5347 time 3.10s\n",
            "Epoch 100/300 70/80 loss: 0.5325 time 3.03s\n",
            "Epoch 100/300 80/80 loss: 0.5272 time 1.57s\n",
            "Final training  100/299 loss: 0.5272 time 228.81s\n",
            "Thu Dec 18 10:33:36 2025 Epoch: 101\n",
            "Epoch 101/300 10/80 loss: 0.5161 time 2.87s\n",
            "Epoch 101/300 20/80 loss: 0.5167 time 3.10s\n",
            "Epoch 101/300 30/80 loss: 0.5105 time 2.89s\n",
            "Epoch 101/300 40/80 loss: 0.5150 time 2.89s\n",
            "Epoch 101/300 50/80 loss: 0.5178 time 2.70s\n",
            "Epoch 101/300 60/80 loss: 0.5095 time 3.08s\n",
            "Epoch 101/300 70/80 loss: 0.5124 time 2.79s\n",
            "Epoch 101/300 80/80 loss: 0.5169 time 1.36s\n",
            "Final training  101/299 loss: 0.5169 time 228.37s\n",
            "Thu Dec 18 10:37:24 2025 Epoch: 102\n",
            "Epoch 102/300 10/80 loss: 0.5262 time 2.86s\n",
            "Epoch 102/300 20/80 loss: 0.5244 time 2.73s\n",
            "Epoch 102/300 30/80 loss: 0.5211 time 2.91s\n",
            "Epoch 102/300 40/80 loss: 0.5253 time 3.18s\n",
            "Epoch 102/300 50/80 loss: 0.5206 time 3.04s\n",
            "Epoch 102/300 60/80 loss: 0.5239 time 2.72s\n",
            "Epoch 102/300 70/80 loss: 0.5213 time 3.01s\n",
            "Epoch 102/300 80/80 loss: 0.5169 time 1.64s\n",
            "Final training  102/299 loss: 0.5169 time 228.30s\n",
            "Thu Dec 18 10:41:13 2025 Epoch: 103\n",
            "Epoch 103/300 10/80 loss: 0.5129 time 3.04s\n",
            "Epoch 103/300 20/80 loss: 0.4926 time 3.11s\n",
            "Epoch 103/300 30/80 loss: 0.4945 time 2.88s\n",
            "Epoch 103/300 40/80 loss: 0.4997 time 2.66s\n",
            "Epoch 103/300 50/80 loss: 0.4998 time 2.87s\n",
            "Epoch 103/300 60/80 loss: 0.4988 time 2.87s\n",
            "Epoch 103/300 70/80 loss: 0.5048 time 2.90s\n",
            "Epoch 103/300 80/80 loss: 0.5054 time 1.64s\n",
            "Final training  103/299 loss: 0.5054 time 228.75s\n",
            "Thu Dec 18 10:45:01 2025 Epoch: 104\n",
            "Epoch 104/300 10/80 loss: 0.4955 time 2.86s\n",
            "Epoch 104/300 20/80 loss: 0.5071 time 3.08s\n",
            "Epoch 104/300 30/80 loss: 0.4978 time 2.93s\n",
            "Epoch 104/300 40/80 loss: 0.4960 time 3.13s\n",
            "Epoch 104/300 50/80 loss: 0.4985 time 3.10s\n",
            "Epoch 104/300 60/80 loss: 0.4986 time 2.83s\n",
            "Epoch 104/300 70/80 loss: 0.4969 time 3.03s\n",
            "Epoch 104/300 80/80 loss: 0.4958 time 1.45s\n",
            "Final training  104/299 loss: 0.4958 time 228.65s\n",
            "Thu Dec 18 10:48:50 2025 Epoch: 105\n",
            "Epoch 105/300 10/80 loss: 0.4829 time 2.93s\n",
            "Epoch 105/300 20/80 loss: 0.4785 time 3.14s\n",
            "Epoch 105/300 30/80 loss: 0.4854 time 3.00s\n",
            "Epoch 105/300 40/80 loss: 0.4856 time 3.07s\n",
            "Epoch 105/300 50/80 loss: 0.4846 time 2.87s\n",
            "Epoch 105/300 60/80 loss: 0.4865 time 3.02s\n",
            "Epoch 105/300 70/80 loss: 0.4915 time 2.64s\n",
            "Epoch 105/300 80/80 loss: 0.4911 time 1.66s\n",
            "Final training  105/299 loss: 0.4911 time 228.49s\n",
            "Thu Dec 18 10:52:39 2025 Epoch: 106\n",
            "Epoch 106/300 10/80 loss: 0.5094 time 3.05s\n",
            "Epoch 106/300 20/80 loss: 0.5114 time 2.69s\n",
            "Epoch 106/300 30/80 loss: 0.5133 time 3.01s\n",
            "Epoch 106/300 40/80 loss: 0.5073 time 2.62s\n",
            "Epoch 106/300 50/80 loss: 0.4958 time 3.10s\n",
            "Epoch 106/300 60/80 loss: 0.5018 time 2.90s\n",
            "Epoch 106/300 70/80 loss: 0.4948 time 3.04s\n",
            "Epoch 106/300 80/80 loss: 0.4978 time 1.45s\n",
            "Final training  106/299 loss: 0.4978 time 228.82s\n",
            "Thu Dec 18 10:56:27 2025 Epoch: 107\n",
            "Epoch 107/300 10/80 loss: 0.5034 time 3.15s\n",
            "Epoch 107/300 20/80 loss: 0.5076 time 2.90s\n",
            "Epoch 107/300 30/80 loss: 0.4912 time 2.81s\n",
            "Epoch 107/300 40/80 loss: 0.4899 time 2.65s\n",
            "Epoch 107/300 50/80 loss: 0.4825 time 2.84s\n",
            "Epoch 107/300 60/80 loss: 0.4880 time 3.03s\n",
            "Epoch 107/300 70/80 loss: 0.4876 time 2.68s\n",
            "Epoch 107/300 80/80 loss: 0.4883 time 1.60s\n",
            "Final training  107/299 loss: 0.4883 time 229.18s\n",
            "Thu Dec 18 11:00:17 2025 Epoch: 108\n",
            "Epoch 108/300 10/80 loss: 0.4524 time 3.05s\n",
            "Epoch 108/300 20/80 loss: 0.4833 time 2.91s\n",
            "Epoch 108/300 30/80 loss: 0.5022 time 2.79s\n",
            "Epoch 108/300 40/80 loss: 0.5061 time 3.08s\n",
            "Epoch 108/300 50/80 loss: 0.5093 time 2.86s\n",
            "Epoch 108/300 60/80 loss: 0.4979 time 2.96s\n",
            "Epoch 108/300 70/80 loss: 0.4875 time 2.89s\n",
            "Epoch 108/300 80/80 loss: 0.4883 time 1.65s\n",
            "Final training  108/299 loss: 0.4883 time 228.47s\n",
            "Thu Dec 18 11:04:05 2025 Epoch: 109\n",
            "Epoch 109/300 10/80 loss: 0.4567 time 2.75s\n",
            "Epoch 109/300 20/80 loss: 0.4673 time 2.86s\n",
            "Epoch 109/300 30/80 loss: 0.4615 time 2.68s\n",
            "Epoch 109/300 40/80 loss: 0.4703 time 3.08s\n",
            "Epoch 109/300 50/80 loss: 0.4686 time 2.85s\n",
            "Epoch 109/300 60/80 loss: 0.4698 time 2.90s\n",
            "Epoch 109/300 70/80 loss: 0.4694 time 2.83s\n",
            "Epoch 109/300 80/80 loss: 0.4691 time 1.60s\n",
            "Final training  109/299 loss: 0.4691 time 227.12s\n",
            "Val 109/300 0/40 , dice_tc: 0.7993921 , dice_wt: 0.945554 , dice_et: 0.8358097 , time 2.73s\n",
            "Val 109/300 1/40 , dice_tc: 0.83020616 , dice_wt: 0.95985305 , dice_et: 0.8358097 , time 2.73s\n",
            "Val 109/300 2/40 , dice_tc: 0.8678791 , dice_wt: 0.9428608 , dice_et: 0.89147496 , time 2.69s\n",
            "Val 109/300 3/40 , dice_tc: 0.8580475 , dice_wt: 0.9252661 , dice_et: 0.8853321 , time 2.67s\n",
            "Val 109/300 4/40 , dice_tc: 0.7295218 , dice_wt: 0.8428074 , dice_et: 0.6963314 , time 2.55s\n",
            "Val 109/300 5/40 , dice_tc: 0.729178 , dice_wt: 0.8430701 , dice_et: 0.7037226 , time 2.55s\n",
            "Val 109/300 6/40 , dice_tc: 0.7445114 , dice_wt: 0.84868765 , dice_et: 0.72955567 , time 2.78s\n",
            "Val 109/300 7/40 , dice_tc: 0.74626666 , dice_wt: 0.8494113 , dice_et: 0.7368256 , time 2.55s\n",
            "Val 109/300 8/40 , dice_tc: 0.752922 , dice_wt: 0.8588762 , dice_et: 0.7501576 , time 2.74s\n",
            "Val 109/300 9/40 , dice_tc: 0.76209706 , dice_wt: 0.86513233 , dice_et: 0.7662046 , time 2.51s\n",
            "Val 109/300 10/40 , dice_tc: 0.7555275 , dice_wt: 0.86611754 , dice_et: 0.76153123 , time 2.48s\n",
            "Val 109/300 11/40 , dice_tc: 0.7428613 , dice_wt: 0.8613694 , dice_et: 0.7510865 , time 2.69s\n",
            "Val 109/300 12/40 , dice_tc: 0.68844026 , dice_wt: 0.86644965 , dice_et: 0.7510865 , time 2.54s\n",
            "Val 109/300 13/40 , dice_tc: 0.6773228 , dice_wt: 0.8626804 , dice_et: 0.7342302 , time 2.52s\n",
            "Val 109/300 14/40 , dice_tc: 0.66680866 , dice_wt: 0.86530966 , dice_et: 0.6783933 , time 2.64s\n",
            "Val 109/300 15/40 , dice_tc: 0.67459244 , dice_wt: 0.86778975 , dice_et: 0.69097024 , time 2.82s\n",
            "Val 109/300 16/40 , dice_tc: 0.66064537 , dice_wt: 0.8670799 , dice_et: 0.6739577 , time 2.73s\n",
            "Val 109/300 17/40 , dice_tc: 0.6552435 , dice_wt: 0.87029594 , dice_et: 0.66863513 , time 2.53s\n",
            "Val 109/300 18/40 , dice_tc: 0.6709626 , dice_wt: 0.8688268 , dice_et: 0.68522674 , time 2.77s\n",
            "Val 109/300 19/40 , dice_tc: 0.6752868 , dice_wt: 0.87001693 , dice_et: 0.6909455 , time 2.71s\n",
            "Val 109/300 20/40 , dice_tc: 0.67036486 , dice_wt: 0.8724962 , dice_et: 0.688771 , time 2.46s\n",
            "Val 109/300 21/40 , dice_tc: 0.68178403 , dice_wt: 0.8739651 , dice_et: 0.7006391 , time 2.52s\n",
            "Val 109/300 22/40 , dice_tc: 0.6747282 , dice_wt: 0.87784255 , dice_et: 0.6960575 , time 2.71s\n",
            "Val 109/300 23/40 , dice_tc: 0.66789794 , dice_wt: 0.8804879 , dice_et: 0.68931574 , time 2.76s\n",
            "Val 109/300 24/40 , dice_tc: 0.6656677 , dice_wt: 0.8837798 , dice_et: 0.6887749 , time 2.80s\n",
            "Val 109/300 25/40 , dice_tc: 0.65556026 , dice_wt: 0.88607097 , dice_et: 0.6887749 , time 2.74s\n",
            "Val 109/300 26/40 , dice_tc: 0.66231436 , dice_wt: 0.88572997 , dice_et: 0.6955523 , time 2.73s\n",
            "Val 109/300 27/40 , dice_tc: 0.65409696 , dice_wt: 0.8834934 , dice_et: 0.68171674 , time 2.71s\n",
            "Val 109/300 28/40 , dice_tc: 0.65376806 , dice_wt: 0.88293374 , dice_et: 0.68269694 , time 2.65s\n",
            "Val 109/300 29/40 , dice_tc: 0.63686216 , dice_wt: 0.8821936 , dice_et: 0.6647019 , time 2.50s\n",
            "Val 109/300 30/40 , dice_tc: 0.6389153 , dice_wt: 0.8827619 , dice_et: 0.6480575 , time 2.68s\n",
            "Val 109/300 31/40 , dice_tc: 0.64710355 , dice_wt: 0.88428575 , dice_et: 0.65736413 , time 2.67s\n",
            "Val 109/300 32/40 , dice_tc: 0.6415396 , dice_wt: 0.88265365 , dice_et: 0.65736413 , time 2.54s\n",
            "Val 109/300 33/40 , dice_tc: 0.6448187 , dice_wt: 0.88087815 , dice_et: 0.66271716 , time 2.76s\n",
            "Val 109/300 34/40 , dice_tc: 0.6516432 , dice_wt: 0.8805319 , dice_et: 0.67048806 , time 2.48s\n",
            "Val 109/300 35/40 , dice_tc: 0.65775794 , dice_wt: 0.88180625 , dice_et: 0.67738587 , time 2.67s\n",
            "Val 109/300 36/40 , dice_tc: 0.6661841 , dice_wt: 0.88324654 , dice_et: 0.6863086 , time 2.51s\n",
            "Val 109/300 37/40 , dice_tc: 0.66138643 , dice_wt: 0.8844314 , dice_et: 0.6827096 , time 2.72s\n",
            "Val 109/300 38/40 , dice_tc: 0.66747147 , dice_wt: 0.88554364 , dice_et: 0.68883795 , time 2.72s\n",
            "Val 109/300 39/40 , dice_tc: 0.65579873 , dice_wt: 0.8851489 , dice_et: 0.68883795 , time 2.70s\n",
            "Final validation stats 109/299 , dice_tc: 0.65579873 , dice_wt: 0.8851489 , dice_et: 0.68883795 , Dice_Avg: 0.7432619 , LR: 0.000030 , time 105.96s\n",
            "new best (0.711299 --> 0.743262). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 11:09:40 2025 Epoch: 110\n",
            "Epoch 110/300 10/80 loss: 0.4801 time 2.68s\n",
            "Epoch 110/300 20/80 loss: 0.4783 time 3.06s\n",
            "Epoch 110/300 30/80 loss: 0.4807 time 2.61s\n",
            "Epoch 110/300 40/80 loss: 0.4803 time 3.04s\n",
            "Epoch 110/300 50/80 loss: 0.4834 time 3.01s\n",
            "Epoch 110/300 60/80 loss: 0.4776 time 2.67s\n",
            "Epoch 110/300 70/80 loss: 0.4699 time 2.98s\n",
            "Epoch 110/300 80/80 loss: 0.4703 time 1.62s\n",
            "Final training  110/299 loss: 0.4703 time 226.54s\n",
            "Thu Dec 18 11:13:27 2025 Epoch: 111\n",
            "Epoch 111/300 10/80 loss: 0.4406 time 2.99s\n",
            "Epoch 111/300 20/80 loss: 0.4623 time 3.16s\n",
            "Epoch 111/300 30/80 loss: 0.4632 time 2.98s\n",
            "Epoch 111/300 40/80 loss: 0.4669 time 2.65s\n",
            "Epoch 111/300 50/80 loss: 0.4588 time 2.98s\n",
            "Epoch 111/300 60/80 loss: 0.4610 time 3.06s\n",
            "Epoch 111/300 70/80 loss: 0.4662 time 2.77s\n",
            "Epoch 111/300 80/80 loss: 0.4700 time 1.47s\n",
            "Final training  111/299 loss: 0.4700 time 226.34s\n",
            "Thu Dec 18 11:17:13 2025 Epoch: 112\n",
            "Epoch 112/300 10/80 loss: 0.4904 time 3.11s\n",
            "Epoch 112/300 20/80 loss: 0.5022 time 2.72s\n",
            "Epoch 112/300 30/80 loss: 0.4945 time 2.71s\n",
            "Epoch 112/300 40/80 loss: 0.4844 time 2.87s\n",
            "Epoch 112/300 50/80 loss: 0.4784 time 3.07s\n",
            "Epoch 112/300 60/80 loss: 0.4693 time 2.98s\n",
            "Epoch 112/300 70/80 loss: 0.4695 time 3.03s\n",
            "Epoch 112/300 80/80 loss: 0.4671 time 1.61s\n",
            "Final training  112/299 loss: 0.4671 time 227.02s\n",
            "Thu Dec 18 11:21:00 2025 Epoch: 113\n",
            "Epoch 113/300 10/80 loss: 0.4566 time 3.07s\n",
            "Epoch 113/300 20/80 loss: 0.4710 time 2.84s\n",
            "Epoch 113/300 30/80 loss: 0.4571 time 2.64s\n",
            "Epoch 113/300 40/80 loss: 0.4577 time 2.91s\n",
            "Epoch 113/300 50/80 loss: 0.4532 time 2.63s\n",
            "Epoch 113/300 60/80 loss: 0.4533 time 2.91s\n",
            "Epoch 113/300 70/80 loss: 0.4537 time 3.07s\n",
            "Epoch 113/300 80/80 loss: 0.4490 time 1.65s\n",
            "Final training  113/299 loss: 0.4490 time 228.02s\n",
            "Thu Dec 18 11:24:48 2025 Epoch: 114\n",
            "Epoch 114/300 10/80 loss: 0.4062 time 2.63s\n",
            "Epoch 114/300 20/80 loss: 0.4295 time 3.03s\n",
            "Epoch 114/300 30/80 loss: 0.4489 time 2.84s\n",
            "Epoch 114/300 40/80 loss: 0.4464 time 2.67s\n",
            "Epoch 114/300 50/80 loss: 0.4521 time 2.86s\n",
            "Epoch 114/300 60/80 loss: 0.4524 time 2.82s\n",
            "Epoch 114/300 70/80 loss: 0.4471 time 2.93s\n",
            "Epoch 114/300 80/80 loss: 0.4451 time 1.57s\n",
            "Final training  114/299 loss: 0.4451 time 226.85s\n",
            "Thu Dec 18 11:28:35 2025 Epoch: 115\n",
            "Epoch 115/300 10/80 loss: 0.4809 time 2.84s\n",
            "Epoch 115/300 20/80 loss: 0.4681 time 3.00s\n",
            "Epoch 115/300 30/80 loss: 0.4622 time 2.82s\n",
            "Epoch 115/300 40/80 loss: 0.4715 time 2.64s\n",
            "Epoch 115/300 50/80 loss: 0.4712 time 3.16s\n",
            "Epoch 115/300 60/80 loss: 0.4601 time 2.87s\n",
            "Epoch 115/300 70/80 loss: 0.4573 time 3.04s\n",
            "Epoch 115/300 80/80 loss: 0.4565 time 1.40s\n",
            "Final training  115/299 loss: 0.4565 time 226.50s\n",
            "Thu Dec 18 11:32:22 2025 Epoch: 116\n",
            "Epoch 116/300 10/80 loss: 0.4677 time 3.02s\n",
            "Epoch 116/300 20/80 loss: 0.4682 time 3.12s\n",
            "Epoch 116/300 30/80 loss: 0.4551 time 2.65s\n",
            "Epoch 116/300 40/80 loss: 0.4676 time 3.08s\n",
            "Epoch 116/300 50/80 loss: 0.4600 time 2.86s\n",
            "Epoch 116/300 60/80 loss: 0.4579 time 3.03s\n",
            "Epoch 116/300 70/80 loss: 0.4518 time 2.70s\n",
            "Epoch 116/300 80/80 loss: 0.4506 time 1.69s\n",
            "Final training  116/299 loss: 0.4506 time 226.54s\n",
            "Thu Dec 18 11:36:08 2025 Epoch: 117\n",
            "Epoch 117/300 10/80 loss: 0.5052 time 3.02s\n",
            "Epoch 117/300 20/80 loss: 0.4807 time 3.06s\n",
            "Epoch 117/300 30/80 loss: 0.4492 time 3.08s\n",
            "Epoch 117/300 40/80 loss: 0.4501 time 2.84s\n",
            "Epoch 117/300 50/80 loss: 0.4541 time 2.82s\n",
            "Epoch 117/300 60/80 loss: 0.4597 time 3.08s\n",
            "Epoch 117/300 70/80 loss: 0.4625 time 2.67s\n",
            "Epoch 117/300 80/80 loss: 0.4534 time 1.44s\n",
            "Final training  117/299 loss: 0.4534 time 226.35s\n",
            "Thu Dec 18 11:39:55 2025 Epoch: 118\n",
            "Epoch 118/300 10/80 loss: 0.4530 time 2.94s\n",
            "Epoch 118/300 20/80 loss: 0.4135 time 2.79s\n",
            "Epoch 118/300 30/80 loss: 0.4143 time 2.81s\n",
            "Epoch 118/300 40/80 loss: 0.4079 time 2.87s\n",
            "Epoch 118/300 50/80 loss: 0.4152 time 3.01s\n",
            "Epoch 118/300 60/80 loss: 0.4151 time 2.87s\n",
            "Epoch 118/300 70/80 loss: 0.4145 time 3.04s\n",
            "Epoch 118/300 80/80 loss: 0.4124 time 1.65s\n",
            "Final training  118/299 loss: 0.4124 time 226.40s\n",
            "Thu Dec 18 11:43:41 2025 Epoch: 119\n",
            "Epoch 119/300 10/80 loss: 0.4271 time 2.84s\n",
            "Epoch 119/300 20/80 loss: 0.4230 time 2.97s\n",
            "Epoch 119/300 30/80 loss: 0.4322 time 2.88s\n",
            "Epoch 119/300 40/80 loss: 0.4246 time 2.88s\n",
            "Epoch 119/300 50/80 loss: 0.4246 time 2.86s\n",
            "Epoch 119/300 60/80 loss: 0.4195 time 2.63s\n",
            "Epoch 119/300 70/80 loss: 0.4282 time 2.80s\n",
            "Epoch 119/300 80/80 loss: 0.4336 time 1.62s\n",
            "Final training  119/299 loss: 0.4336 time 225.33s\n",
            "Val 119/300 0/40 , dice_tc: 0.66968524 , dice_wt: 0.93146634 , dice_et: 0.81492704 , time 2.74s\n",
            "Val 119/300 1/40 , dice_tc: 0.46680808 , dice_wt: 0.9502114 , dice_et: 0.81492704 , time 2.67s\n",
            "Val 119/300 2/40 , dice_tc: 0.6217992 , dice_wt: 0.9311405 , dice_et: 0.87487286 , time 2.69s\n",
            "Val 119/300 3/40 , dice_tc: 0.658272 , dice_wt: 0.92256546 , dice_et: 0.8632827 , time 2.71s\n",
            "Val 119/300 4/40 , dice_tc: 0.6285838 , dice_wt: 0.84152544 , dice_et: 0.71788335 , time 2.53s\n",
            "Val 119/300 5/40 , dice_tc: 0.59588647 , dice_wt: 0.8354564 , dice_et: 0.6558231 , time 2.54s\n",
            "Val 119/300 6/40 , dice_tc: 0.59533024 , dice_wt: 0.82762337 , dice_et: 0.64602536 , time 2.70s\n",
            "Val 119/300 7/40 , dice_tc: 0.61515856 , dice_wt: 0.82359844 , dice_et: 0.6685948 , time 2.53s\n",
            "Val 119/300 8/40 , dice_tc: 0.62172085 , dice_wt: 0.8339863 , dice_et: 0.68957657 , time 2.79s\n",
            "Val 119/300 9/40 , dice_tc: 0.59944874 , dice_wt: 0.82709247 , dice_et: 0.63984835 , time 2.56s\n",
            "Val 119/300 10/40 , dice_tc: 0.62255543 , dice_wt: 0.83369994 , dice_et: 0.66245204 , time 2.46s\n",
            "Val 119/300 11/40 , dice_tc: 0.62259495 , dice_wt: 0.8360607 , dice_et: 0.66791767 , time 2.65s\n",
            "Val 119/300 12/40 , dice_tc: 0.5763742 , dice_wt: 0.8441074 , dice_et: 0.66791767 , time 2.46s\n",
            "Val 119/300 13/40 , dice_tc: 0.57441473 , dice_wt: 0.84048605 , dice_et: 0.66651875 , time 2.48s\n",
            "Val 119/300 14/40 , dice_tc: 0.56336653 , dice_wt: 0.84520304 , dice_et: 0.653566 , time 2.63s\n",
            "Val 119/300 15/40 , dice_tc: 0.5658908 , dice_wt: 0.84838116 , dice_et: 0.657255 , time 2.70s\n",
            "Val 119/300 16/40 , dice_tc: 0.54872686 , dice_wt: 0.84711725 , dice_et: 0.6326877 , time 2.67s\n",
            "Val 119/300 17/40 , dice_tc: 0.5493228 , dice_wt: 0.85133743 , dice_et: 0.63210905 , time 2.53s\n",
            "Val 119/300 18/40 , dice_tc: 0.56313205 , dice_wt: 0.8484397 , dice_et: 0.643106 , time 2.69s\n",
            "Val 119/300 19/40 , dice_tc: 0.57095456 , dice_wt: 0.85032284 , dice_et: 0.6514675 , time 2.66s\n",
            "Val 119/300 20/40 , dice_tc: 0.5671422 , dice_wt: 0.8535267 , dice_et: 0.6511681 , time 2.45s\n",
            "Val 119/300 21/40 , dice_tc: 0.5812984 , dice_wt: 0.8565348 , dice_et: 0.662356 , time 2.47s\n",
            "Val 119/300 22/40 , dice_tc: 0.56628096 , dice_wt: 0.8605165 , dice_et: 0.64971006 , time 2.78s\n",
            "Val 119/300 23/40 , dice_tc: 0.5635167 , dice_wt: 0.8640394 , dice_et: 0.65421844 , time 2.74s\n",
            "Val 119/300 24/40 , dice_tc: 0.55485404 , dice_wt: 0.86752486 , dice_et: 0.6486976 , time 2.74s\n",
            "Val 119/300 25/40 , dice_tc: 0.5431223 , dice_wt: 0.87030214 , dice_et: 0.6486976 , time 2.69s\n",
            "Val 119/300 26/40 , dice_tc: 0.55556935 , dice_wt: 0.8709777 , dice_et: 0.65907806 , time 2.75s\n",
            "Val 119/300 27/40 , dice_tc: 0.5610654 , dice_wt: 0.87119436 , dice_et: 0.6535587 , time 2.71s\n",
            "Val 119/300 28/40 , dice_tc: 0.562222 , dice_wt: 0.87254703 , dice_et: 0.6557628 , time 2.61s\n",
            "Val 119/300 29/40 , dice_tc: 0.54833305 , dice_wt: 0.8732466 , dice_et: 0.6406539 , time 2.46s\n",
            "Val 119/300 30/40 , dice_tc: 0.5496886 , dice_wt: 0.87456495 , dice_et: 0.6253991 , time 2.66s\n",
            "Val 119/300 31/40 , dice_tc: 0.56093276 , dice_wt: 0.87640154 , dice_et: 0.6365528 , time 2.72s\n",
            "Val 119/300 32/40 , dice_tc: 0.5664678 , dice_wt: 0.8752818 , dice_et: 0.6365528 , time 2.47s\n",
            "Val 119/300 33/40 , dice_tc: 0.5734576 , dice_wt: 0.8748146 , dice_et: 0.6444139 , time 2.66s\n",
            "Val 119/300 34/40 , dice_tc: 0.58167225 , dice_wt: 0.87365264 , dice_et: 0.65176857 , time 2.45s\n",
            "Val 119/300 35/40 , dice_tc: 0.57448435 , dice_wt: 0.8716671 , dice_et: 0.6415254 , time 2.69s\n",
            "Val 119/300 36/40 , dice_tc: 0.5834963 , dice_wt: 0.8717869 , dice_et: 0.64962435 , time 2.52s\n",
            "Val 119/300 37/40 , dice_tc: 0.57334685 , dice_wt: 0.8736446 , dice_et: 0.6480507 , time 2.70s\n",
            "Val 119/300 38/40 , dice_tc: 0.58024174 , dice_wt: 0.87474376 , dice_et: 0.65467614 , time 2.68s\n",
            "Val 119/300 39/40 , dice_tc: 0.56961775 , dice_wt: 0.8754215 , dice_et: 0.65467614 , time 2.67s\n",
            "Final validation stats 119/299 , dice_tc: 0.56961775 , dice_wt: 0.8754215 , dice_et: 0.65467614 , Dice_Avg: 0.69990516 , LR: 0.000033 , time 105.03s\n",
            "Thu Dec 18 11:49:11 2025 Epoch: 120\n",
            "Epoch 120/300 10/80 loss: 0.4287 time 2.90s\n",
            "Epoch 120/300 20/80 loss: 0.4542 time 2.96s\n",
            "Epoch 120/300 30/80 loss: 0.4536 time 3.01s\n",
            "Epoch 120/300 40/80 loss: 0.4437 time 2.94s\n",
            "Epoch 120/300 50/80 loss: 0.4497 time 3.04s\n",
            "Epoch 120/300 60/80 loss: 0.4513 time 2.83s\n",
            "Epoch 120/300 70/80 loss: 0.4421 time 2.86s\n",
            "Epoch 120/300 80/80 loss: 0.4483 time 1.57s\n",
            "Final training  120/299 loss: 0.4483 time 224.63s\n",
            "Thu Dec 18 11:52:56 2025 Epoch: 121\n",
            "Epoch 121/300 10/80 loss: 0.3951 time 2.64s\n",
            "Epoch 121/300 20/80 loss: 0.4345 time 3.09s\n",
            "Epoch 121/300 30/80 loss: 0.4303 time 2.85s\n",
            "Epoch 121/300 40/80 loss: 0.4189 time 2.76s\n",
            "Epoch 121/300 50/80 loss: 0.4150 time 2.92s\n",
            "Epoch 121/300 60/80 loss: 0.4176 time 2.98s\n",
            "Epoch 121/300 70/80 loss: 0.4213 time 2.94s\n",
            "Epoch 121/300 80/80 loss: 0.4137 time 1.67s\n",
            "Final training  121/299 loss: 0.4137 time 224.88s\n",
            "Thu Dec 18 11:56:41 2025 Epoch: 122\n",
            "Epoch 122/300 10/80 loss: 0.4039 time 2.83s\n",
            "Epoch 122/300 20/80 loss: 0.4071 time 2.76s\n",
            "Epoch 122/300 30/80 loss: 0.4232 time 2.74s\n",
            "Epoch 122/300 40/80 loss: 0.4274 time 3.07s\n",
            "Epoch 122/300 50/80 loss: 0.4224 time 2.82s\n",
            "Epoch 122/300 60/80 loss: 0.4082 time 2.79s\n",
            "Epoch 122/300 70/80 loss: 0.4209 time 2.89s\n",
            "Epoch 122/300 80/80 loss: 0.4119 time 1.41s\n",
            "Final training  122/299 loss: 0.4119 time 225.46s\n",
            "Thu Dec 18 12:00:26 2025 Epoch: 123\n",
            "Epoch 123/300 10/80 loss: 0.4127 time 2.84s\n",
            "Epoch 123/300 20/80 loss: 0.4138 time 2.81s\n",
            "Epoch 123/300 30/80 loss: 0.4260 time 2.70s\n",
            "Epoch 123/300 40/80 loss: 0.4150 time 3.13s\n",
            "Epoch 123/300 50/80 loss: 0.3988 time 2.97s\n",
            "Epoch 123/300 60/80 loss: 0.4068 time 3.04s\n",
            "Epoch 123/300 70/80 loss: 0.4041 time 2.66s\n",
            "Epoch 123/300 80/80 loss: 0.4036 time 1.39s\n",
            "Final training  123/299 loss: 0.4036 time 225.53s\n",
            "Thu Dec 18 12:04:12 2025 Epoch: 124\n",
            "Epoch 124/300 10/80 loss: 0.3615 time 2.78s\n",
            "Epoch 124/300 20/80 loss: 0.3864 time 3.13s\n",
            "Epoch 124/300 30/80 loss: 0.4010 time 3.06s\n",
            "Epoch 124/300 40/80 loss: 0.3841 time 2.87s\n",
            "Epoch 124/300 50/80 loss: 0.3926 time 2.91s\n",
            "Epoch 124/300 60/80 loss: 0.3981 time 2.64s\n",
            "Epoch 124/300 70/80 loss: 0.3944 time 2.56s\n",
            "Epoch 124/300 80/80 loss: 0.3971 time 1.44s\n",
            "Final training  124/299 loss: 0.3971 time 225.78s\n",
            "Thu Dec 18 12:07:58 2025 Epoch: 125\n",
            "Epoch 125/300 10/80 loss: 0.3579 time 2.83s\n",
            "Epoch 125/300 20/80 loss: 0.3878 time 3.00s\n",
            "Epoch 125/300 30/80 loss: 0.3832 time 3.02s\n",
            "Epoch 125/300 40/80 loss: 0.3913 time 2.85s\n",
            "Epoch 125/300 50/80 loss: 0.3751 time 2.89s\n",
            "Epoch 125/300 60/80 loss: 0.3675 time 2.84s\n",
            "Epoch 125/300 70/80 loss: 0.3772 time 2.85s\n",
            "Epoch 125/300 80/80 loss: 0.3842 time 1.49s\n",
            "Final training  125/299 loss: 0.3842 time 226.11s\n",
            "Thu Dec 18 12:11:44 2025 Epoch: 126\n",
            "Epoch 126/300 10/80 loss: 0.4504 time 3.07s\n",
            "Epoch 126/300 20/80 loss: 0.3958 time 2.78s\n",
            "Epoch 126/300 30/80 loss: 0.4126 time 2.92s\n",
            "Epoch 126/300 40/80 loss: 0.4140 time 2.81s\n",
            "Epoch 126/300 50/80 loss: 0.4113 time 2.79s\n",
            "Epoch 126/300 60/80 loss: 0.4188 time 2.69s\n",
            "Epoch 126/300 70/80 loss: 0.4194 time 2.86s\n",
            "Epoch 126/300 80/80 loss: 0.4110 time 1.60s\n",
            "Final training  126/299 loss: 0.4110 time 225.12s\n",
            "Thu Dec 18 12:15:29 2025 Epoch: 127\n",
            "Epoch 127/300 10/80 loss: 0.3751 time 3.03s\n",
            "Epoch 127/300 20/80 loss: 0.3815 time 2.88s\n",
            "Epoch 127/300 30/80 loss: 0.4033 time 3.03s\n",
            "Epoch 127/300 40/80 loss: 0.4115 time 2.84s\n",
            "Epoch 127/300 50/80 loss: 0.4018 time 3.06s\n",
            "Epoch 127/300 60/80 loss: 0.4053 time 2.84s\n",
            "Epoch 127/300 70/80 loss: 0.4075 time 2.77s\n",
            "Epoch 127/300 80/80 loss: 0.4029 time 1.54s\n",
            "Final training  127/299 loss: 0.4029 time 225.08s\n",
            "Thu Dec 18 12:19:14 2025 Epoch: 128\n",
            "Epoch 128/300 10/80 loss: 0.3747 time 3.01s\n",
            "Epoch 128/300 20/80 loss: 0.3846 time 3.13s\n",
            "Epoch 128/300 30/80 loss: 0.3842 time 3.06s\n",
            "Epoch 128/300 40/80 loss: 0.3954 time 3.02s\n",
            "Epoch 128/300 50/80 loss: 0.3995 time 3.09s\n",
            "Epoch 128/300 60/80 loss: 0.4154 time 2.79s\n",
            "Epoch 128/300 70/80 loss: 0.4232 time 2.80s\n",
            "Epoch 128/300 80/80 loss: 0.4167 time 1.42s\n",
            "Final training  128/299 loss: 0.4167 time 224.75s\n",
            "Thu Dec 18 12:22:59 2025 Epoch: 129\n",
            "Epoch 129/300 10/80 loss: 0.4120 time 2.98s\n",
            "Epoch 129/300 20/80 loss: 0.4160 time 2.94s\n",
            "Epoch 129/300 30/80 loss: 0.4013 time 3.07s\n",
            "Epoch 129/300 40/80 loss: 0.4189 time 2.97s\n",
            "Epoch 129/300 50/80 loss: 0.4108 time 2.77s\n",
            "Epoch 129/300 60/80 loss: 0.3953 time 2.98s\n",
            "Epoch 129/300 70/80 loss: 0.4014 time 2.83s\n",
            "Epoch 129/300 80/80 loss: 0.4058 time 1.38s\n",
            "Final training  129/299 loss: 0.4058 time 224.67s\n",
            "Val 129/300 0/40 , dice_tc: 0.80607444 , dice_wt: 0.94081753 , dice_et: 0.814028 , time 2.72s\n",
            "Val 129/300 1/40 , dice_tc: 0.84842557 , dice_wt: 0.95776385 , dice_et: 0.814028 , time 2.76s\n",
            "Val 129/300 2/40 , dice_tc: 0.8753713 , dice_wt: 0.9198896 , dice_et: 0.8726183 , time 2.72s\n",
            "Val 129/300 3/40 , dice_tc: 0.8680267 , dice_wt: 0.89739406 , dice_et: 0.8691997 , time 2.67s\n",
            "Val 129/300 4/40 , dice_tc: 0.73735017 , dice_wt: 0.801445 , dice_et: 0.6865726 , time 2.53s\n",
            "Val 129/300 5/40 , dice_tc: 0.7477918 , dice_wt: 0.8062143 , dice_et: 0.7052802 , time 2.50s\n",
            "Val 129/300 6/40 , dice_tc: 0.76908 , dice_wt: 0.8063411 , dice_et: 0.7380915 , time 2.68s\n",
            "Val 129/300 7/40 , dice_tc: 0.7802769 , dice_wt: 0.80654335 , dice_et: 0.7557333 , time 2.53s\n",
            "Val 129/300 8/40 , dice_tc: 0.7898561 , dice_wt: 0.8201081 , dice_et: 0.7706416 , time 2.75s\n",
            "Val 129/300 9/40 , dice_tc: 0.78960884 , dice_wt: 0.826711 , dice_et: 0.7875465 , time 2.55s\n",
            "Val 129/300 10/40 , dice_tc: 0.79397947 , dice_wt: 0.83308023 , dice_et: 0.79249537 , time 2.51s\n",
            "Val 129/300 11/40 , dice_tc: 0.7913398 , dice_wt: 0.825502 , dice_et: 0.79055464 , time 2.67s\n",
            "Val 129/300 12/40 , dice_tc: 0.73638785 , dice_wt: 0.8326776 , dice_et: 0.79055464 , time 2.49s\n",
            "Val 129/300 13/40 , dice_tc: 0.7324118 , dice_wt: 0.835063 , dice_et: 0.7814787 , time 2.52s\n",
            "Val 129/300 14/40 , dice_tc: 0.72734034 , dice_wt: 0.840361 , dice_et: 0.7241266 , time 2.63s\n",
            "Val 129/300 15/40 , dice_tc: 0.7315723 , dice_wt: 0.8446882 , dice_et: 0.73403937 , time 2.78s\n",
            "Val 129/300 16/40 , dice_tc: 0.716484 , dice_wt: 0.8443583 , dice_et: 0.7177169 , time 2.72s\n",
            "Val 129/300 17/40 , dice_tc: 0.7148644 , dice_wt: 0.8486975 , dice_et: 0.71724415 , time 2.53s\n",
            "Val 129/300 18/40 , dice_tc: 0.72459704 , dice_wt: 0.8412564 , dice_et: 0.72793144 , time 2.74s\n",
            "Val 129/300 19/40 , dice_tc: 0.72683287 , dice_wt: 0.84369767 , dice_et: 0.7316169 , time 2.72s\n",
            "Val 129/300 20/40 , dice_tc: 0.72491443 , dice_wt: 0.8471786 , dice_et: 0.7310912 , time 2.47s\n",
            "Val 129/300 21/40 , dice_tc: 0.7317343 , dice_wt: 0.8490559 , dice_et: 0.73831403 , time 2.49s\n",
            "Val 129/300 22/40 , dice_tc: 0.71929044 , dice_wt: 0.8534894 , dice_et: 0.7277217 , time 2.74s\n",
            "Val 129/300 23/40 , dice_tc: 0.7121491 , dice_wt: 0.8571186 , dice_et: 0.7190325 , time 2.75s\n",
            "Val 129/300 24/40 , dice_tc: 0.7078119 , dice_wt: 0.8609903 , dice_et: 0.7153004 , time 2.79s\n",
            "Val 129/300 25/40 , dice_tc: 0.69880354 , dice_wt: 0.8643354 , dice_et: 0.7153004 , time 2.70s\n",
            "Val 129/300 26/40 , dice_tc: 0.70591915 , dice_wt: 0.8633479 , dice_et: 0.72277015 , time 2.75s\n",
            "Val 129/300 27/40 , dice_tc: 0.7067283 , dice_wt: 0.8613366 , dice_et: 0.71448576 , time 2.73s\n",
            "Val 129/300 28/40 , dice_tc: 0.7086975 , dice_wt: 0.86086005 , dice_et: 0.71691376 , time 2.70s\n",
            "Val 129/300 29/40 , dice_tc: 0.6927341 , dice_wt: 0.8616693 , dice_et: 0.7013883 , time 2.51s\n",
            "Val 129/300 30/40 , dice_tc: 0.69373727 , dice_wt: 0.86251754 , dice_et: 0.68381125 , time 2.65s\n",
            "Val 129/300 31/40 , dice_tc: 0.7008505 , dice_wt: 0.8641995 , dice_et: 0.6920073 , time 2.62s\n",
            "Val 129/300 32/40 , dice_tc: 0.6796126 , dice_wt: 0.8388216 , dice_et: 0.6920073 , time 2.48s\n",
            "Val 129/300 33/40 , dice_tc: 0.68343127 , dice_wt: 0.83892053 , dice_et: 0.6976178 , time 2.75s\n",
            "Val 129/300 34/40 , dice_tc: 0.68474406 , dice_wt: 0.83289623 , dice_et: 0.699082 , time 2.47s\n",
            "Val 129/300 35/40 , dice_tc: 0.6887606 , dice_wt: 0.8337512 , dice_et: 0.703427 , time 2.70s\n",
            "Val 129/300 36/40 , dice_tc: 0.69613653 , dice_wt: 0.83621806 , dice_et: 0.711293 , time 2.52s\n",
            "Val 129/300 37/40 , dice_tc: 0.69251007 , dice_wt: 0.83879316 , dice_et: 0.7089956 , time 2.70s\n",
            "Val 129/300 38/40 , dice_tc: 0.6975893 , dice_wt: 0.8399541 , dice_et: 0.7140579 , time 2.67s\n",
            "Val 129/300 39/40 , dice_tc: 0.68573976 , dice_wt: 0.8404399 , dice_et: 0.7140579 , time 2.67s\n",
            "Final validation stats 129/299 , dice_tc: 0.68573976 , dice_wt: 0.8404399 , dice_et: 0.7140579 , Dice_Avg: 0.7467459 , LR: 0.000035 , time 105.60s\n",
            "new best (0.743262 --> 0.746746). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 12:28:31 2025 Epoch: 130\n",
            "Epoch 130/300 10/80 loss: 0.4065 time 2.86s\n",
            "Epoch 130/300 20/80 loss: 0.4431 time 2.86s\n",
            "Epoch 130/300 30/80 loss: 0.4348 time 2.86s\n",
            "Epoch 130/300 40/80 loss: 0.4237 time 3.10s\n",
            "Epoch 130/300 50/80 loss: 0.4259 time 3.04s\n",
            "Epoch 130/300 60/80 loss: 0.4305 time 2.84s\n",
            "Epoch 130/300 70/80 loss: 0.4309 time 3.10s\n",
            "Epoch 130/300 80/80 loss: 0.4302 time 1.45s\n",
            "Final training  130/299 loss: 0.4302 time 226.37s\n",
            "Thu Dec 18 12:32:18 2025 Epoch: 131\n",
            "Epoch 131/300 10/80 loss: 0.3053 time 2.83s\n",
            "Epoch 131/300 20/80 loss: 0.3496 time 2.76s\n",
            "Epoch 131/300 30/80 loss: 0.3503 time 2.81s\n",
            "Epoch 131/300 40/80 loss: 0.3622 time 2.89s\n",
            "Epoch 131/300 50/80 loss: 0.3744 time 2.99s\n",
            "Epoch 131/300 60/80 loss: 0.3759 time 2.97s\n",
            "Epoch 131/300 70/80 loss: 0.3805 time 3.11s\n",
            "Epoch 131/300 80/80 loss: 0.3877 time 1.66s\n",
            "Final training  131/299 loss: 0.3877 time 226.89s\n",
            "Thu Dec 18 12:36:05 2025 Epoch: 132\n",
            "Epoch 132/300 10/80 loss: 0.4069 time 3.07s\n",
            "Epoch 132/300 20/80 loss: 0.3778 time 2.81s\n",
            "Epoch 132/300 30/80 loss: 0.3856 time 2.85s\n",
            "Epoch 132/300 40/80 loss: 0.3950 time 2.84s\n",
            "Epoch 132/300 50/80 loss: 0.3965 time 3.07s\n",
            "Epoch 132/300 60/80 loss: 0.3927 time 2.99s\n",
            "Epoch 132/300 70/80 loss: 0.4001 time 3.07s\n",
            "Epoch 132/300 80/80 loss: 0.3878 time 1.46s\n",
            "Final training  132/299 loss: 0.3878 time 225.85s\n",
            "Thu Dec 18 12:39:51 2025 Epoch: 133\n",
            "Epoch 133/300 10/80 loss: 0.3368 time 3.05s\n",
            "Epoch 133/300 20/80 loss: 0.3405 time 3.08s\n",
            "Epoch 133/300 30/80 loss: 0.3435 time 2.96s\n",
            "Epoch 133/300 40/80 loss: 0.3532 time 2.88s\n",
            "Epoch 133/300 50/80 loss: 0.3556 time 2.98s\n",
            "Epoch 133/300 60/80 loss: 0.3630 time 2.83s\n",
            "Epoch 133/300 70/80 loss: 0.3576 time 2.98s\n",
            "Epoch 133/300 80/80 loss: 0.3666 time 1.41s\n",
            "Final training  133/299 loss: 0.3666 time 225.48s\n",
            "Thu Dec 18 12:43:36 2025 Epoch: 134\n",
            "Epoch 134/300 10/80 loss: 0.3299 time 2.66s\n",
            "Epoch 134/300 20/80 loss: 0.3514 time 2.96s\n",
            "Epoch 134/300 30/80 loss: 0.3852 time 2.85s\n",
            "Epoch 134/300 40/80 loss: 0.3856 time 2.91s\n",
            "Epoch 134/300 50/80 loss: 0.3829 time 2.87s\n",
            "Epoch 134/300 60/80 loss: 0.3883 time 3.13s\n",
            "Epoch 134/300 70/80 loss: 0.3922 time 3.03s\n",
            "Epoch 134/300 80/80 loss: 0.3828 time 1.64s\n",
            "Final training  134/299 loss: 0.3828 time 225.85s\n",
            "Thu Dec 18 12:47:22 2025 Epoch: 135\n",
            "Epoch 135/300 10/80 loss: 0.3818 time 2.81s\n",
            "Epoch 135/300 20/80 loss: 0.3847 time 3.02s\n",
            "Epoch 135/300 30/80 loss: 0.3739 time 2.99s\n",
            "Epoch 135/300 40/80 loss: 0.3772 time 2.82s\n",
            "Epoch 135/300 50/80 loss: 0.3710 time 2.86s\n",
            "Epoch 135/300 60/80 loss: 0.3682 time 3.06s\n",
            "Epoch 135/300 70/80 loss: 0.3699 time 2.81s\n",
            "Epoch 135/300 80/80 loss: 0.3654 time 1.71s\n",
            "Final training  135/299 loss: 0.3654 time 225.71s\n",
            "Thu Dec 18 12:51:08 2025 Epoch: 136\n",
            "Epoch 136/300 10/80 loss: 0.3363 time 2.61s\n",
            "Epoch 136/300 20/80 loss: 0.3089 time 3.07s\n",
            "Epoch 136/300 30/80 loss: 0.3107 time 3.14s\n",
            "Epoch 136/300 40/80 loss: 0.3300 time 3.03s\n",
            "Epoch 136/300 50/80 loss: 0.3365 time 3.01s\n",
            "Epoch 136/300 60/80 loss: 0.3458 time 3.12s\n",
            "Epoch 136/300 70/80 loss: 0.3491 time 2.65s\n",
            "Epoch 136/300 80/80 loss: 0.3589 time 1.61s\n",
            "Final training  136/299 loss: 0.3589 time 226.25s\n",
            "Thu Dec 18 12:54:54 2025 Epoch: 137\n",
            "Epoch 137/300 10/80 loss: 0.3600 time 3.00s\n",
            "Epoch 137/300 20/80 loss: 0.3338 time 3.01s\n",
            "Epoch 137/300 30/80 loss: 0.3683 time 2.64s\n",
            "Epoch 137/300 40/80 loss: 0.3473 time 2.83s\n",
            "Epoch 137/300 50/80 loss: 0.3433 time 2.89s\n",
            "Epoch 137/300 60/80 loss: 0.3426 time 2.98s\n",
            "Epoch 137/300 70/80 loss: 0.3572 time 2.84s\n",
            "Epoch 137/300 80/80 loss: 0.3567 time 1.59s\n",
            "Final training  137/299 loss: 0.3567 time 226.69s\n",
            "Thu Dec 18 12:58:41 2025 Epoch: 138\n",
            "Epoch 138/300 10/80 loss: 0.4002 time 3.03s\n",
            "Epoch 138/300 20/80 loss: 0.3916 time 2.80s\n",
            "Epoch 138/300 30/80 loss: 0.3468 time 2.88s\n",
            "Epoch 138/300 40/80 loss: 0.3473 time 2.72s\n",
            "Epoch 138/300 50/80 loss: 0.3592 time 2.88s\n",
            "Epoch 138/300 60/80 loss: 0.3582 time 2.84s\n",
            "Epoch 138/300 70/80 loss: 0.3533 time 3.12s\n",
            "Epoch 138/300 80/80 loss: 0.3662 time 1.39s\n",
            "Final training  138/299 loss: 0.3662 time 226.25s\n",
            "Thu Dec 18 13:02:27 2025 Epoch: 139\n",
            "Epoch 139/300 10/80 loss: 0.3552 time 3.00s\n",
            "Epoch 139/300 20/80 loss: 0.3571 time 3.13s\n",
            "Epoch 139/300 30/80 loss: 0.3558 time 2.65s\n",
            "Epoch 139/300 40/80 loss: 0.3656 time 3.07s\n",
            "Epoch 139/300 50/80 loss: 0.3649 time 2.97s\n",
            "Epoch 139/300 60/80 loss: 0.3619 time 3.04s\n",
            "Epoch 139/300 70/80 loss: 0.3575 time 2.87s\n",
            "Epoch 139/300 80/80 loss: 0.3557 time 1.45s\n",
            "Final training  139/299 loss: 0.3557 time 226.59s\n",
            "Val 139/300 0/40 , dice_tc: 0.85164595 , dice_wt: 0.94061536 , dice_et: 0.86701626 , time 2.73s\n",
            "Val 139/300 1/40 , dice_tc: 0.7136828 , dice_wt: 0.95085317 , dice_et: 0.86701626 , time 2.73s\n",
            "Val 139/300 2/40 , dice_tc: 0.79554796 , dice_wt: 0.9366314 , dice_et: 0.9133911 , time 2.68s\n",
            "Val 139/300 3/40 , dice_tc: 0.8130213 , dice_wt: 0.9254644 , dice_et: 0.9051822 , time 2.76s\n",
            "Val 139/300 4/40 , dice_tc: 0.74891067 , dice_wt: 0.84046733 , dice_et: 0.7340768 , time 2.54s\n",
            "Val 139/300 5/40 , dice_tc: 0.7794903 , dice_wt: 0.84722066 , dice_et: 0.7727009 , time 2.53s\n",
            "Val 139/300 6/40 , dice_tc: 0.78524417 , dice_wt: 0.844924 , dice_et: 0.78280324 , time 2.71s\n",
            "Val 139/300 7/40 , dice_tc: 0.786948 , dice_wt: 0.8437083 , dice_et: 0.78970754 , time 2.57s\n",
            "Val 139/300 8/40 , dice_tc: 0.78348565 , dice_wt: 0.8529023 , dice_et: 0.79698765 , time 2.80s\n",
            "Val 139/300 9/40 , dice_tc: 0.7844785 , dice_wt: 0.85967857 , dice_et: 0.81213623 , time 2.58s\n",
            "Val 139/300 10/40 , dice_tc: 0.7907226 , dice_wt: 0.8637678 , dice_et: 0.8178344 , time 2.47s\n",
            "Val 139/300 11/40 , dice_tc: 0.7870627 , dice_wt: 0.86140156 , dice_et: 0.8153526 , time 2.67s\n",
            "Val 139/300 12/40 , dice_tc: 0.7408777 , dice_wt: 0.867632 , dice_et: 0.8153526 , time 2.50s\n",
            "Val 139/300 13/40 , dice_tc: 0.7300394 , dice_wt: 0.8654228 , dice_et: 0.80082536 , time 2.51s\n",
            "Val 139/300 14/40 , dice_tc: 0.7220439 , dice_wt: 0.8686377 , dice_et: 0.73934853 , time 2.62s\n",
            "Val 139/300 15/40 , dice_tc: 0.72855335 , dice_wt: 0.8698298 , dice_et: 0.7508532 , time 2.71s\n",
            "Val 139/300 16/40 , dice_tc: 0.7114689 , dice_wt: 0.8677941 , dice_et: 0.7306859 , time 2.67s\n",
            "Val 139/300 17/40 , dice_tc: 0.70866966 , dice_wt: 0.87064284 , dice_et: 0.7312096 , time 2.58s\n",
            "Val 139/300 18/40 , dice_tc: 0.7204835 , dice_wt: 0.8698261 , dice_et: 0.7429103 , time 2.70s\n",
            "Val 139/300 19/40 , dice_tc: 0.72337395 , dice_wt: 0.87121516 , dice_et: 0.7469722 , time 2.66s\n",
            "Val 139/300 20/40 , dice_tc: 0.7198693 , dice_wt: 0.87356395 , dice_et: 0.7462617 , time 2.44s\n",
            "Val 139/300 21/40 , dice_tc: 0.7293098 , dice_wt: 0.8755299 , dice_et: 0.755089 , time 2.52s\n",
            "Val 139/300 22/40 , dice_tc: 0.71812755 , dice_wt: 0.8790069 , dice_et: 0.7469372 , time 2.78s\n",
            "Val 139/300 23/40 , dice_tc: 0.7144572 , dice_wt: 0.8811722 , dice_et: 0.7433514 , time 2.73s\n",
            "Val 139/300 24/40 , dice_tc: 0.7099238 , dice_wt: 0.88420594 , dice_et: 0.7421102 , time 2.74s\n",
            "Val 139/300 25/40 , dice_tc: 0.7000334 , dice_wt: 0.8861724 , dice_et: 0.7421102 , time 2.73s\n",
            "Val 139/300 26/40 , dice_tc: 0.7071982 , dice_wt: 0.8864221 , dice_et: 0.7487607 , time 2.80s\n",
            "Val 139/300 27/40 , dice_tc: 0.7093113 , dice_wt: 0.88647574 , dice_et: 0.740029 , time 2.74s\n",
            "Val 139/300 28/40 , dice_tc: 0.7099884 , dice_wt: 0.88725793 , dice_et: 0.741719 , time 2.63s\n",
            "Val 139/300 29/40 , dice_tc: 0.69124997 , dice_wt: 0.88631135 , dice_et: 0.7221782 , time 2.53s\n",
            "Val 139/300 30/40 , dice_tc: 0.69237113 , dice_wt: 0.88690126 , dice_et: 0.70554584 , time 2.67s\n",
            "Val 139/300 31/40 , dice_tc: 0.7001626 , dice_wt: 0.8880495 , dice_et: 0.7139062 , time 2.73s\n",
            "Val 139/300 32/40 , dice_tc: 0.69746906 , dice_wt: 0.88653785 , dice_et: 0.7139062 , time 2.53s\n",
            "Val 139/300 33/40 , dice_tc: 0.7011687 , dice_wt: 0.8865213 , dice_et: 0.71931607 , time 2.70s\n",
            "Val 139/300 34/40 , dice_tc: 0.70652837 , dice_wt: 0.8852254 , dice_et: 0.7246543 , time 2.50s\n",
            "Val 139/300 35/40 , dice_tc: 0.70443344 , dice_wt: 0.8837368 , dice_et: 0.7224191 , time 2.79s\n",
            "Val 139/300 36/40 , dice_tc: 0.71151847 , dice_wt: 0.88477623 , dice_et: 0.7298282 , time 2.55s\n",
            "Val 139/300 37/40 , dice_tc: 0.70265555 , dice_wt: 0.88628757 , dice_et: 0.7292844 , time 2.72s\n",
            "Val 139/300 38/40 , dice_tc: 0.7074666 , dice_wt: 0.8860103 , dice_et: 0.7337731 , time 2.69s\n",
            "Val 139/300 39/40 , dice_tc: 0.69674814 , dice_wt: 0.88622534 , dice_et: 0.7337731 , time 2.68s\n",
            "Final validation stats 139/299 , dice_tc: 0.69674814 , dice_wt: 0.88622534 , dice_et: 0.7337731 , Dice_Avg: 0.7722489 , LR: 0.000038 , time 105.93s\n",
            "new best (0.746746 --> 0.772249). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 13:08:02 2025 Epoch: 140\n",
            "Epoch 140/300 10/80 loss: 0.3322 time 3.10s\n",
            "Epoch 140/300 20/80 loss: 0.3441 time 3.09s\n",
            "Epoch 140/300 30/80 loss: 0.3462 time 2.78s\n",
            "Epoch 140/300 40/80 loss: 0.3456 time 2.67s\n",
            "Epoch 140/300 50/80 loss: 0.3533 time 3.03s\n",
            "Epoch 140/300 60/80 loss: 0.3515 time 2.66s\n",
            "Epoch 140/300 70/80 loss: 0.3576 time 2.80s\n",
            "Epoch 140/300 80/80 loss: 0.3525 time 1.70s\n",
            "Final training  140/299 loss: 0.3525 time 227.30s\n",
            "Thu Dec 18 13:11:49 2025 Epoch: 141\n",
            "Epoch 141/300 10/80 loss: 0.3486 time 2.87s\n",
            "Epoch 141/300 20/80 loss: 0.3609 time 2.79s\n",
            "Epoch 141/300 30/80 loss: 0.3791 time 3.04s\n",
            "Epoch 141/300 40/80 loss: 0.3742 time 2.86s\n",
            "Epoch 141/300 50/80 loss: 0.3661 time 2.94s\n",
            "Epoch 141/300 60/80 loss: 0.3686 time 3.09s\n",
            "Epoch 141/300 70/80 loss: 0.3550 time 2.82s\n",
            "Epoch 141/300 80/80 loss: 0.3574 time 1.60s\n",
            "Final training  141/299 loss: 0.3574 time 226.21s\n",
            "Thu Dec 18 13:15:35 2025 Epoch: 142\n",
            "Epoch 142/300 10/80 loss: 0.3794 time 3.13s\n",
            "Epoch 142/300 20/80 loss: 0.3887 time 2.66s\n",
            "Epoch 142/300 30/80 loss: 0.3837 time 3.06s\n",
            "Epoch 142/300 40/80 loss: 0.3918 time 2.69s\n",
            "Epoch 142/300 50/80 loss: 0.3859 time 2.67s\n",
            "Epoch 142/300 60/80 loss: 0.3776 time 3.05s\n",
            "Epoch 142/300 70/80 loss: 0.3623 time 3.01s\n",
            "Epoch 142/300 80/80 loss: 0.3611 time 1.62s\n",
            "Final training  142/299 loss: 0.3611 time 226.14s\n",
            "Thu Dec 18 13:19:21 2025 Epoch: 143\n",
            "Epoch 143/300 10/80 loss: 0.3734 time 2.86s\n",
            "Epoch 143/300 20/80 loss: 0.3420 time 3.10s\n",
            "Epoch 143/300 30/80 loss: 0.3560 time 2.81s\n",
            "Epoch 143/300 40/80 loss: 0.3726 time 3.00s\n",
            "Epoch 143/300 50/80 loss: 0.3645 time 3.03s\n",
            "Epoch 143/300 60/80 loss: 0.3638 time 2.63s\n",
            "Epoch 143/300 70/80 loss: 0.3658 time 2.88s\n",
            "Epoch 143/300 80/80 loss: 0.3535 time 1.42s\n",
            "Final training  143/299 loss: 0.3535 time 226.68s\n",
            "Thu Dec 18 13:23:08 2025 Epoch: 144\n",
            "Epoch 144/300 10/80 loss: 0.3639 time 2.79s\n",
            "Epoch 144/300 20/80 loss: 0.3676 time 3.04s\n",
            "Epoch 144/300 30/80 loss: 0.3743 time 2.92s\n",
            "Epoch 144/300 40/80 loss: 0.3893 time 2.62s\n",
            "Epoch 144/300 50/80 loss: 0.3800 time 2.89s\n",
            "Epoch 144/300 60/80 loss: 0.3676 time 3.03s\n",
            "Epoch 144/300 70/80 loss: 0.3606 time 3.00s\n",
            "Epoch 144/300 80/80 loss: 0.3577 time 1.64s\n",
            "Final training  144/299 loss: 0.3577 time 226.49s\n",
            "Thu Dec 18 13:26:55 2025 Epoch: 145\n",
            "Epoch 145/300 10/80 loss: 0.4052 time 3.11s\n",
            "Epoch 145/300 20/80 loss: 0.3853 time 3.03s\n",
            "Epoch 145/300 30/80 loss: 0.3739 time 3.06s\n",
            "Epoch 145/300 40/80 loss: 0.3607 time 3.06s\n",
            "Epoch 145/300 50/80 loss: 0.3557 time 2.80s\n",
            "Epoch 145/300 60/80 loss: 0.3562 time 2.90s\n",
            "Epoch 145/300 70/80 loss: 0.3563 time 2.95s\n",
            "Epoch 145/300 80/80 loss: 0.3561 time 1.40s\n",
            "Final training  145/299 loss: 0.3561 time 226.20s\n",
            "Thu Dec 18 13:30:41 2025 Epoch: 146\n",
            "Epoch 146/300 10/80 loss: 0.3274 time 2.88s\n",
            "Epoch 146/300 20/80 loss: 0.3512 time 3.04s\n",
            "Epoch 146/300 30/80 loss: 0.3523 time 2.85s\n",
            "Epoch 146/300 40/80 loss: 0.3786 time 2.73s\n",
            "Epoch 146/300 50/80 loss: 0.3784 time 3.01s\n",
            "Epoch 146/300 60/80 loss: 0.3627 time 2.89s\n",
            "Epoch 146/300 70/80 loss: 0.3628 time 2.86s\n",
            "Epoch 146/300 80/80 loss: 0.3616 time 1.40s\n",
            "Final training  146/299 loss: 0.3616 time 227.15s\n",
            "Thu Dec 18 13:34:28 2025 Epoch: 147\n",
            "Epoch 147/300 10/80 loss: 0.3302 time 2.91s\n",
            "Epoch 147/300 20/80 loss: 0.2991 time 2.79s\n",
            "Epoch 147/300 30/80 loss: 0.3197 time 2.80s\n",
            "Epoch 147/300 40/80 loss: 0.3309 time 2.91s\n",
            "Epoch 147/300 50/80 loss: 0.3449 time 3.01s\n",
            "Epoch 147/300 60/80 loss: 0.3515 time 2.64s\n",
            "Epoch 147/300 70/80 loss: 0.3396 time 3.06s\n",
            "Epoch 147/300 80/80 loss: 0.3316 time 1.65s\n",
            "Final training  147/299 loss: 0.3316 time 226.32s\n",
            "Thu Dec 18 13:38:14 2025 Epoch: 148\n",
            "Epoch 148/300 10/80 loss: 0.3679 time 2.82s\n",
            "Epoch 148/300 20/80 loss: 0.3787 time 2.86s\n",
            "Epoch 148/300 30/80 loss: 0.3547 time 2.87s\n",
            "Epoch 148/300 40/80 loss: 0.3731 time 3.06s\n",
            "Epoch 148/300 50/80 loss: 0.3751 time 2.98s\n",
            "Epoch 148/300 60/80 loss: 0.3769 time 2.60s\n",
            "Epoch 148/300 70/80 loss: 0.3684 time 2.83s\n",
            "Epoch 148/300 80/80 loss: 0.3572 time 1.65s\n",
            "Final training  148/299 loss: 0.3572 time 225.43s\n",
            "Thu Dec 18 13:42:00 2025 Epoch: 149\n",
            "Epoch 149/300 10/80 loss: 0.4035 time 3.03s\n",
            "Epoch 149/300 20/80 loss: 0.3599 time 3.10s\n",
            "Epoch 149/300 30/80 loss: 0.3518 time 3.19s\n",
            "Epoch 149/300 40/80 loss: 0.3564 time 2.86s\n",
            "Epoch 149/300 50/80 loss: 0.3555 time 2.79s\n",
            "Epoch 149/300 60/80 loss: 0.3475 time 3.03s\n",
            "Epoch 149/300 70/80 loss: 0.3563 time 2.78s\n",
            "Epoch 149/300 80/80 loss: 0.3535 time 1.61s\n",
            "Final training  149/299 loss: 0.3535 time 225.97s\n",
            "Val 149/300 0/40 , dice_tc: 0.81103987 , dice_wt: 0.93656397 , dice_et: 0.8122701 , time 2.78s\n",
            "Val 149/300 1/40 , dice_tc: 0.8873742 , dice_wt: 0.95512336 , dice_et: 0.8122701 , time 2.69s\n",
            "Val 149/300 2/40 , dice_tc: 0.90607244 , dice_wt: 0.93258643 , dice_et: 0.8797388 , time 2.72s\n",
            "Val 149/300 3/40 , dice_tc: 0.86389244 , dice_wt: 0.891968 , dice_et: 0.83912563 , time 2.69s\n",
            "Val 149/300 4/40 , dice_tc: 0.7549473 , dice_wt: 0.812891 , dice_et: 0.74397254 , time 2.52s\n",
            "Val 149/300 5/40 , dice_tc: 0.76661015 , dice_wt: 0.82007605 , dice_et: 0.7622744 , time 2.61s\n",
            "Val 149/300 6/40 , dice_tc: 0.7268977 , dice_wt: 0.7885648 , dice_et: 0.7204976 , time 2.69s\n",
            "Val 149/300 7/40 , dice_tc: 0.74452317 , dice_wt: 0.79362214 , dice_et: 0.74251074 , time 2.55s\n",
            "Val 149/300 8/40 , dice_tc: 0.75192004 , dice_wt: 0.8084073 , dice_et: 0.7524449 , time 2.73s\n",
            "Val 149/300 9/40 , dice_tc: 0.7478527 , dice_wt: 0.81788826 , dice_et: 0.77159745 , time 2.55s\n",
            "Val 149/300 10/40 , dice_tc: 0.7585933 , dice_wt: 0.8261885 , dice_et: 0.78005975 , time 2.47s\n",
            "Val 149/300 11/40 , dice_tc: 0.7668925 , dice_wt: 0.8221808 , dice_et: 0.7878939 , time 2.65s\n",
            "Val 149/300 12/40 , dice_tc: 0.75189275 , dice_wt: 0.83036387 , dice_et: 0.7878939 , time 2.46s\n",
            "Val 149/300 13/40 , dice_tc: 0.70451087 , dice_wt: 0.8294612 , dice_et: 0.72502375 , time 2.46s\n",
            "Val 149/300 14/40 , dice_tc: 0.6818621 , dice_wt: 0.8344851 , dice_et: 0.6692527 , time 2.69s\n",
            "Val 149/300 15/40 , dice_tc: 0.6904282 , dice_wt: 0.8390666 , dice_et: 0.68420255 , time 2.68s\n",
            "Val 149/300 16/40 , dice_tc: 0.6658932 , dice_wt: 0.83825296 , dice_et: 0.6564513 , time 2.70s\n",
            "Val 149/300 17/40 , dice_tc: 0.6749705 , dice_wt: 0.84297496 , dice_et: 0.66858053 , time 2.54s\n",
            "Val 149/300 18/40 , dice_tc: 0.6880866 , dice_wt: 0.8391601 , dice_et: 0.68356204 , time 2.75s\n",
            "Val 149/300 19/40 , dice_tc: 0.6964337 , dice_wt: 0.84230864 , dice_et: 0.6934631 , time 2.76s\n",
            "Val 149/300 20/40 , dice_tc: 0.7008787 , dice_wt: 0.84436655 , dice_et: 0.6988136 , time 2.48s\n",
            "Val 149/300 21/40 , dice_tc: 0.71002775 , dice_wt: 0.84816355 , dice_et: 0.7089808 , time 2.49s\n",
            "Val 149/300 22/40 , dice_tc: 0.7101599 , dice_wt: 0.8528218 , dice_et: 0.71108407 , time 2.67s\n",
            "Val 149/300 23/40 , dice_tc: 0.71023035 , dice_wt: 0.85654086 , dice_et: 0.71127534 , time 2.81s\n",
            "Val 149/300 24/40 , dice_tc: 0.71019226 , dice_wt: 0.8604679 , dice_et: 0.7121684 , time 2.77s\n",
            "Val 149/300 25/40 , dice_tc: 0.7104413 , dice_wt: 0.8636736 , dice_et: 0.7121684 , time 2.73s\n",
            "Val 149/300 26/40 , dice_tc: 0.71728605 , dice_wt: 0.8627324 , dice_et: 0.720069 , time 2.72s\n",
            "Val 149/300 27/40 , dice_tc: 0.7179152 , dice_wt: 0.86356896 , dice_et: 0.7170095 , time 2.74s\n",
            "Val 149/300 28/40 , dice_tc: 0.7221133 , dice_wt: 0.8634103 , dice_et: 0.72229636 , time 2.66s\n",
            "Val 149/300 29/40 , dice_tc: 0.7093837 , dice_wt: 0.8633428 , dice_et: 0.7116626 , time 2.53s\n",
            "Val 149/300 30/40 , dice_tc: 0.71305066 , dice_wt: 0.8649163 , dice_et: 0.6960336 , time 2.67s\n",
            "Val 149/300 31/40 , dice_tc: 0.72056067 , dice_wt: 0.8670079 , dice_et: 0.70499396 , time 2.66s\n",
            "Val 149/300 32/40 , dice_tc: 0.7208289 , dice_wt: 0.8660414 , dice_et: 0.70499396 , time 2.61s\n",
            "Val 149/300 33/40 , dice_tc: 0.7245999 , dice_wt: 0.86596715 , dice_et: 0.7110905 , time 2.70s\n",
            "Val 149/300 34/40 , dice_tc: 0.7255516 , dice_wt: 0.8639313 , dice_et: 0.71383214 , time 2.46s\n",
            "Val 149/300 35/40 , dice_tc: 0.7094836 , dice_wt: 0.86068594 , dice_et: 0.696315 , time 2.70s\n",
            "Val 149/300 36/40 , dice_tc: 0.7151285 , dice_wt: 0.861632 , dice_et: 0.7033079 , time 2.53s\n",
            "Val 149/300 37/40 , dice_tc: 0.7139624 , dice_wt: 0.86353964 , dice_et: 0.70398456 , time 2.78s\n",
            "Val 149/300 38/40 , dice_tc: 0.71541786 , dice_wt: 0.86391133 , dice_et: 0.7058471 , time 2.69s\n",
            "Val 149/300 39/40 , dice_tc: 0.7034983 , dice_wt: 0.86444694 , dice_et: 0.7058471 , time 2.66s\n",
            "Final validation stats 149/299 , dice_tc: 0.7034983 , dice_wt: 0.86444694 , dice_et: 0.7058471 , Dice_Avg: 0.75793076 , LR: 0.000040 , time 105.77s\n",
            "Thu Dec 18 13:47:31 2025 Epoch: 150\n",
            "Epoch 150/300 10/80 loss: 0.4902 time 3.00s\n",
            "Epoch 150/300 20/80 loss: 0.4584 time 3.05s\n",
            "Epoch 150/300 30/80 loss: 0.4184 time 2.64s\n",
            "Epoch 150/300 40/80 loss: 0.4009 time 2.64s\n",
            "Epoch 150/300 50/80 loss: 0.3956 time 2.83s\n",
            "Epoch 150/300 60/80 loss: 0.3868 time 2.85s\n",
            "Epoch 150/300 70/80 loss: 0.3705 time 2.90s\n",
            "Epoch 150/300 80/80 loss: 0.3760 time 1.59s\n",
            "Final training  150/299 loss: 0.3760 time 225.70s\n",
            "Thu Dec 18 13:51:17 2025 Epoch: 151\n",
            "Epoch 151/300 10/80 loss: 0.2356 time 2.86s\n",
            "Epoch 151/300 20/80 loss: 0.2826 time 2.94s\n",
            "Epoch 151/300 30/80 loss: 0.2987 time 3.02s\n",
            "Epoch 151/300 40/80 loss: 0.3105 time 3.04s\n",
            "Epoch 151/300 50/80 loss: 0.3137 time 3.15s\n",
            "Epoch 151/300 60/80 loss: 0.3082 time 2.85s\n",
            "Epoch 151/300 70/80 loss: 0.3174 time 2.90s\n",
            "Epoch 151/300 80/80 loss: 0.3196 time 1.47s\n",
            "Final training  151/299 loss: 0.3196 time 227.28s\n",
            "Thu Dec 18 13:55:05 2025 Epoch: 152\n",
            "Epoch 152/300 10/80 loss: 0.3354 time 2.81s\n",
            "Epoch 152/300 20/80 loss: 0.3266 time 2.71s\n",
            "Epoch 152/300 30/80 loss: 0.3331 time 2.96s\n",
            "Epoch 152/300 40/80 loss: 0.3510 time 2.84s\n",
            "Epoch 152/300 50/80 loss: 0.3534 time 2.56s\n",
            "Epoch 152/300 60/80 loss: 0.3527 time 2.66s\n",
            "Epoch 152/300 70/80 loss: 0.3580 time 2.67s\n",
            "Epoch 152/300 80/80 loss: 0.3580 time 1.43s\n",
            "Final training  152/299 loss: 0.3580 time 226.22s\n",
            "Thu Dec 18 13:58:51 2025 Epoch: 153\n",
            "Epoch 153/300 10/80 loss: 0.4041 time 2.89s\n",
            "Epoch 153/300 20/80 loss: 0.3712 time 3.11s\n",
            "Epoch 153/300 30/80 loss: 0.3463 time 2.88s\n",
            "Epoch 153/300 40/80 loss: 0.3553 time 2.96s\n",
            "Epoch 153/300 50/80 loss: 0.3651 time 2.88s\n",
            "Epoch 153/300 60/80 loss: 0.3754 time 2.77s\n",
            "Epoch 153/300 70/80 loss: 0.3581 time 2.87s\n",
            "Epoch 153/300 80/80 loss: 0.3470 time 1.62s\n",
            "Final training  153/299 loss: 0.3470 time 227.73s\n",
            "Thu Dec 18 14:02:38 2025 Epoch: 154\n",
            "Epoch 154/300 10/80 loss: 0.4129 time 3.11s\n",
            "Epoch 154/300 20/80 loss: 0.4280 time 2.87s\n",
            "Epoch 154/300 30/80 loss: 0.4066 time 3.04s\n",
            "Epoch 154/300 40/80 loss: 0.3880 time 2.93s\n",
            "Epoch 154/300 50/80 loss: 0.3810 time 2.82s\n",
            "Epoch 154/300 60/80 loss: 0.3783 time 2.79s\n",
            "Epoch 154/300 70/80 loss: 0.3744 time 3.01s\n",
            "Epoch 154/300 80/80 loss: 0.3733 time 1.43s\n",
            "Final training  154/299 loss: 0.3733 time 227.28s\n",
            "Thu Dec 18 14:06:26 2025 Epoch: 155\n",
            "Epoch 155/300 10/80 loss: 0.3254 time 3.08s\n",
            "Epoch 155/300 20/80 loss: 0.3244 time 2.88s\n",
            "Epoch 155/300 30/80 loss: 0.3265 time 3.00s\n",
            "Epoch 155/300 40/80 loss: 0.3541 time 2.98s\n",
            "Epoch 155/300 50/80 loss: 0.3424 time 2.89s\n",
            "Epoch 155/300 60/80 loss: 0.3326 time 2.65s\n",
            "Epoch 155/300 70/80 loss: 0.3310 time 2.92s\n",
            "Epoch 155/300 80/80 loss: 0.3456 time 1.66s\n",
            "Final training  155/299 loss: 0.3456 time 226.73s\n",
            "Thu Dec 18 14:10:12 2025 Epoch: 156\n",
            "Epoch 156/300 10/80 loss: 0.4035 time 2.81s\n",
            "Epoch 156/300 20/80 loss: 0.3703 time 2.68s\n",
            "Epoch 156/300 30/80 loss: 0.3709 time 2.86s\n",
            "Epoch 156/300 40/80 loss: 0.3644 time 2.88s\n",
            "Epoch 156/300 50/80 loss: 0.3381 time 2.92s\n",
            "Epoch 156/300 60/80 loss: 0.3251 time 2.86s\n",
            "Epoch 156/300 70/80 loss: 0.3274 time 2.81s\n",
            "Epoch 156/300 80/80 loss: 0.3301 time 1.71s\n",
            "Final training  156/299 loss: 0.3301 time 226.52s\n",
            "Thu Dec 18 14:13:59 2025 Epoch: 157\n",
            "Epoch 157/300 10/80 loss: 0.2923 time 2.99s\n",
            "Epoch 157/300 20/80 loss: 0.2845 time 2.85s\n",
            "Epoch 157/300 30/80 loss: 0.2886 time 2.91s\n",
            "Epoch 157/300 40/80 loss: 0.2917 time 2.82s\n",
            "Epoch 157/300 50/80 loss: 0.2891 time 3.01s\n",
            "Epoch 157/300 60/80 loss: 0.2749 time 2.93s\n",
            "Epoch 157/300 70/80 loss: 0.2895 time 2.84s\n",
            "Epoch 157/300 80/80 loss: 0.2941 time 1.57s\n",
            "Final training  157/299 loss: 0.2941 time 225.47s\n",
            "Thu Dec 18 14:17:44 2025 Epoch: 158\n",
            "Epoch 158/300 10/80 loss: 0.3024 time 3.12s\n",
            "Epoch 158/300 20/80 loss: 0.2949 time 3.01s\n",
            "Epoch 158/300 30/80 loss: 0.3179 time 2.62s\n",
            "Epoch 158/300 40/80 loss: 0.3082 time 2.81s\n",
            "Epoch 158/300 50/80 loss: 0.3258 time 2.62s\n",
            "Epoch 158/300 60/80 loss: 0.3273 time 3.03s\n",
            "Epoch 158/300 70/80 loss: 0.3229 time 2.81s\n",
            "Epoch 158/300 80/80 loss: 0.3191 time 1.62s\n",
            "Final training  158/299 loss: 0.3191 time 226.59s\n",
            "Thu Dec 18 14:21:31 2025 Epoch: 159\n",
            "Epoch 159/300 10/80 loss: 0.3541 time 3.06s\n",
            "Epoch 159/300 20/80 loss: 0.3308 time 3.07s\n",
            "Epoch 159/300 30/80 loss: 0.3357 time 3.10s\n",
            "Epoch 159/300 40/80 loss: 0.3137 time 2.75s\n",
            "Epoch 159/300 50/80 loss: 0.3156 time 2.61s\n",
            "Epoch 159/300 60/80 loss: 0.3149 time 2.73s\n",
            "Epoch 159/300 70/80 loss: 0.3193 time 2.91s\n",
            "Epoch 159/300 80/80 loss: 0.3141 time 1.41s\n",
            "Final training  159/299 loss: 0.3141 time 226.94s\n",
            "Val 159/300 0/40 , dice_tc: 0.8632506 , dice_wt: 0.9457461 , dice_et: 0.8640061 , time 2.72s\n",
            "Val 159/300 1/40 , dice_tc: 0.90001535 , dice_wt: 0.95981026 , dice_et: 0.8640061 , time 2.68s\n",
            "Val 159/300 2/40 , dice_tc: 0.9100644 , dice_wt: 0.931303 , dice_et: 0.900091 , time 2.69s\n",
            "Val 159/300 3/40 , dice_tc: 0.89161444 , dice_wt: 0.9170229 , dice_et: 0.8850136 , time 2.72s\n",
            "Val 159/300 4/40 , dice_tc: 0.80552834 , dice_wt: 0.83874846 , dice_et: 0.6899952 , time 2.53s\n",
            "Val 159/300 5/40 , dice_tc: 0.80712295 , dice_wt: 0.8418637 , dice_et: 0.71545064 , time 2.54s\n",
            "Val 159/300 6/40 , dice_tc: 0.7973243 , dice_wt: 0.83089495 , dice_et: 0.72112244 , time 2.69s\n",
            "Val 159/300 7/40 , dice_tc: 0.8090511 , dice_wt: 0.8294669 , dice_et: 0.74613523 , time 2.57s\n",
            "Val 159/300 8/40 , dice_tc: 0.8091612 , dice_wt: 0.8405056 , dice_et: 0.756636 , time 2.72s\n",
            "Val 159/300 9/40 , dice_tc: 0.8033029 , dice_wt: 0.8471575 , dice_et: 0.76859677 , time 2.58s\n",
            "Val 159/300 10/40 , dice_tc: 0.81109023 , dice_wt: 0.8538822 , dice_et: 0.7805863 , time 2.47s\n",
            "Val 159/300 11/40 , dice_tc: 0.8129248 , dice_wt: 0.84441656 , dice_et: 0.7862543 , time 2.68s\n",
            "Val 159/300 12/40 , dice_tc: 0.7889397 , dice_wt: 0.8515196 , dice_et: 0.7862543 , time 2.59s\n",
            "Val 159/300 13/40 , dice_tc: 0.77883154 , dice_wt: 0.8505253 , dice_et: 0.7742142 , time 2.53s\n",
            "Val 159/300 14/40 , dice_tc: 0.775915 , dice_wt: 0.855051 , dice_et: 0.7146593 , time 2.64s\n",
            "Val 159/300 15/40 , dice_tc: 0.77619535 , dice_wt: 0.8575166 , dice_et: 0.725392 , time 2.72s\n",
            "Val 159/300 16/40 , dice_tc: 0.7496926 , dice_wt: 0.8562348 , dice_et: 0.6982315 , time 2.67s\n",
            "Val 159/300 17/40 , dice_tc: 0.7518522 , dice_wt: 0.86048996 , dice_et: 0.7060355 , time 2.58s\n",
            "Val 159/300 18/40 , dice_tc: 0.76131165 , dice_wt: 0.8567615 , dice_et: 0.71920323 , time 2.74s\n",
            "Val 159/300 19/40 , dice_tc: 0.7649486 , dice_wt: 0.85861236 , dice_et: 0.72606725 , time 2.72s\n",
            "Val 159/300 20/40 , dice_tc: 0.76781434 , dice_wt: 0.8616042 , dice_et: 0.73178464 , time 2.46s\n",
            "Val 159/300 21/40 , dice_tc: 0.77297115 , dice_wt: 0.8641666 , dice_et: 0.73939455 , time 2.52s\n",
            "Val 159/300 22/40 , dice_tc: 0.7690462 , dice_wt: 0.8677446 , dice_et: 0.73972183 , time 2.70s\n",
            "Val 159/300 23/40 , dice_tc: 0.76952076 , dice_wt: 0.87117225 , dice_et: 0.7415388 , time 2.71s\n",
            "Val 159/300 24/40 , dice_tc: 0.76989686 , dice_wt: 0.87445587 , dice_et: 0.74342704 , time 2.66s\n",
            "Val 159/300 25/40 , dice_tc: 0.7659165 , dice_wt: 0.87722206 , dice_et: 0.74342704 , time 2.69s\n",
            "Val 159/300 26/40 , dice_tc: 0.77080244 , dice_wt: 0.8773021 , dice_et: 0.7501242 , time 2.75s\n",
            "Val 159/300 27/40 , dice_tc: 0.7712266 , dice_wt: 0.8771168 , dice_et: 0.74604714 , time 2.72s\n",
            "Val 159/300 28/40 , dice_tc: 0.7732902 , dice_wt: 0.87590176 , dice_et: 0.7499807 , time 2.64s\n",
            "Val 159/300 29/40 , dice_tc: 0.7602945 , dice_wt: 0.87635607 , dice_et: 0.7435003 , time 2.49s\n",
            "Val 159/300 30/40 , dice_tc: 0.762553 , dice_wt: 0.8776053 , dice_et: 0.7269002 , time 2.72s\n",
            "Val 159/300 31/40 , dice_tc: 0.7684942 , dice_wt: 0.8792642 , dice_et: 0.734801 , time 2.65s\n",
            "Val 159/300 32/40 , dice_tc: 0.7608733 , dice_wt: 0.8754251 , dice_et: 0.734801 , time 2.55s\n",
            "Val 159/300 33/40 , dice_tc: 0.76303965 , dice_wt: 0.8752684 , dice_et: 0.7399328 , time 2.70s\n",
            "Val 159/300 34/40 , dice_tc: 0.74911577 , dice_wt: 0.8692119 , dice_et: 0.7259797 , time 2.46s\n",
            "Val 159/300 35/40 , dice_tc: 0.73860365 , dice_wt: 0.86605287 , dice_et: 0.7155589 , time 2.71s\n",
            "Val 159/300 36/40 , dice_tc: 0.7445693 , dice_wt: 0.86743164 , dice_et: 0.72302186 , time 2.47s\n",
            "Val 159/300 37/40 , dice_tc: 0.7424787 , dice_wt: 0.8694006 , dice_et: 0.7234919 , time 2.63s\n",
            "Val 159/300 38/40 , dice_tc: 0.74537265 , dice_wt: 0.87051654 , dice_et: 0.72682714 , time 2.67s\n",
            "Val 159/300 39/40 , dice_tc: 0.7376164 , dice_wt: 0.87049806 , dice_et: 0.72682714 , time 2.68s\n",
            "Final validation stats 159/299 , dice_tc: 0.7376164 , dice_wt: 0.87049806 , dice_et: 0.72682714 , Dice_Avg: 0.7783139 , LR: 0.000043 , time 105.34s\n",
            "new best (0.772249 --> 0.778314). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 14:27:06 2025 Epoch: 160\n",
            "Epoch 160/300 10/80 loss: 0.3392 time 2.69s\n",
            "Epoch 160/300 20/80 loss: 0.3225 time 2.94s\n",
            "Epoch 160/300 30/80 loss: 0.3619 time 2.86s\n",
            "Epoch 160/300 40/80 loss: 0.3407 time 2.92s\n",
            "Epoch 160/300 50/80 loss: 0.3482 time 3.02s\n",
            "Epoch 160/300 60/80 loss: 0.3490 time 3.06s\n",
            "Epoch 160/300 70/80 loss: 0.3435 time 2.88s\n",
            "Epoch 160/300 80/80 loss: 0.3385 time 1.52s\n",
            "Final training  160/299 loss: 0.3385 time 227.91s\n",
            "Thu Dec 18 14:30:54 2025 Epoch: 161\n",
            "Epoch 161/300 10/80 loss: 0.3593 time 3.04s\n",
            "Epoch 161/300 20/80 loss: 0.3382 time 3.02s\n",
            "Epoch 161/300 30/80 loss: 0.3395 time 3.16s\n",
            "Epoch 161/300 40/80 loss: 0.3346 time 2.89s\n",
            "Epoch 161/300 50/80 loss: 0.3423 time 3.12s\n",
            "Epoch 161/300 60/80 loss: 0.3527 time 2.98s\n",
            "Epoch 161/300 70/80 loss: 0.3342 time 2.69s\n",
            "Epoch 161/300 80/80 loss: 0.3291 time 1.63s\n",
            "Final training  161/299 loss: 0.3291 time 226.90s\n",
            "Thu Dec 18 14:34:41 2025 Epoch: 162\n",
            "Epoch 162/300 10/80 loss: 0.3093 time 2.93s\n",
            "Epoch 162/300 20/80 loss: 0.3345 time 2.84s\n",
            "Epoch 162/300 30/80 loss: 0.3628 time 2.79s\n",
            "Epoch 162/300 40/80 loss: 0.3611 time 2.86s\n",
            "Epoch 162/300 50/80 loss: 0.3671 time 3.07s\n",
            "Epoch 162/300 60/80 loss: 0.3557 time 2.84s\n",
            "Epoch 162/300 70/80 loss: 0.3600 time 3.06s\n",
            "Epoch 162/300 80/80 loss: 0.3491 time 1.49s\n",
            "Final training  162/299 loss: 0.3491 time 225.72s\n",
            "Thu Dec 18 14:38:26 2025 Epoch: 163\n",
            "Epoch 163/300 10/80 loss: 0.4087 time 3.03s\n",
            "Epoch 163/300 20/80 loss: 0.4035 time 2.88s\n",
            "Epoch 163/300 30/80 loss: 0.3713 time 3.05s\n",
            "Epoch 163/300 40/80 loss: 0.3752 time 2.80s\n",
            "Epoch 163/300 50/80 loss: 0.3584 time 2.77s\n",
            "Epoch 163/300 60/80 loss: 0.3473 time 2.86s\n",
            "Epoch 163/300 70/80 loss: 0.3404 time 2.99s\n",
            "Epoch 163/300 80/80 loss: 0.3431 time 1.43s\n",
            "Final training  163/299 loss: 0.3431 time 226.56s\n",
            "Thu Dec 18 14:42:13 2025 Epoch: 164\n",
            "Epoch 164/300 10/80 loss: 0.2930 time 2.86s\n",
            "Epoch 164/300 20/80 loss: 0.3115 time 3.19s\n",
            "Epoch 164/300 30/80 loss: 0.3272 time 2.76s\n",
            "Epoch 164/300 40/80 loss: 0.3113 time 3.12s\n",
            "Epoch 164/300 50/80 loss: 0.3176 time 3.21s\n",
            "Epoch 164/300 60/80 loss: 0.3232 time 3.01s\n",
            "Epoch 164/300 70/80 loss: 0.3363 time 2.71s\n",
            "Epoch 164/300 80/80 loss: 0.3390 time 1.46s\n",
            "Final training  164/299 loss: 0.3390 time 226.25s\n",
            "Thu Dec 18 14:45:59 2025 Epoch: 165\n",
            "Epoch 165/300 10/80 loss: 0.2971 time 2.81s\n",
            "Epoch 165/300 20/80 loss: 0.3259 time 2.95s\n",
            "Epoch 165/300 30/80 loss: 0.3071 time 3.10s\n",
            "Epoch 165/300 40/80 loss: 0.3211 time 2.65s\n",
            "Epoch 165/300 50/80 loss: 0.3241 time 2.81s\n",
            "Epoch 165/300 60/80 loss: 0.3215 time 2.98s\n",
            "Epoch 165/300 70/80 loss: 0.3175 time 2.85s\n",
            "Epoch 165/300 80/80 loss: 0.3254 time 1.59s\n",
            "Final training  165/299 loss: 0.3254 time 225.50s\n",
            "Thu Dec 18 14:49:45 2025 Epoch: 166\n",
            "Epoch 166/300 10/80 loss: 0.3523 time 3.01s\n",
            "Epoch 166/300 20/80 loss: 0.2924 time 3.37s\n",
            "Epoch 166/300 30/80 loss: 0.3189 time 2.83s\n",
            "Epoch 166/300 40/80 loss: 0.3103 time 2.97s\n",
            "Epoch 166/300 50/80 loss: 0.3018 time 2.89s\n",
            "Epoch 166/300 60/80 loss: 0.2978 time 2.86s\n",
            "Epoch 166/300 70/80 loss: 0.3253 time 2.84s\n",
            "Epoch 166/300 80/80 loss: 0.3256 time 1.60s\n",
            "Final training  166/299 loss: 0.3256 time 226.45s\n",
            "Thu Dec 18 14:53:31 2025 Epoch: 167\n",
            "Epoch 167/300 10/80 loss: 0.3116 time 2.91s\n",
            "Epoch 167/300 20/80 loss: 0.3092 time 2.82s\n",
            "Epoch 167/300 30/80 loss: 0.3304 time 3.07s\n",
            "Epoch 167/300 40/80 loss: 0.3214 time 3.10s\n",
            "Epoch 167/300 50/80 loss: 0.3200 time 2.72s\n",
            "Epoch 167/300 60/80 loss: 0.3227 time 3.04s\n",
            "Epoch 167/300 70/80 loss: 0.3204 time 2.95s\n",
            "Epoch 167/300 80/80 loss: 0.3205 time 1.48s\n",
            "Final training  167/299 loss: 0.3205 time 227.27s\n",
            "Thu Dec 18 14:57:18 2025 Epoch: 168\n",
            "Epoch 168/300 10/80 loss: 0.3531 time 3.07s\n",
            "Epoch 168/300 20/80 loss: 0.3549 time 3.14s\n",
            "Epoch 168/300 30/80 loss: 0.3435 time 2.83s\n",
            "Epoch 168/300 40/80 loss: 0.3331 time 2.91s\n",
            "Epoch 168/300 50/80 loss: 0.3237 time 2.85s\n",
            "Epoch 168/300 60/80 loss: 0.3273 time 2.60s\n",
            "Epoch 168/300 70/80 loss: 0.3308 time 2.87s\n",
            "Epoch 168/300 80/80 loss: 0.3258 time 1.63s\n",
            "Final training  168/299 loss: 0.3258 time 226.53s\n",
            "Thu Dec 18 15:01:05 2025 Epoch: 169\n",
            "Epoch 169/300 10/80 loss: 0.2616 time 2.78s\n",
            "Epoch 169/300 20/80 loss: 0.2910 time 2.62s\n",
            "Epoch 169/300 30/80 loss: 0.3089 time 2.97s\n",
            "Epoch 169/300 40/80 loss: 0.3023 time 2.89s\n",
            "Epoch 169/300 50/80 loss: 0.3149 time 2.62s\n",
            "Epoch 169/300 60/80 loss: 0.3098 time 2.85s\n",
            "Epoch 169/300 70/80 loss: 0.3071 time 2.87s\n",
            "Epoch 169/300 80/80 loss: 0.3071 time 1.61s\n",
            "Final training  169/299 loss: 0.3071 time 226.93s\n",
            "Val 169/300 0/40 , dice_tc: 0.851633 , dice_wt: 0.9485916 , dice_et: 0.8617697 , time 2.79s\n",
            "Val 169/300 1/40 , dice_tc: 0.9127089 , dice_wt: 0.96182334 , dice_et: 0.8617697 , time 2.69s\n",
            "Val 169/300 2/40 , dice_tc: 0.9264805 , dice_wt: 0.94977206 , dice_et: 0.90794003 , time 2.70s\n",
            "Val 169/300 3/40 , dice_tc: 0.9082419 , dice_wt: 0.9343804 , dice_et: 0.89655346 , time 2.67s\n",
            "Val 169/300 4/40 , dice_tc: 0.8260561 , dice_wt: 0.8558313 , dice_et: 0.7714193 , time 2.60s\n",
            "Val 169/300 5/40 , dice_tc: 0.7762911 , dice_wt: 0.8469873 , dice_et: 0.72754747 , time 2.54s\n",
            "Val 169/300 6/40 , dice_tc: 0.7764368 , dice_wt: 0.8458267 , dice_et: 0.7366972 , time 2.71s\n",
            "Val 169/300 7/40 , dice_tc: 0.774586 , dice_wt: 0.8402977 , dice_et: 0.74187 , time 2.54s\n",
            "Val 169/300 8/40 , dice_tc: 0.78578526 , dice_wt: 0.8509739 , dice_et: 0.75950426 , time 2.77s\n",
            "Val 169/300 9/40 , dice_tc: 0.75979745 , dice_wt: 0.8502706 , dice_et: 0.74732083 , time 2.61s\n",
            "Val 169/300 10/40 , dice_tc: 0.7692759 , dice_wt: 0.8555965 , dice_et: 0.7590438 , time 2.46s\n",
            "Val 169/300 11/40 , dice_tc: 0.77451485 , dice_wt: 0.8535759 , dice_et: 0.7650482 , time 2.70s\n",
            "Val 169/300 12/40 , dice_tc: 0.7692354 , dice_wt: 0.85999864 , dice_et: 0.7650482 , time 2.46s\n",
            "Val 169/300 13/40 , dice_tc: 0.75871783 , dice_wt: 0.8579803 , dice_et: 0.75352883 , time 2.52s\n",
            "Val 169/300 14/40 , dice_tc: 0.754509 , dice_wt: 0.8621319 , dice_et: 0.6955651 , time 2.62s\n",
            "Val 169/300 15/40 , dice_tc: 0.75869447 , dice_wt: 0.8647131 , dice_et: 0.7100025 , time 2.71s\n",
            "Val 169/300 16/40 , dice_tc: 0.73200274 , dice_wt: 0.8628506 , dice_et: 0.68383443 , time 2.72s\n",
            "Val 169/300 17/40 , dice_tc: 0.7316103 , dice_wt: 0.8668746 , dice_et: 0.68795997 , time 2.54s\n",
            "Val 169/300 18/40 , dice_tc: 0.7408532 , dice_wt: 0.86511654 , dice_et: 0.7010354 , time 2.80s\n",
            "Val 169/300 19/40 , dice_tc: 0.744922 , dice_wt: 0.8670126 , dice_et: 0.70792 , time 2.69s\n",
            "Val 169/300 20/40 , dice_tc: 0.7458302 , dice_wt: 0.8696837 , dice_et: 0.71163523 , time 2.46s\n",
            "Val 169/300 21/40 , dice_tc: 0.7537285 , dice_wt: 0.87113947 , dice_et: 0.722217 , time 2.53s\n",
            "Val 169/300 22/40 , dice_tc: 0.74571615 , dice_wt: 0.8749404 , dice_et: 0.71654075 , time 2.72s\n",
            "Val 169/300 23/40 , dice_tc: 0.748005 , dice_wt: 0.8782775 , dice_et: 0.71954465 , time 2.75s\n",
            "Val 169/300 24/40 , dice_tc: 0.74533087 , dice_wt: 0.8815541 , dice_et: 0.7180178 , time 2.75s\n",
            "Val 169/300 25/40 , dice_tc: 0.7407712 , dice_wt: 0.88392234 , dice_et: 0.7180178 , time 2.68s\n",
            "Val 169/300 26/40 , dice_tc: 0.7466722 , dice_wt: 0.88460076 , dice_et: 0.72550195 , time 2.70s\n",
            "Val 169/300 27/40 , dice_tc: 0.74702746 , dice_wt: 0.884126 , dice_et: 0.7230178 , time 2.79s\n",
            "Val 169/300 28/40 , dice_tc: 0.75030094 , dice_wt: 0.8840807 , dice_et: 0.7274345 , time 2.64s\n",
            "Val 169/300 29/40 , dice_tc: 0.73285216 , dice_wt: 0.8822292 , dice_et: 0.7091603 , time 2.46s\n",
            "Val 169/300 30/40 , dice_tc: 0.7352839 , dice_wt: 0.8834196 , dice_et: 0.69494164 , time 2.66s\n",
            "Val 169/300 31/40 , dice_tc: 0.741867 , dice_wt: 0.8846925 , dice_et: 0.70369804 , time 2.71s\n",
            "Val 169/300 32/40 , dice_tc: 0.7294614 , dice_wt: 0.883654 , dice_et: 0.70369804 , time 2.62s\n",
            "Val 169/300 33/40 , dice_tc: 0.7329217 , dice_wt: 0.8828846 , dice_et: 0.7094347 , time 2.69s\n",
            "Val 169/300 34/40 , dice_tc: 0.73115057 , dice_wt: 0.87975913 , dice_et: 0.70993024 , time 2.47s\n",
            "Val 169/300 35/40 , dice_tc: 0.7191098 , dice_wt: 0.87726283 , dice_et: 0.6984069 , time 2.70s\n",
            "Val 169/300 36/40 , dice_tc: 0.7259438 , dice_wt: 0.87863714 , dice_et: 0.706717 , time 2.63s\n",
            "Val 169/300 37/40 , dice_tc: 0.72556126 , dice_wt: 0.8802191 , dice_et: 0.7077334 , time 2.71s\n",
            "Val 169/300 38/40 , dice_tc: 0.72975224 , dice_wt: 0.8811075 , dice_et: 0.71294284 , time 2.67s\n",
            "Val 169/300 39/40 , dice_tc: 0.71863633 , dice_wt: 0.88125515 , dice_et: 0.71294284 , time 2.74s\n",
            "Final validation stats 169/299 , dice_tc: 0.71863633 , dice_wt: 0.88125515 , dice_et: 0.71294284 , Dice_Avg: 0.7709448 , LR: 0.000045 , time 105.94s\n",
            "Thu Dec 18 15:06:38 2025 Epoch: 170\n",
            "Epoch 170/300 10/80 loss: 0.3163 time 3.15s\n",
            "Epoch 170/300 20/80 loss: 0.2742 time 2.97s\n",
            "Epoch 170/300 30/80 loss: 0.2653 time 2.80s\n",
            "Epoch 170/300 40/80 loss: 0.2784 time 2.71s\n",
            "Epoch 170/300 50/80 loss: 0.2922 time 2.85s\n",
            "Epoch 170/300 60/80 loss: 0.2841 time 3.00s\n",
            "Epoch 170/300 70/80 loss: 0.2945 time 2.84s\n",
            "Epoch 170/300 80/80 loss: 0.2985 time 1.40s\n",
            "Final training  170/299 loss: 0.2985 time 226.00s\n",
            "Thu Dec 18 15:10:24 2025 Epoch: 171\n",
            "Epoch 171/300 10/80 loss: 0.3239 time 2.77s\n",
            "Epoch 171/300 20/80 loss: 0.2989 time 3.15s\n",
            "Epoch 171/300 30/80 loss: 0.2920 time 2.99s\n",
            "Epoch 171/300 40/80 loss: 0.2862 time 2.81s\n",
            "Epoch 171/300 50/80 loss: 0.2810 time 3.01s\n",
            "Epoch 171/300 60/80 loss: 0.2849 time 2.63s\n",
            "Epoch 171/300 70/80 loss: 0.2831 time 2.75s\n",
            "Epoch 171/300 80/80 loss: 0.2884 time 1.66s\n",
            "Final training  171/299 loss: 0.2884 time 225.96s\n",
            "Thu Dec 18 15:14:10 2025 Epoch: 172\n",
            "Epoch 172/300 10/80 loss: 0.2623 time 3.04s\n",
            "Epoch 172/300 20/80 loss: 0.2364 time 2.84s\n",
            "Epoch 172/300 30/80 loss: 0.2662 time 3.02s\n",
            "Epoch 172/300 40/80 loss: 0.2801 time 3.06s\n",
            "Epoch 172/300 50/80 loss: 0.2845 time 2.89s\n",
            "Epoch 172/300 60/80 loss: 0.2990 time 2.83s\n",
            "Epoch 172/300 70/80 loss: 0.3107 time 3.00s\n",
            "Epoch 172/300 80/80 loss: 0.3100 time 1.63s\n",
            "Final training  172/299 loss: 0.3100 time 225.88s\n",
            "Thu Dec 18 15:17:56 2025 Epoch: 173\n",
            "Epoch 173/300 10/80 loss: 0.3850 time 2.94s\n",
            "Epoch 173/300 20/80 loss: 0.3576 time 3.02s\n",
            "Epoch 173/300 30/80 loss: 0.3390 time 2.91s\n",
            "Epoch 173/300 40/80 loss: 0.3280 time 2.84s\n",
            "Epoch 173/300 50/80 loss: 0.3197 time 2.80s\n",
            "Epoch 173/300 60/80 loss: 0.3277 time 3.06s\n",
            "Epoch 173/300 70/80 loss: 0.3407 time 3.01s\n",
            "Epoch 173/300 80/80 loss: 0.3332 time 1.42s\n",
            "Final training  173/299 loss: 0.3332 time 226.44s\n",
            "Thu Dec 18 15:21:42 2025 Epoch: 174\n",
            "Epoch 174/300 10/80 loss: 0.3386 time 2.98s\n",
            "Epoch 174/300 20/80 loss: 0.3441 time 2.91s\n",
            "Epoch 174/300 30/80 loss: 0.3501 time 2.96s\n",
            "Epoch 174/300 40/80 loss: 0.3480 time 2.90s\n",
            "Epoch 174/300 50/80 loss: 0.3446 time 2.62s\n",
            "Epoch 174/300 60/80 loss: 0.3409 time 3.06s\n",
            "Epoch 174/300 70/80 loss: 0.3340 time 2.70s\n",
            "Epoch 174/300 80/80 loss: 0.3257 time 1.67s\n",
            "Final training  174/299 loss: 0.3257 time 226.99s\n",
            "Thu Dec 18 15:25:29 2025 Epoch: 175\n",
            "Epoch 175/300 10/80 loss: 0.3077 time 2.82s\n",
            "Epoch 175/300 20/80 loss: 0.3469 time 2.79s\n",
            "Epoch 175/300 30/80 loss: 0.3305 time 3.06s\n",
            "Epoch 175/300 40/80 loss: 0.3094 time 2.67s\n",
            "Epoch 175/300 50/80 loss: 0.2871 time 3.07s\n",
            "Epoch 175/300 60/80 loss: 0.2891 time 2.82s\n",
            "Epoch 175/300 70/80 loss: 0.3003 time 2.83s\n",
            "Epoch 175/300 80/80 loss: 0.3113 time 1.63s\n",
            "Final training  175/299 loss: 0.3113 time 226.45s\n",
            "Thu Dec 18 15:29:15 2025 Epoch: 176\n",
            "Epoch 176/300 10/80 loss: 0.2760 time 3.05s\n",
            "Epoch 176/300 20/80 loss: 0.3271 time 3.08s\n",
            "Epoch 176/300 30/80 loss: 0.3127 time 2.73s\n",
            "Epoch 176/300 40/80 loss: 0.3135 time 2.70s\n",
            "Epoch 176/300 50/80 loss: 0.3174 time 3.07s\n",
            "Epoch 176/300 60/80 loss: 0.3101 time 3.04s\n",
            "Epoch 176/300 70/80 loss: 0.3190 time 3.04s\n",
            "Epoch 176/300 80/80 loss: 0.3176 time 1.48s\n",
            "Final training  176/299 loss: 0.3176 time 227.23s\n",
            "Thu Dec 18 15:33:03 2025 Epoch: 177\n",
            "Epoch 177/300 10/80 loss: 0.2843 time 2.89s\n",
            "Epoch 177/300 20/80 loss: 0.3234 time 3.06s\n",
            "Epoch 177/300 30/80 loss: 0.3227 time 3.09s\n",
            "Epoch 177/300 40/80 loss: 0.3103 time 2.63s\n",
            "Epoch 177/300 50/80 loss: 0.3067 time 2.81s\n",
            "Epoch 177/300 60/80 loss: 0.3126 time 2.72s\n",
            "Epoch 177/300 70/80 loss: 0.3106 time 3.06s\n",
            "Epoch 177/300 80/80 loss: 0.3113 time 1.67s\n",
            "Final training  177/299 loss: 0.3113 time 227.04s\n",
            "Thu Dec 18 15:36:50 2025 Epoch: 178\n",
            "Epoch 178/300 10/80 loss: 0.2936 time 2.93s\n",
            "Epoch 178/300 20/80 loss: 0.2983 time 3.04s\n",
            "Epoch 178/300 30/80 loss: 0.3006 time 3.06s\n",
            "Epoch 178/300 40/80 loss: 0.3297 time 2.86s\n",
            "Epoch 178/300 50/80 loss: 0.3189 time 3.11s\n",
            "Epoch 178/300 60/80 loss: 0.3132 time 2.82s\n",
            "Epoch 178/300 70/80 loss: 0.3078 time 2.79s\n",
            "Epoch 178/300 80/80 loss: 0.3108 time 1.63s\n",
            "Final training  178/299 loss: 0.3108 time 226.33s\n",
            "Thu Dec 18 15:40:36 2025 Epoch: 179\n",
            "Epoch 179/300 10/80 loss: 0.2134 time 2.85s\n",
            "Epoch 179/300 20/80 loss: 0.2236 time 2.81s\n",
            "Epoch 179/300 30/80 loss: 0.2480 time 3.00s\n",
            "Epoch 179/300 40/80 loss: 0.2655 time 2.67s\n",
            "Epoch 179/300 50/80 loss: 0.2690 time 2.86s\n",
            "Epoch 179/300 60/80 loss: 0.2777 time 3.08s\n",
            "Epoch 179/300 70/80 loss: 0.2816 time 3.04s\n",
            "Epoch 179/300 80/80 loss: 0.2835 time 1.62s\n",
            "Final training  179/299 loss: 0.2835 time 226.92s\n",
            "Val 179/300 0/40 , dice_tc: 0.8497711 , dice_wt: 0.94541 , dice_et: 0.86388946 , time 2.73s\n",
            "Val 179/300 1/40 , dice_tc: 0.80954957 , dice_wt: 0.9566289 , dice_et: 0.86388946 , time 2.68s\n",
            "Val 179/300 2/40 , dice_tc: 0.8581738 , dice_wt: 0.9476773 , dice_et: 0.9092128 , time 2.70s\n",
            "Val 179/300 3/40 , dice_tc: 0.8623207 , dice_wt: 0.9350879 , dice_et: 0.9022587 , time 2.69s\n",
            "Val 179/300 4/40 , dice_tc: 0.7890855 , dice_wt: 0.85077363 , dice_et: 0.7203846 , time 2.53s\n",
            "Val 179/300 5/40 , dice_tc: 0.79845667 , dice_wt: 0.8534841 , dice_et: 0.7434117 , time 2.54s\n",
            "Val 179/300 6/40 , dice_tc: 0.8187109 , dice_wt: 0.8570486 , dice_et: 0.7764468 , time 2.74s\n",
            "Val 179/300 7/40 , dice_tc: 0.8156986 , dice_wt: 0.8537064 , dice_et: 0.78147334 , time 2.64s\n",
            "Val 179/300 8/40 , dice_tc: 0.8157645 , dice_wt: 0.86237144 , dice_et: 0.79164064 , time 2.76s\n",
            "Val 179/300 9/40 , dice_tc: 0.8138102 , dice_wt: 0.86821634 , dice_et: 0.8075917 , time 2.57s\n",
            "Val 179/300 10/40 , dice_tc: 0.8191709 , dice_wt: 0.87255496 , dice_et: 0.8143447 , time 2.49s\n",
            "Val 179/300 11/40 , dice_tc: 0.82113475 , dice_wt: 0.8717327 , dice_et: 0.8164697 , time 2.67s\n",
            "Val 179/300 12/40 , dice_tc: 0.82280034 , dice_wt: 0.87660986 , dice_et: 0.8164697 , time 2.60s\n",
            "Val 179/300 13/40 , dice_tc: 0.81152976 , dice_wt: 0.8753268 , dice_et: 0.8034389 , time 2.54s\n",
            "Val 179/300 14/40 , dice_tc: 0.8109818 , dice_wt: 0.87900704 , dice_et: 0.7416359 , time 2.63s\n",
            "Val 179/300 15/40 , dice_tc: 0.81087 , dice_wt: 0.8802667 , dice_et: 0.7532911 , time 2.72s\n",
            "Val 179/300 16/40 , dice_tc: 0.7866586 , dice_wt: 0.877143 , dice_et: 0.73104095 , time 2.76s\n",
            "Val 179/300 17/40 , dice_tc: 0.786838 , dice_wt: 0.88025504 , dice_et: 0.73641455 , time 2.53s\n",
            "Val 179/300 18/40 , dice_tc: 0.79475486 , dice_wt: 0.87774163 , dice_et: 0.7481481 , time 2.76s\n",
            "Val 179/300 19/40 , dice_tc: 0.79551417 , dice_wt: 0.87874115 , dice_et: 0.75197357 , time 2.71s\n",
            "Val 179/300 20/40 , dice_tc: 0.7958968 , dice_wt: 0.88093007 , dice_et: 0.75537413 , time 2.45s\n",
            "Val 179/300 21/40 , dice_tc: 0.8025673 , dice_wt: 0.8827709 , dice_et: 0.76459455 , time 2.58s\n",
            "Val 179/300 22/40 , dice_tc: 0.7939 , dice_wt: 0.8859616 , dice_et: 0.75843096 , time 2.74s\n",
            "Val 179/300 23/40 , dice_tc: 0.7897306 , dice_wt: 0.8882373 , dice_et: 0.7564848 , time 2.76s\n",
            "Val 179/300 24/40 , dice_tc: 0.7867315 , dice_wt: 0.89099616 , dice_et: 0.7558495 , time 2.74s\n",
            "Val 179/300 25/40 , dice_tc: 0.7762401 , dice_wt: 0.8931589 , dice_et: 0.7558495 , time 2.78s\n",
            "Val 179/300 26/40 , dice_tc: 0.7812469 , dice_wt: 0.8935676 , dice_et: 0.7623872 , time 2.75s\n",
            "Val 179/300 27/40 , dice_tc: 0.7791174 , dice_wt: 0.89361477 , dice_et: 0.7510681 , time 2.71s\n",
            "Val 179/300 28/40 , dice_tc: 0.7817192 , dice_wt: 0.89363694 , dice_et: 0.7552992 , time 2.65s\n",
            "Val 179/300 29/40 , dice_tc: 0.76841724 , dice_wt: 0.8939402 , dice_et: 0.74493194 , time 2.47s\n",
            "Val 179/300 30/40 , dice_tc: 0.7698744 , dice_wt: 0.8947386 , dice_et: 0.72719747 , time 2.71s\n",
            "Val 179/300 31/40 , dice_tc: 0.77567923 , dice_wt: 0.89602757 , dice_et: 0.73507226 , time 2.68s\n",
            "Val 179/300 32/40 , dice_tc: 0.75820696 , dice_wt: 0.8933551 , dice_et: 0.73507226 , time 2.54s\n",
            "Val 179/300 33/40 , dice_tc: 0.76041126 , dice_wt: 0.892986 , dice_et: 0.7397025 , time 2.69s\n",
            "Val 179/300 34/40 , dice_tc: 0.7648539 , dice_wt: 0.8923962 , dice_et: 0.74548703 , time 2.50s\n",
            "Val 179/300 35/40 , dice_tc: 0.7642622 , dice_wt: 0.89133805 , dice_et: 0.7461758 , time 2.72s\n",
            "Val 179/300 36/40 , dice_tc: 0.7697501 , dice_wt: 0.8925466 , dice_et: 0.7528602 , time 2.46s\n",
            "Val 179/300 37/40 , dice_tc: 0.76411057 , dice_wt: 0.89388496 , dice_et: 0.75330305 , time 2.72s\n",
            "Val 179/300 38/40 , dice_tc: 0.767848 , dice_wt: 0.89446914 , dice_et: 0.7574148 , time 2.67s\n",
            "Val 179/300 39/40 , dice_tc: 0.75954634 , dice_wt: 0.8940562 , dice_et: 0.7574148 , time 2.72s\n",
            "Final validation stats 179/299 , dice_tc: 0.75954634 , dice_wt: 0.8940562 , dice_et: 0.7574148 , Dice_Avg: 0.8036725 , LR: 0.000048 , time 106.06s\n",
            "new best (0.778314 --> 0.803672). \n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "Thu Dec 18 15:46:11 2025 Epoch: 180\n",
            "Epoch 180/300 10/80 loss: 0.3569 time 2.89s\n",
            "Epoch 180/300 20/80 loss: 0.3491 time 2.89s\n",
            "Epoch 180/300 30/80 loss: 0.3467 time 3.09s\n",
            "Epoch 180/300 40/80 loss: 0.3065 time 3.02s\n",
            "Epoch 180/300 50/80 loss: 0.3072 time 2.95s\n",
            "Epoch 180/300 60/80 loss: 0.3111 time 2.84s\n",
            "Epoch 180/300 70/80 loss: 0.3162 time 2.70s\n",
            "Epoch 180/300 80/80 loss: 0.3228 time 1.46s\n",
            "Final training  180/299 loss: 0.3228 time 227.06s\n",
            "Thu Dec 18 15:49:59 2025 Epoch: 181\n",
            "Epoch 181/300 10/80 loss: 0.3512 time 2.81s\n",
            "Epoch 181/300 20/80 loss: 0.3811 time 2.86s\n",
            "Epoch 181/300 30/80 loss: 0.3530 time 2.80s\n",
            "Epoch 181/300 40/80 loss: 0.3535 time 3.03s\n",
            "Epoch 181/300 50/80 loss: 0.3485 time 2.92s\n",
            "Epoch 181/300 60/80 loss: 0.3521 time 2.82s\n",
            "Epoch 181/300 70/80 loss: 0.3495 time 2.63s\n",
            "Epoch 181/300 80/80 loss: 0.3314 time 1.66s\n",
            "Final training  181/299 loss: 0.3314 time 226.66s\n",
            "Thu Dec 18 15:53:45 2025 Epoch: 182\n",
            "Epoch 182/300 10/80 loss: 0.3507 time 2.71s\n",
            "Epoch 182/300 20/80 loss: 0.3276 time 3.10s\n",
            "Epoch 182/300 30/80 loss: 0.3223 time 2.70s\n",
            "Epoch 182/300 40/80 loss: 0.3146 time 2.74s\n",
            "Epoch 182/300 50/80 loss: 0.2915 time 2.84s\n",
            "Epoch 182/300 60/80 loss: 0.3075 time 2.73s\n",
            "Epoch 182/300 70/80 loss: 0.3188 time 2.90s\n",
            "Epoch 182/300 80/80 loss: 0.3226 time 1.42s\n",
            "Final training  182/299 loss: 0.3226 time 227.04s\n",
            "Thu Dec 18 15:57:32 2025 Epoch: 183\n",
            "Epoch 183/300 10/80 loss: 0.3662 time 3.06s\n",
            "Epoch 183/300 20/80 loss: 0.3333 time 2.89s\n",
            "Epoch 183/300 30/80 loss: 0.3340 time 2.87s\n",
            "Epoch 183/300 40/80 loss: 0.3360 time 2.93s\n",
            "Epoch 183/300 50/80 loss: 0.3499 time 3.06s\n",
            "Epoch 183/300 60/80 loss: 0.3243 time 2.92s\n",
            "Epoch 183/300 70/80 loss: 0.3270 time 2.95s\n",
            "Epoch 183/300 80/80 loss: 0.3293 time 1.66s\n",
            "Final training  183/299 loss: 0.3293 time 227.51s\n",
            "Thu Dec 18 16:01:20 2025 Epoch: 184\n",
            "Epoch 184/300 10/80 loss: 0.2318 time 3.05s\n",
            "Epoch 184/300 20/80 loss: 0.2710 time 2.87s\n",
            "Epoch 184/300 30/80 loss: 0.2919 time 3.05s\n",
            "Epoch 184/300 40/80 loss: 0.3008 time 2.96s\n",
            "Epoch 184/300 50/80 loss: 0.3043 time 2.67s\n",
            "Epoch 184/300 60/80 loss: 0.3088 time 2.87s\n",
            "Epoch 184/300 70/80 loss: 0.3060 time 3.01s\n",
            "Epoch 184/300 80/80 loss: 0.3124 time 1.60s\n",
            "Final training  184/299 loss: 0.3124 time 226.18s\n",
            "Thu Dec 18 16:05:06 2025 Epoch: 185\n",
            "Epoch 185/300 10/80 loss: 0.3951 time 2.68s\n",
            "Epoch 185/300 20/80 loss: 0.3835 time 2.64s\n",
            "Epoch 185/300 30/80 loss: 0.3788 time 2.82s\n",
            "Epoch 185/300 40/80 loss: 0.3527 time 2.83s\n",
            "Epoch 185/300 50/80 loss: 0.3282 time 3.04s\n",
            "Epoch 185/300 60/80 loss: 0.3094 time 3.00s\n",
            "Epoch 185/300 70/80 loss: 0.3090 time 2.85s\n",
            "Epoch 185/300 80/80 loss: 0.3037 time 1.65s\n",
            "Final training  185/299 loss: 0.3037 time 226.84s\n",
            "Thu Dec 18 16:08:53 2025 Epoch: 186\n",
            "Epoch 186/300 10/80 loss: 0.2852 time 2.86s\n",
            "Epoch 186/300 20/80 loss: 0.3052 time 2.82s\n",
            "Epoch 186/300 30/80 loss: 0.2864 time 2.84s\n",
            "Epoch 186/300 40/80 loss: 0.2854 time 3.05s\n",
            "Epoch 186/300 50/80 loss: 0.2793 time 2.82s\n",
            "Epoch 186/300 60/80 loss: 0.2842 time 2.98s\n",
            "Epoch 186/300 70/80 loss: 0.2924 time 3.07s\n",
            "Epoch 186/300 80/80 loss: 0.2923 time 1.39s\n",
            "Final training  186/299 loss: 0.2923 time 226.45s\n",
            "Thu Dec 18 16:12:39 2025 Epoch: 187\n",
            "Epoch 187/300 10/80 loss: 0.2971 time 2.95s\n",
            "Epoch 187/300 20/80 loss: 0.2978 time 2.66s\n",
            "Epoch 187/300 30/80 loss: 0.3028 time 2.84s\n",
            "Epoch 187/300 40/80 loss: 0.3280 time 2.92s\n",
            "Epoch 187/300 50/80 loss: 0.3197 time 2.81s\n",
            "Epoch 187/300 60/80 loss: 0.3223 time 2.81s\n",
            "Epoch 187/300 70/80 loss: 0.3155 time 3.06s\n",
            "Epoch 187/300 80/80 loss: 0.3096 time 1.41s\n",
            "Final training  187/299 loss: 0.3096 time 226.76s\n",
            "Thu Dec 18 16:16:26 2025 Epoch: 188\n",
            "Epoch 188/300 10/80 loss: 0.2688 time 3.00s\n",
            "Epoch 188/300 20/80 loss: 0.2697 time 2.69s\n",
            "Epoch 188/300 30/80 loss: 0.2901 time 2.81s\n",
            "Epoch 188/300 40/80 loss: 0.2767 time 2.97s\n",
            "Epoch 188/300 50/80 loss: 0.2840 time 3.06s\n",
            "Epoch 188/300 60/80 loss: 0.2756 time 2.88s\n",
            "Epoch 188/300 70/80 loss: 0.2860 time 3.10s\n",
            "Epoch 188/300 80/80 loss: 0.2934 time 1.61s\n",
            "Final training  188/299 loss: 0.2934 time 227.59s\n",
            "Thu Dec 18 16:20:14 2025 Epoch: 189\n",
            "Epoch 189/300 10/80 loss: 0.3256 time 2.66s\n",
            "Epoch 189/300 20/80 loss: 0.3445 time 3.14s\n",
            "Epoch 189/300 30/80 loss: 0.3210 time 2.86s\n",
            "Epoch 189/300 40/80 loss: 0.3036 time 2.83s\n",
            "Epoch 189/300 50/80 loss: 0.2983 time 2.73s\n",
            "Epoch 189/300 60/80 loss: 0.2952 time 2.90s\n",
            "Epoch 189/300 70/80 loss: 0.2899 time 3.04s\n",
            "Epoch 189/300 80/80 loss: 0.2867 time 1.63s\n",
            "Final training  189/299 loss: 0.2867 time 227.31s\n",
            "Val 189/300 0/40 , dice_tc: 0.8374245 , dice_wt: 0.94197667 , dice_et: 0.8423883 , time 2.71s\n",
            "Val 189/300 1/40 , dice_tc: 0.9057306 , dice_wt: 0.95660806 , dice_et: 0.8423883 , time 2.66s\n",
            "Val 189/300 2/40 , dice_tc: 0.9138603 , dice_wt: 0.9343259 , dice_et: 0.8874998 , time 2.70s\n",
            "Val 189/300 3/40 , dice_tc: 0.8914244 , dice_wt: 0.9190624 , dice_et: 0.87192243 , time 2.67s\n",
            "Val 189/300 4/40 , dice_tc: 0.8158822 , dice_wt: 0.84724534 , dice_et: 0.6597488 , time 2.52s\n",
            "Val 189/300 5/40 , dice_tc: 0.8196041 , dice_wt: 0.8491835 , dice_et: 0.69581234 , time 2.53s\n",
            "Val 189/300 6/40 , dice_tc: 0.81558764 , dice_wt: 0.84100306 , dice_et: 0.714009 , time 2.69s\n",
            "Val 189/300 7/40 , dice_tc: 0.8225192 , dice_wt: 0.84375644 , dice_et: 0.7367665 , time 2.53s\n",
            "Val 189/300 8/40 , dice_tc: 0.8198572 , dice_wt: 0.8532732 , dice_et: 0.7471009 , time 2.81s\n",
            "Val 189/300 9/40 , dice_tc: 0.81381685 , dice_wt: 0.8589514 , dice_et: 0.7687478 , time 2.62s\n",
            "Val 189/300 10/40 , dice_tc: 0.82039875 , dice_wt: 0.86386067 , dice_et: 0.78048503 , time 2.48s\n",
            "Val 189/300 11/40 , dice_tc: 0.8233679 , dice_wt: 0.8600722 , dice_et: 0.7880241 , time 2.68s\n",
            "Val 189/300 12/40 , dice_tc: 0.81591326 , dice_wt: 0.8660699 , dice_et: 0.7880241 , time 2.56s\n",
            "Val 189/300 13/40 , dice_tc: 0.8121062 , dice_wt: 0.86629146 , dice_et: 0.7861784 , time 2.56s\n",
            "Val 189/300 14/40 , dice_tc: 0.80264354 , dice_wt: 0.8703988 , dice_et: 0.7257032 , time 2.62s\n",
            "Val 189/300 15/40 , dice_tc: 0.80228674 , dice_wt: 0.8724466 , dice_et: 0.7370022 , time 2.71s\n",
            "Val 189/300 16/40 , dice_tc: 0.7727457 , dice_wt: 0.86974573 , dice_et: 0.7077062 , time 2.71s\n",
            "Val 189/300 17/40 , dice_tc: 0.772134 , dice_wt: 0.87290764 , dice_et: 0.71196634 , time 2.58s\n",
            "Val 189/300 18/40 , dice_tc: 0.77822644 , dice_wt: 0.8694718 , dice_et: 0.72248274 , time 2.73s\n",
            "Val 189/300 19/40 , dice_tc: 0.7816199 , dice_wt: 0.8713544 , dice_et: 0.72994626 , time 2.66s\n",
            "Val 189/300 20/40 , dice_tc: 0.7821742 , dice_wt: 0.87292254 , dice_et: 0.7330947 , time 2.46s\n",
            "Val 189/300 21/40 , dice_tc: 0.7852112 , dice_wt: 0.8740572 , dice_et: 0.73917043 , time 2.50s\n",
            "Val 189/300 22/40 , dice_tc: 0.78057784 , dice_wt: 0.87720793 , dice_et: 0.7380129 , time 2.81s\n",
            "Val 189/300 23/40 , dice_tc: 0.7759223 , dice_wt: 0.87990195 , dice_et: 0.73637426 , time 2.77s\n",
            "Val 189/300 24/40 , dice_tc: 0.77837753 , dice_wt: 0.88314795 , dice_et: 0.7407354 , time 2.73s\n",
            "Val 189/300 25/40 , dice_tc: 0.77544564 , dice_wt: 0.8855191 , dice_et: 0.7407354 , time 2.77s\n",
            "Val 189/300 26/40 , dice_tc: 0.7805061 , dice_wt: 0.88562995 , dice_et: 0.74803656 , time 2.82s\n",
            "Val 189/300 27/40 , dice_tc: 0.78137606 , dice_wt: 0.8855977 , dice_et: 0.74035454 , time 2.73s\n",
            "Val 189/300 28/40 , dice_tc: 0.7842689 , dice_wt: 0.8852155 , dice_et: 0.7453149 , time 2.65s\n",
            "Val 189/300 29/40 , dice_tc: 0.7707907 , dice_wt: 0.88595015 , dice_et: 0.73595405 , time 2.48s\n",
            "Val 189/300 30/40 , dice_tc: 0.7726703 , dice_wt: 0.88658035 , dice_et: 0.72009623 , time 2.67s\n",
            "Val 189/300 31/40 , dice_tc: 0.77829766 , dice_wt: 0.8877702 , dice_et: 0.72816575 , time 2.66s\n",
            "Val 189/300 32/40 , dice_tc: 0.7547129 , dice_wt: 0.8817487 , dice_et: 0.72816575 , time 2.48s\n",
            "Val 189/300 33/40 , dice_tc: 0.7575676 , dice_wt: 0.8817023 , dice_et: 0.73368156 , time 2.66s\n",
            "Val 189/300 34/40 , dice_tc: 0.7600378 , dice_wt: 0.8808391 , dice_et: 0.7378185 , time 2.46s\n",
            "Val 189/300 35/40 , dice_tc: 0.7457086 , dice_wt: 0.8775696 , dice_et: 0.7233399 , time 2.73s\n",
            "Val 189/300 36/40 , dice_tc: 0.7514562 , dice_wt: 0.87893045 , dice_et: 0.7304846 , time 2.49s\n",
            "Val 189/300 37/40 , dice_tc: 0.7507421 , dice_wt: 0.88059986 , dice_et: 0.73108923 , time 2.72s\n",
            "Val 189/300 38/40 , dice_tc: 0.75295067 , dice_wt: 0.88101286 , dice_et: 0.7342074 , time 2.68s\n",
            "Val 189/300 39/40 , dice_tc: 0.74138916 , dice_wt: 0.8811325 , dice_et: 0.7342074 , time 2.65s\n",
            "Final validation stats 189/299 , dice_tc: 0.74138916 , dice_wt: 0.8811325 , dice_et: 0.7342074 , Dice_Avg: 0.78557634 , LR: 0.000050 , time 105.64s\n",
            "Thu Dec 18 16:25:47 2025 Epoch: 190\n",
            "Epoch 190/300 10/80 loss: 0.3134 time 2.75s\n",
            "Epoch 190/300 20/80 loss: 0.3438 time 2.63s\n",
            "Epoch 190/300 30/80 loss: 0.3184 time 2.81s\n",
            "Epoch 190/300 40/80 loss: 0.3235 time 2.81s\n",
            "Epoch 190/300 50/80 loss: 0.3410 time 2.82s\n",
            "Epoch 190/300 60/80 loss: 0.3434 time 2.91s\n",
            "Epoch 190/300 70/80 loss: 0.3612 time 2.82s\n",
            "Epoch 190/300 80/80 loss: 0.3543 time 1.61s\n",
            "Final training  190/299 loss: 0.3543 time 225.51s\n",
            "Thu Dec 18 16:29:32 2025 Epoch: 191\n",
            "Epoch 191/300 10/80 loss: 0.3344 time 2.84s\n",
            "Epoch 191/300 20/80 loss: 0.3169 time 3.09s\n",
            "Epoch 191/300 30/80 loss: 0.3127 time 3.02s\n",
            "Epoch 191/300 40/80 loss: 0.3219 time 2.87s\n",
            "Epoch 191/300 50/80 loss: 0.3240 time 2.68s\n",
            "Epoch 191/300 60/80 loss: 0.3307 time 2.84s\n",
            "Epoch 191/300 70/80 loss: 0.3259 time 2.91s\n",
            "Epoch 191/300 80/80 loss: 0.3258 time 1.65s\n",
            "Final training  191/299 loss: 0.3258 time 226.26s\n",
            "Thu Dec 18 16:33:18 2025 Epoch: 192\n",
            "Epoch 192/300 10/80 loss: 0.2507 time 3.03s\n",
            "Epoch 192/300 20/80 loss: 0.3013 time 3.10s\n",
            "Epoch 192/300 30/80 loss: 0.3143 time 2.86s\n",
            "Epoch 192/300 40/80 loss: 0.3201 time 3.07s\n",
            "Epoch 192/300 50/80 loss: 0.3047 time 2.73s\n",
            "Epoch 192/300 60/80 loss: 0.2976 time 3.07s\n",
            "Epoch 192/300 70/80 loss: 0.2926 time 3.10s\n",
            "Epoch 192/300 80/80 loss: 0.2984 time 1.65s\n",
            "Final training  192/299 loss: 0.2984 time 227.80s\n",
            "Thu Dec 18 16:37:06 2025 Epoch: 193\n",
            "Epoch 193/300 10/80 loss: 0.3174 time 3.06s\n",
            "Epoch 193/300 20/80 loss: 0.3081 time 2.83s\n",
            "Epoch 193/300 30/80 loss: 0.3230 time 3.09s\n",
            "Epoch 193/300 40/80 loss: 0.3053 time 2.69s\n",
            "Epoch 193/300 50/80 loss: 0.3151 time 2.86s\n",
            "Epoch 193/300 60/80 loss: 0.3196 time 2.99s\n",
            "Epoch 193/300 70/80 loss: 0.3222 time 2.84s\n",
            "Epoch 193/300 80/80 loss: 0.3125 time 1.51s\n",
            "Final training  193/299 loss: 0.3125 time 226.25s\n",
            "Thu Dec 18 16:40:52 2025 Epoch: 194\n",
            "Epoch 194/300 10/80 loss: 0.2898 time 2.85s\n",
            "Epoch 194/300 20/80 loss: 0.2828 time 2.63s\n",
            "Epoch 194/300 30/80 loss: 0.2805 time 3.08s\n",
            "Epoch 194/300 40/80 loss: 0.2988 time 3.07s\n",
            "Epoch 194/300 50/80 loss: 0.2939 time 2.67s\n",
            "Epoch 194/300 60/80 loss: 0.3080 time 3.06s\n",
            "Epoch 194/300 70/80 loss: 0.3095 time 3.06s\n",
            "Epoch 194/300 80/80 loss: 0.3027 time 1.57s\n",
            "Final training  194/299 loss: 0.3027 time 227.19s\n",
            "Thu Dec 18 16:44:40 2025 Epoch: 195\n",
            "Epoch 195/300 10/80 loss: 0.2939 time 2.76s\n",
            "Epoch 195/300 20/80 loss: 0.3016 time 2.88s\n",
            "Epoch 195/300 30/80 loss: 0.3170 time 3.02s\n",
            "Epoch 195/300 40/80 loss: 0.3147 time 3.06s\n",
            "Epoch 195/300 50/80 loss: 0.3158 time 2.97s\n",
            "Epoch 195/300 60/80 loss: 0.2970 time 3.17s\n",
            "Epoch 195/300 70/80 loss: 0.3060 time 2.87s\n",
            "Epoch 195/300 80/80 loss: 0.3133 time 1.59s\n",
            "Final training  195/299 loss: 0.3133 time 227.44s\n",
            "Thu Dec 18 16:48:27 2025 Epoch: 196\n",
            "Epoch 196/300 10/80 loss: 0.2594 time 2.82s\n",
            "Epoch 196/300 20/80 loss: 0.2621 time 2.88s\n",
            "Epoch 196/300 30/80 loss: 0.2710 time 2.83s\n",
            "Epoch 196/300 40/80 loss: 0.2776 time 2.96s\n",
            "Epoch 196/300 50/80 loss: 0.2771 time 2.87s\n",
            "Epoch 196/300 60/80 loss: 0.2816 time 2.87s\n",
            "Epoch 196/300 70/80 loss: 0.2929 time 3.06s\n",
            "Epoch 196/300 80/80 loss: 0.2818 time 1.61s\n",
            "Final training  196/299 loss: 0.2818 time 226.85s\n",
            "Thu Dec 18 16:52:14 2025 Epoch: 197\n",
            "Epoch 197/300 10/80 loss: 0.2746 time 3.00s\n",
            "Epoch 197/300 20/80 loss: 0.2880 time 3.07s\n",
            "Epoch 197/300 30/80 loss: 0.2969 time 2.87s\n"
          ]
        }
      ],
      "source": [
        "start_epoch = 0\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Maximum epochs: {max_epochs}\")\n",
        "print(f\"Validation frequency: every {val_every} epochs\")\n",
        "print(f\"Early stopping patience: {early_stop_patience} epochs\")\n",
        "print(f\"Early stopping minimum improvement: {early_stop_min_delta}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "(\n",
        "    val_acc_max,\n",
        "    dices_tc,\n",
        "    dices_wt,\n",
        "    dices_et,\n",
        "    dices_avg,\n",
        "    loss_epochs,\n",
        "    trains_epoch,\n",
        ") = trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_func=loss_func,\n",
        "    acc_func=dice_acc,\n",
        "    scheduler=scheduler,\n",
        "    model_inferer=model_inferer,\n",
        "    start_epoch=start_epoch,\n",
        "    post_sigmoid=post_sigmoid,\n",
        "    post_pred=post_pred,\n",
        "    scaler=scaler,\n",
        "    use_amp=use_amp,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    early_stop_patience=early_stop_patience,\n",
        "    early_stop_min_delta=early_stop_min_delta,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nuYSqcd226V",
        "outputId": "a2939ca1-1d13-479e-baa3-8fc3fc458221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Â∑≤Âä†ËΩΩÁ¨¨ 179 ËΩÆÊùÉÈáçÔºåÂáÜÂ§á‰ªéÁ¨¨ 180 ËΩÆËµ∑Ë∑ë...\n",
            "\n",
            "Thu Dec 18 19:45:52 2025 --- Epoch 180/300 ---\n",
            "  Step 5/80 | Loss: 0.1549 | Êó∂Èó¥: 228.1s\n",
            "  Step 10/80 | Loss: 0.1584 | Êó∂Èó¥: 166.3s\n",
            "  Step 15/80 | Loss: 0.4431 | Êó∂Èó¥: 192.5s\n",
            "  Step 20/80 | Loss: 0.2920 | Êó∂Èó¥: 213.3s\n",
            "  Step 25/80 | Loss: 0.0864 | Êó∂Èó¥: 189.9s\n",
            "  Step 30/80 | Loss: 0.3196 | Êó∂Èó¥: 262.2s\n",
            "  Step 35/80 | Loss: 0.3952 | Êó∂Èó¥: 143.2s\n",
            "  Step 40/80 | Loss: 0.3186 | Êó∂Èó¥: 271.4s\n",
            "  Step 45/80 | Loss: 0.1355 | Êó∂Èó¥: 254.1s\n",
            "  Step 50/80 | Loss: 0.0758 | Êó∂Èó¥: 203.5s\n",
            "  Step 55/80 | Loss: 0.3747 | Êó∂Èó¥: 199.5s\n",
            "  Step 60/80 | Loss: 0.2201 | Êó∂Èó¥: 175.2s\n",
            "  Step 65/80 | Loss: 0.1345 | Êó∂Èó¥: 198.8s\n",
            "  Step 70/80 | Loss: 0.5960 | Êó∂Èó¥: 320.4s\n",
            "  Step 75/80 | Loss: 0.1355 | Êó∂Èó¥: 374.8s\n",
            "  Step 80/80 | Loss: 0.1342 | Êó∂Èó¥: 189.7s\n",
            "üö© Epoch 180 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3138\n",
            "\n",
            "Thu Dec 18 20:45:35 2025 --- Epoch 181/300 ---\n",
            "  Step 5/80 | Loss: 0.5942 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.5086 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.6589 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.2618 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.2674 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.2941 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.3828 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.4023 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.2000 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.3654 | Êó∂Èó¥: 13.0s\n",
            "  Step 55/80 | Loss: 0.3956 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.4068 | Êó∂Èó¥: 13.2s\n",
            "  Step 65/80 | Loss: 0.1658 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.2777 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.2807 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.1588 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 181 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2851\n",
            "\n",
            "Thu Dec 18 20:49:11 2025 --- Epoch 182/300 ---\n",
            "  Step 5/80 | Loss: 0.4611 | Êó∂Èó¥: 13.0s\n",
            "  Step 10/80 | Loss: 0.0892 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.2038 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.1667 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.2933 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.0603 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.0729 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.3772 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.2726 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.2046 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.3011 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.0805 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.3633 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.5819 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.1607 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.5436 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 182 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2579\n",
            "\n",
            "Thu Dec 18 20:52:47 2025 --- Epoch 183/300 ---\n",
            "  Step 5/80 | Loss: 0.4897 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.4436 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.2947 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.4356 | Êó∂Èó¥: 12.8s\n",
            "  Step 25/80 | Loss: 0.2442 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.0787 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.2714 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.3705 | Êó∂Èó¥: 14.1s\n",
            "  Step 45/80 | Loss: 0.1725 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.3125 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.0834 | Êó∂Èó¥: 13.1s\n",
            "  Step 60/80 | Loss: 0.5020 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.3141 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.1362 | Êó∂Èó¥: 12.9s\n",
            "  Step 75/80 | Loss: 0.2714 | Êó∂Èó¥: 13.1s\n",
            "  Step 80/80 | Loss: 0.2124 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 183 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2796\n",
            "\n",
            "Thu Dec 18 20:56:23 2025 --- Epoch 184/300 ---\n",
            "  Step 5/80 | Loss: 0.3348 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.1735 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.6044 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.1743 | Êó∂Èó¥: 13.2s\n",
            "  Step 25/80 | Loss: 0.2411 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.4810 | Êó∂Èó¥: 13.4s\n",
            "  Step 35/80 | Loss: 0.1339 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.3273 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.6247 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.3790 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.2846 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.1932 | Êó∂Èó¥: 14.1s\n",
            "  Step 65/80 | Loss: 0.2642 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.0776 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.2751 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.2973 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 184 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2942\n",
            "\n",
            "Thu Dec 18 20:59:58 2025 --- Epoch 185/300 ---\n",
            "  Step 5/80 | Loss: 0.4142 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.4862 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.5467 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.4324 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.4115 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.3370 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.2214 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.2552 | Êó∂Èó¥: 14.1s\n",
            "  Step 45/80 | Loss: 0.4801 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.4719 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.1632 | Êó∂Èó¥: 13.2s\n",
            "  Step 60/80 | Loss: 0.5310 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.3197 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.4515 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.2962 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 11.7s\n",
            "üö© Epoch 185 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3325\n",
            "\n",
            "Thu Dec 18 21:03:34 2025 --- Epoch 186/300 ---\n",
            "  Step 5/80 | Loss: 0.4357 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.5974 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.4001 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.2738 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.3031 | Êó∂Èó¥: 14.1s\n",
            "  Step 30/80 | Loss: 0.4425 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.1225 | Êó∂Èó¥: 13.1s\n",
            "  Step 40/80 | Loss: 0.1604 | Êó∂Èó¥: 13.0s\n",
            "  Step 45/80 | Loss: 0.3163 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.3017 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.0517 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.1240 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.3576 | Êó∂Èó¥: 13.1s\n",
            "  Step 70/80 | Loss: 0.5805 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.3392 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.0299 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 186 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2801\n",
            "\n",
            "Thu Dec 18 21:07:09 2025 --- Epoch 187/300 ---\n",
            "  Step 5/80 | Loss: 0.4319 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.0629 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.4532 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.0811 | Êó∂Èó¥: 13.2s\n",
            "  Step 25/80 | Loss: 0.3685 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.3412 | Êó∂Èó¥: 12.9s\n",
            "  Step 35/80 | Loss: 0.1239 | Êó∂Èó¥: 13.3s\n",
            "  Step 40/80 | Loss: 0.1088 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.3082 | Êó∂Èó¥: 13.2s\n",
            "  Step 50/80 | Loss: 0.4983 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.2953 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.0621 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.4352 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.3135 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.1022 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.1118 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 187 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2741\n",
            "\n",
            "Thu Dec 18 21:10:45 2025 --- Epoch 188/300 ---\n",
            "  Step 5/80 | Loss: 0.4209 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.5413 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.4999 | Êó∂Èó¥: 13.0s\n",
            "  Step 20/80 | Loss: 0.2442 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.0623 | Êó∂Èó¥: 13.1s\n",
            "  Step 30/80 | Loss: 0.3888 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.2082 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.1256 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.0787 | Êó∂Èó¥: 13.2s\n",
            "  Step 50/80 | Loss: 0.2984 | Êó∂Èó¥: 13.3s\n",
            "  Step 55/80 | Loss: 0.2085 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.4521 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.3275 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.1027 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.1294 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.0977 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 188 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3082\n",
            "\n",
            "Thu Dec 18 21:14:20 2025 --- Epoch 189/300 ---\n",
            "  Step 5/80 | Loss: 0.6039 | Êó∂Èó¥: 14.8s\n",
            "  Step 10/80 | Loss: 0.1813 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.3489 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.4136 | Êó∂Èó¥: 13.2s\n",
            "  Step 25/80 | Loss: 0.4365 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.3739 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.4063 | Êó∂Èó¥: 12.9s\n",
            "  Step 40/80 | Loss: 0.1303 | Êó∂Èó¥: 13.1s\n",
            "  Step 45/80 | Loss: 0.6185 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.3348 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.1319 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.3285 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.1916 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.6270 | Êó∂Èó¥: 13.3s\n",
            "  Step 75/80 | Loss: 0.3444 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.0838 | Êó∂Èó¥: 11.8s\n",
            "üö© Epoch 189 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:226: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  win_data = torch.cat([inputs[win_slice] for win_slice in unravel_slice]).to(sw_device)\n",
            "/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:370: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
            "  out[idx_zm] += p\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val 189/300 0/40 , dice_tc: 0.8816999 , dice_wt: 0.9504229 , dice_et: 0.88304096 , time 45.41s\n",
            "Val 189/300 1/40 , dice_tc: 0.92298806 , dice_wt: 0.96179813 , dice_et: 0.88304096 , time 28.14s\n",
            "Val 189/300 2/40 , dice_tc: 0.93644255 , dice_wt: 0.95056534 , dice_et: 0.9229428 , time 26.31s\n",
            "Val 189/300 3/40 , dice_tc: 0.9228527 , dice_wt: 0.9401576 , dice_et: 0.91438586 , time 35.16s\n",
            "Val 189/300 4/40 , dice_tc: 0.8594373 , dice_wt: 0.8607942 , dice_et: 0.79658306 , time 32.41s\n",
            "Val 189/300 5/40 , dice_tc: 0.8462383 , dice_wt: 0.86038446 , dice_et: 0.7954371 , time 16.14s\n",
            "Val 189/300 6/40 , dice_tc: 0.8485648 , dice_wt: 0.8592156 , dice_et: 0.8077484 , time 27.66s\n",
            "Val 189/300 7/40 , dice_tc: 0.84602547 , dice_wt: 0.8583464 , dice_et: 0.81258804 , time 15.73s\n",
            "Val 189/300 8/40 , dice_tc: 0.8425243 , dice_wt: 0.8674143 , dice_et: 0.8163063 , time 31.44s\n",
            "Val 189/300 9/40 , dice_tc: 0.83513373 , dice_wt: 0.87280905 , dice_et: 0.82941145 , time 14.38s\n",
            "Val 189/300 10/40 , dice_tc: 0.8397124 , dice_wt: 0.8767372 , dice_et: 0.835282 , time 14.28s\n",
            "Val 189/300 11/40 , dice_tc: 0.84274846 , dice_wt: 0.8764463 , dice_et: 0.83895785 , time 24.95s\n",
            "Val 189/300 12/40 , dice_tc: 0.84447026 , dice_wt: 0.88162315 , dice_et: 0.83895785 , time 14.38s\n",
            "Val 189/300 13/40 , dice_tc: 0.8337165 , dice_wt: 0.8793761 , dice_et: 0.8261058 , time 14.31s\n",
            "Val 189/300 14/40 , dice_tc: 0.8321474 , dice_wt: 0.88277227 , dice_et: 0.7625592 , time 21.79s\n",
            "Val 189/300 15/40 , dice_tc: 0.83140045 , dice_wt: 0.88360673 , dice_et: 0.77211857 , time 25.03s\n",
            "Val 189/300 16/40 , dice_tc: 0.8013783 , dice_wt: 0.87992126 , dice_et: 0.7422809 , time 21.80s\n",
            "Val 189/300 17/40 , dice_tc: 0.8007818 , dice_wt: 0.8827125 , dice_et: 0.74698234 , time 14.39s\n",
            "Val 189/300 18/40 , dice_tc: 0.8076057 , dice_wt: 0.8803058 , dice_et: 0.7578409 , time 26.35s\n",
            "Val 189/300 19/40 , dice_tc: 0.80988675 , dice_wt: 0.88184273 , dice_et: 0.76337373 , time 21.90s\n",
            "Val 189/300 20/40 , dice_tc: 0.8109235 , dice_wt: 0.8839738 , dice_et: 0.7674728 , time 16.19s\n",
            "Val 189/300 21/40 , dice_tc: 0.81609654 , dice_wt: 0.886066 , dice_et: 0.7753329 , time 14.40s\n",
            "Val 189/300 22/40 , dice_tc: 0.811444 , dice_wt: 0.88900906 , dice_et: 0.7737383 , time 24.96s\n",
            "Val 189/300 23/40 , dice_tc: 0.8111885 , dice_wt: 0.8913829 , dice_et: 0.7752311 , time 24.87s\n",
            "Val 189/300 24/40 , dice_tc: 0.8111648 , dice_wt: 0.89425987 , dice_et: 0.7765266 , time 32.76s\n",
            "Val 189/300 25/40 , dice_tc: 0.80929077 , dice_wt: 0.896261 , dice_et: 0.7765266 , time 32.55s\n",
            "Val 189/300 26/40 , dice_tc: 0.81321746 , dice_wt: 0.89662445 , dice_et: 0.7823772 , time 25.40s\n",
            "Val 189/300 27/40 , dice_tc: 0.8122919 , dice_wt: 0.89657587 , dice_et: 0.7783213 , time 26.17s\n",
            "Val 189/300 28/40 , dice_tc: 0.8145283 , dice_wt: 0.8968271 , dice_et: 0.7823465 , time 21.51s\n",
            "Val 189/300 29/40 , dice_tc: 0.801433 , dice_wt: 0.8971271 , dice_et: 0.7740068 , time 22.58s\n",
            "Val 189/300 30/40 , dice_tc: 0.80296385 , dice_wt: 0.8979702 , dice_et: 0.7544888 , time 22.54s\n",
            "Val 189/300 31/40 , dice_tc: 0.80784893 , dice_wt: 0.8993676 , dice_et: 0.76162183 , time 21.59s\n",
            "Val 189/300 32/40 , dice_tc: 0.8031315 , dice_wt: 0.8977177 , dice_et: 0.76162183 , time 12.64s\n",
            "Val 189/300 33/40 , dice_tc: 0.8044902 , dice_wt: 0.89730877 , dice_et: 0.7656086 , time 21.27s\n",
            "Val 189/300 34/40 , dice_tc: 0.8072406 , dice_wt: 0.89690065 , dice_et: 0.7702955 , time 14.88s\n",
            "Val 189/300 35/40 , dice_tc: 0.79406 , dice_wt: 0.89355606 , dice_et: 0.75771767 , time 26.28s\n",
            "Val 189/300 36/40 , dice_tc: 0.7987993 , dice_wt: 0.89462703 , dice_et: 0.7641306 , time 16.25s\n",
            "Val 189/300 37/40 , dice_tc: 0.7966128 , dice_wt: 0.89599407 , dice_et: 0.7644768 , time 23.10s\n",
            "Val 189/300 38/40 , dice_tc: 0.7991575 , dice_wt: 0.8967333 , dice_et: 0.7679536 , time 25.33s\n",
            "Val 189/300 39/40 , dice_tc: 0.7899871 , dice_wt: 0.8966796 , dice_et: 0.7679536 , time 29.53s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "üåü Êñ∞Á∫™ÂΩïÔºÅDice: 0.8182\n",
            "\n",
            "Thu Dec 18 21:33:27 2025 --- Epoch 190/300 ---\n",
            "  Step 5/80 | Loss: 0.2919 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.1439 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.1154 | Êó∂Èó¥: 13.1s\n",
            "  Step 20/80 | Loss: 0.6207 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.2816 | Êó∂Èó¥: 14.1s\n",
            "  Step 30/80 | Loss: 0.4946 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.1206 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.3575 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.5938 | Êó∂Èó¥: 13.2s\n",
            "  Step 50/80 | Loss: 0.1646 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.2460 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.3760 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.4040 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.0876 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.0949 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.6217 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 190 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3022\n",
            "\n",
            "Thu Dec 18 21:37:04 2025 --- Epoch 191/300 ---\n",
            "  Step 5/80 | Loss: 0.4272 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.1207 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.2293 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.3002 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.1944 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.4916 | Êó∂Èó¥: 13.2s\n",
            "  Step 35/80 | Loss: 0.1066 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.1091 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.4017 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.5945 | Êó∂Èó¥: 14.1s\n",
            "  Step 55/80 | Loss: 0.1451 | Êó∂Èó¥: 14.2s\n",
            "  Step 60/80 | Loss: 0.3712 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.3024 | Êó∂Èó¥: 14.1s\n",
            "  Step 70/80 | Loss: 0.1729 | Êó∂Èó¥: 12.8s\n",
            "  Step 75/80 | Loss: 0.0901 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.4822 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 191 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3133\n",
            "\n",
            "Thu Dec 18 21:40:40 2025 --- Epoch 192/300 ---\n",
            "  Step 5/80 | Loss: 0.1503 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.5082 | Êó∂Èó¥: 14.2s\n",
            "  Step 15/80 | Loss: 0.4077 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2047 | Êó∂Èó¥: 13.1s\n",
            "  Step 25/80 | Loss: 0.1062 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.5122 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.3665 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.1261 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.3653 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.4360 | Êó∂Èó¥: 13.0s\n",
            "  Step 55/80 | Loss: 0.3748 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.4299 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1427 | Êó∂Èó¥: 13.2s\n",
            "  Step 70/80 | Loss: 0.2983 | Êó∂Èó¥: 13.1s\n",
            "  Step 75/80 | Loss: 0.1028 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.0581 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 192 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2992\n",
            "\n",
            "Thu Dec 18 21:44:16 2025 --- Epoch 193/300 ---\n",
            "  Step 5/80 | Loss: 0.3010 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.1851 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.3161 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.2766 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.5047 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.4362 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.2818 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.3811 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.2682 | Êó∂Èó¥: 12.9s\n",
            "  Step 50/80 | Loss: 0.2098 | Êó∂Èó¥: 13.3s\n",
            "  Step 55/80 | Loss: 0.4564 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.1827 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.4623 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.4181 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.5975 | Êó∂Èó¥: 13.1s\n",
            "  Step 80/80 | Loss: 0.1523 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 193 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3283\n",
            "\n",
            "Thu Dec 18 21:47:52 2025 --- Epoch 194/300 ---\n",
            "  Step 5/80 | Loss: 0.3722 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.5130 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.3388 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.0861 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.0527 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.2072 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.4072 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.5932 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.2022 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.3001 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.1905 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.2142 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.5128 | Êó∂Èó¥: 12.9s\n",
            "  Step 70/80 | Loss: 0.3473 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.3822 | Êó∂Èó¥: 13.2s\n",
            "  Step 80/80 | Loss: 0.1100 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 194 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2922\n",
            "\n",
            "Thu Dec 18 21:51:28 2025 --- Epoch 195/300 ---\n",
            "  Step 5/80 | Loss: 0.5507 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.0863 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.3254 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.1616 | Êó∂Èó¥: 14.1s\n",
            "  Step 25/80 | Loss: 0.2307 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.6339 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.3820 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.0522 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.4496 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.1982 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.4193 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.7000 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.4270 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.4603 | Êó∂Èó¥: 13.0s\n",
            "  Step 75/80 | Loss: 0.3124 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.1404 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 195 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2920\n",
            "\n",
            "Thu Dec 18 21:55:04 2025 --- Epoch 196/300 ---\n",
            "  Step 5/80 | Loss: 0.2806 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.1938 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.3730 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3954 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.2896 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.4157 | Êó∂Èó¥: 13.2s\n",
            "  Step 35/80 | Loss: 0.4621 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.1978 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.4718 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.4198 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.2551 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.1177 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.2733 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1712 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.4961 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.0757 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 196 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3030\n",
            "\n",
            "Thu Dec 18 21:58:40 2025 --- Epoch 197/300 ---\n",
            "  Step 5/80 | Loss: 0.0811 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.3760 | Êó∂Èó¥: 13.2s\n",
            "  Step 15/80 | Loss: 0.0552 | Êó∂Èó¥: 13.0s\n",
            "  Step 20/80 | Loss: 0.2849 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.2280 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.1642 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.3687 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.1146 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.3233 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.4207 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.2647 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.0854 | Êó∂Èó¥: 14.2s\n",
            "  Step 65/80 | Loss: 0.1388 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.4016 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.4678 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.3812 | Êó∂Èó¥: 11.8s\n",
            "üö© Epoch 197 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2803\n",
            "\n",
            "Thu Dec 18 22:02:16 2025 --- Epoch 198/300 ---\n",
            "  Step 5/80 | Loss: 0.2963 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.3733 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.1385 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.1222 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.3126 | Êó∂Èó¥: 13.2s\n",
            "  Step 30/80 | Loss: 0.1833 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.1015 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.2628 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.1004 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.5088 | Êó∂Èó¥: 13.2s\n",
            "  Step 55/80 | Loss: 0.0718 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.1575 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.1287 | Êó∂Èó¥: 14.1s\n",
            "  Step 70/80 | Loss: 0.2046 | Êó∂Èó¥: 14.2s\n",
            "  Step 75/80 | Loss: 0.2850 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.1558 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 198 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2725\n",
            "\n",
            "Thu Dec 18 22:05:53 2025 --- Epoch 199/300 ---\n",
            "  Step 5/80 | Loss: 0.3680 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.1107 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.2980 | Êó∂Èó¥: 13.1s\n",
            "  Step 20/80 | Loss: 0.3090 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.4081 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.4533 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.2074 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.3230 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.5451 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.0556 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.3634 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.4766 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1165 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.1446 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.5986 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.0997 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 199 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2937\n",
            "Val 199/300 0/40 , dice_tc: 0.8868869 , dice_wt: 0.95171905 , dice_et: 0.883853 , time 2.67s\n",
            "Val 199/300 1/40 , dice_tc: 0.92609245 , dice_wt: 0.96414626 , dice_et: 0.883853 , time 2.66s\n",
            "Val 199/300 2/40 , dice_tc: 0.93926376 , dice_wt: 0.95099026 , dice_et: 0.92442197 , time 2.67s\n",
            "Val 199/300 3/40 , dice_tc: 0.92478347 , dice_wt: 0.9380088 , dice_et: 0.914604 , time 2.66s\n",
            "Val 199/300 4/40 , dice_tc: 0.8525793 , dice_wt: 0.8572235 , dice_et: 0.7757424 , time 2.51s\n",
            "Val 199/300 5/40 , dice_tc: 0.85011214 , dice_wt: 0.8592405 , dice_et: 0.79043925 , time 2.54s\n",
            "Val 199/300 6/40 , dice_tc: 0.85432625 , dice_wt: 0.85763025 , dice_et: 0.8062312 , time 2.69s\n",
            "Val 199/300 7/40 , dice_tc: 0.8529725 , dice_wt: 0.857458 , dice_et: 0.8137743 , time 2.52s\n",
            "Val 199/300 8/40 , dice_tc: 0.8511115 , dice_wt: 0.8671038 , dice_et: 0.8187388 , time 2.73s\n",
            "Val 199/300 9/40 , dice_tc: 0.84108 , dice_wt: 0.87294084 , dice_et: 0.8333255 , time 2.59s\n",
            "Val 199/300 10/40 , dice_tc: 0.84485096 , dice_wt: 0.8768192 , dice_et: 0.83850205 , time 2.47s\n",
            "Val 199/300 11/40 , dice_tc: 0.847397 , dice_wt: 0.87515575 , dice_et: 0.84182745 , time 2.67s\n",
            "Val 199/300 12/40 , dice_tc: 0.85006976 , dice_wt: 0.880309 , dice_et: 0.84182745 , time 2.53s\n",
            "Val 199/300 13/40 , dice_tc: 0.8410216 , dice_wt: 0.8799085 , dice_et: 0.83152825 , time 2.47s\n",
            "Val 199/300 14/40 , dice_tc: 0.838004 , dice_wt: 0.8829423 , dice_et: 0.76756454 , time 2.66s\n",
            "Val 199/300 15/40 , dice_tc: 0.8369244 , dice_wt: 0.8846151 , dice_et: 0.77695835 , time 2.65s\n",
            "Val 199/300 16/40 , dice_tc: 0.8067614 , dice_wt: 0.8812919 , dice_et: 0.74671537 , time 2.66s\n",
            "Val 199/300 17/40 , dice_tc: 0.8082316 , dice_wt: 0.8840909 , dice_et: 0.753133 , time 2.44s\n",
            "Val 199/300 18/40 , dice_tc: 0.8151043 , dice_wt: 0.88104117 , dice_et: 0.76418084 , time 2.67s\n",
            "Val 199/300 19/40 , dice_tc: 0.8172151 , dice_wt: 0.8825513 , dice_et: 0.76939034 , time 2.66s\n",
            "Val 199/300 20/40 , dice_tc: 0.818971 , dice_wt: 0.8842935 , dice_et: 0.77412874 , time 2.44s\n",
            "Val 199/300 21/40 , dice_tc: 0.82384527 , dice_wt: 0.88648313 , dice_et: 0.7817306 , time 2.44s\n",
            "Val 199/300 22/40 , dice_tc: 0.8212248 , dice_wt: 0.8893562 , dice_et: 0.7818347 , time 2.68s\n",
            "Val 199/300 23/40 , dice_tc: 0.82054454 , dice_wt: 0.8918809 , dice_et: 0.78281516 , time 2.74s\n",
            "Val 199/300 24/40 , dice_tc: 0.82012117 , dice_wt: 0.894791 , dice_et: 0.78400004 , time 2.70s\n",
            "Val 199/300 25/40 , dice_tc: 0.8189272 , dice_wt: 0.8968751 , dice_et: 0.78400004 , time 2.71s\n",
            "Val 199/300 26/40 , dice_tc: 0.82242084 , dice_wt: 0.8969437 , dice_et: 0.7894431 , time 2.68s\n",
            "Val 199/300 27/40 , dice_tc: 0.8209119 , dice_wt: 0.8965138 , dice_et: 0.78499025 , time 2.69s\n",
            "Val 199/300 28/40 , dice_tc: 0.8228177 , dice_wt: 0.89630216 , dice_et: 0.7887959 , time 2.66s\n",
            "Val 199/300 29/40 , dice_tc: 0.8107433 , dice_wt: 0.8964813 , dice_et: 0.77987224 , time 2.45s\n",
            "Val 199/300 30/40 , dice_tc: 0.8122165 , dice_wt: 0.89703196 , dice_et: 0.75964504 , time 2.63s\n",
            "Val 199/300 31/40 , dice_tc: 0.8168688 , dice_wt: 0.8983948 , dice_et: 0.76662594 , time 2.61s\n",
            "Val 199/300 32/40 , dice_tc: 0.8143589 , dice_wt: 0.8962907 , dice_et: 0.76662594 , time 2.45s\n",
            "Val 199/300 33/40 , dice_tc: 0.8155716 , dice_wt: 0.89593154 , dice_et: 0.7703442 , time 2.71s\n",
            "Val 199/300 34/40 , dice_tc: 0.8179537 , dice_wt: 0.8957665 , dice_et: 0.7746765 , time 2.44s\n",
            "Val 199/300 35/40 , dice_tc: 0.80477405 , dice_wt: 0.8922179 , dice_et: 0.76256645 , time 2.60s\n",
            "Val 199/300 36/40 , dice_tc: 0.8092547 , dice_wt: 0.8934716 , dice_et: 0.7688663 , time 2.44s\n",
            "Val 199/300 37/40 , dice_tc: 0.8072949 , dice_wt: 0.8946999 , dice_et: 0.7690331 , time 2.62s\n",
            "Val 199/300 38/40 , dice_tc: 0.809323 , dice_wt: 0.8954085 , dice_et: 0.7721228 , time 2.64s\n",
            "Val 199/300 39/40 , dice_tc: 0.7992403 , dice_wt: 0.89502364 , dice_et: 0.7721228 , time 2.64s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model.pt\n",
            "üåü Êñ∞Á∫™ÂΩïÔºÅDice: 0.8221\n",
            "\n",
            "Thu Dec 18 22:11:18 2025 --- Epoch 200/300 ---\n",
            "  Step 5/80 | Loss: 0.4305 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.1865 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.5359 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.4818 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.6912 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.4009 | Êó∂Èó¥: 14.2s\n",
            "  Step 35/80 | Loss: 0.4834 | Êó∂Èó¥: 14.7s\n",
            "  Step 40/80 | Loss: 0.1506 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.0799 | Êó∂Èó¥: 13.1s\n",
            "  Step 50/80 | Loss: 0.2620 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.3657 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.3806 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.0913 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1807 | Êó∂Èó¥: 14.0s\n",
            "  Step 75/80 | Loss: 0.2343 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.3078 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 200 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3167\n",
            "\n",
            "Thu Dec 18 22:14:56 2025 --- Epoch 201/300 ---\n",
            "  Step 5/80 | Loss: 0.1692 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.1139 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.2830 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.2812 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.3328 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.2413 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.3912 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.5077 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.4167 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.2554 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.3243 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.1054 | Êó∂Èó¥: 14.2s\n",
            "  Step 65/80 | Loss: 0.0526 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.1793 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.3615 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.1572 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 201 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2846\n",
            "\n",
            "Thu Dec 18 22:18:33 2025 --- Epoch 202/300 ---\n",
            "  Step 5/80 | Loss: 0.1927 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.2648 | Êó∂Èó¥: 12.7s\n",
            "  Step 15/80 | Loss: 0.2199 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.2880 | Êó∂Èó¥: 14.2s\n",
            "  Step 25/80 | Loss: 0.2850 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.3427 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.7001 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.0645 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.2571 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.0562 | Êó∂Èó¥: 13.2s\n",
            "  Step 55/80 | Loss: 0.2232 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.4089 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.3346 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.0889 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.4431 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.1607 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 202 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2831\n",
            "\n",
            "Thu Dec 18 22:22:09 2025 --- Epoch 203/300 ---\n",
            "  Step 5/80 | Loss: 0.3737 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.1006 | Êó∂Èó¥: 13.9s\n",
            "  Step 15/80 | Loss: 0.1320 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.5825 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.4234 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.3686 | Êó∂Èó¥: 14.3s\n",
            "  Step 35/80 | Loss: 0.3909 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.4573 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.0807 | Êó∂Èó¥: 13.1s\n",
            "  Step 50/80 | Loss: 0.1373 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.3316 | Êó∂Èó¥: 14.0s\n",
            "  Step 60/80 | Loss: 0.3195 | Êó∂Èó¥: 14.2s\n",
            "  Step 65/80 | Loss: 0.1100 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.3647 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.3506 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.1079 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 203 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3164\n",
            "\n",
            "Thu Dec 18 22:25:47 2025 --- Epoch 204/300 ---\n",
            "  Step 5/80 | Loss: 0.1496 | Êó∂Èó¥: 13.9s\n",
            "  Step 10/80 | Loss: 0.4436 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.1017 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.1039 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.3245 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.4061 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.4861 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.1442 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.3451 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.5298 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.5975 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.1572 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.4896 | Êó∂Èó¥: 13.1s\n",
            "  Step 70/80 | Loss: 0.3930 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.3543 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.4444 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 204 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3181\n",
            "\n",
            "Thu Dec 18 22:29:23 2025 --- Epoch 205/300 ---\n",
            "  Step 5/80 | Loss: 0.0641 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.2767 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.1306 | Êó∂Èó¥: 14.3s\n",
            "  Step 20/80 | Loss: 0.1001 | Êó∂Èó¥: 13.2s\n",
            "  Step 25/80 | Loss: 0.1659 | Êó∂Èó¥: 14.4s\n",
            "  Step 30/80 | Loss: 0.5704 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.2078 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.0870 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.4566 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.5066 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.1839 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.1608 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.3762 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.3584 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.0691 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.4883 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 205 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3137\n",
            "\n",
            "Thu Dec 18 22:33:01 2025 --- Epoch 206/300 ---\n",
            "  Step 5/80 | Loss: 0.3402 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.5064 | Êó∂Èó¥: 13.0s\n",
            "  Step 15/80 | Loss: 0.2869 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3896 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.4551 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.2831 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.3115 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.2152 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.6296 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.7000 | Êó∂Èó¥: 14.2s\n",
            "  Step 55/80 | Loss: 0.4832 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.0881 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.5974 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.1883 | Êó∂Èó¥: 14.1s\n",
            "  Step 75/80 | Loss: 0.3010 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.0774 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 206 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2834\n",
            "\n",
            "Thu Dec 18 22:36:38 2025 --- Epoch 207/300 ---\n",
            "  Step 5/80 | Loss: 0.3528 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.4324 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.0807 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.0683 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.0544 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.2420 | Êó∂Èó¥: 14.4s\n",
            "  Step 35/80 | Loss: 0.5480 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.3017 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.3487 | Êó∂Èó¥: 14.3s\n",
            "  Step 50/80 | Loss: 0.1875 | Êó∂Èó¥: 13.1s\n",
            "  Step 55/80 | Loss: 0.2892 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.0916 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.1639 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.4187 | Êó∂Èó¥: 13.3s\n",
            "  Step 75/80 | Loss: 0.2935 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.5240 | Êó∂Èó¥: 12.8s\n",
            "üö© Epoch 207 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2891\n",
            "\n",
            "Thu Dec 18 22:40:15 2025 --- Epoch 208/300 ---\n",
            "  Step 5/80 | Loss: 0.1892 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.2862 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.0672 | Êó∂Èó¥: 14.3s\n",
            "  Step 20/80 | Loss: 0.0934 | Êó∂Èó¥: 13.2s\n",
            "  Step 25/80 | Loss: 0.3010 | Êó∂Èó¥: 13.2s\n",
            "  Step 30/80 | Loss: 0.1086 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.2455 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.0976 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.2177 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.2770 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.4919 | Êó∂Èó¥: 14.1s\n",
            "  Step 60/80 | Loss: 0.5215 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.1517 | Êó∂Èó¥: 14.3s\n",
            "  Step 70/80 | Loss: 0.2710 | Êó∂Èó¥: 13.1s\n",
            "  Step 75/80 | Loss: 0.4057 | Êó∂Èó¥: 12.8s\n",
            "  Step 80/80 | Loss: 0.0852 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 208 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2533\n",
            "\n",
            "Thu Dec 18 22:43:52 2025 --- Epoch 209/300 ---\n",
            "  Step 5/80 | Loss: 0.2471 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.2960 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.3791 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.5145 | Êó∂Èó¥: 14.2s\n",
            "  Step 25/80 | Loss: 0.3513 | Êó∂Èó¥: 13.2s\n",
            "  Step 30/80 | Loss: 0.3485 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.2692 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.4235 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.2626 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.1552 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.1368 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.3273 | Êó∂Èó¥: 13.2s\n",
            "  Step 65/80 | Loss: 0.3927 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.6260 | Êó∂Èó¥: 13.1s\n",
            "  Step 75/80 | Loss: 0.4793 | Êó∂Èó¥: 14.0s\n",
            "  Step 80/80 | Loss: 0.5402 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 209 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3114\n",
            "Val 209/300 0/40 , dice_tc: 0.8888889 , dice_wt: 0.95134574 , dice_et: 0.88541937 , time 2.71s\n",
            "Val 209/300 1/40 , dice_tc: 0.9297228 , dice_wt: 0.96420395 , dice_et: 0.88541937 , time 2.65s\n",
            "Val 209/300 2/40 , dice_tc: 0.942995 , dice_wt: 0.9490595 , dice_et: 0.92714274 , time 2.68s\n",
            "Val 209/300 3/40 , dice_tc: 0.9279257 , dice_wt: 0.93756473 , dice_et: 0.91747445 , time 2.61s\n",
            "Val 209/300 4/40 , dice_tc: 0.85962903 , dice_wt: 0.8623592 , dice_et: 0.8078566 , time 2.58s\n",
            "Val 209/300 5/40 , dice_tc: 0.84740424 , dice_wt: 0.8622984 , dice_et: 0.80560577 , time 2.52s\n",
            "Val 209/300 6/40 , dice_tc: 0.8505303 , dice_wt: 0.860651 , dice_et: 0.8171153 , time 2.67s\n",
            "Val 209/300 7/40 , dice_tc: 0.8491188 , dice_wt: 0.8600258 , dice_et: 0.8232875 , time 2.53s\n",
            "Val 209/300 8/40 , dice_tc: 0.84474546 , dice_wt: 0.8690622 , dice_et: 0.82485116 , time 2.72s\n",
            "Val 209/300 9/40 , dice_tc: 0.833773 , dice_wt: 0.87412786 , dice_et: 0.83640236 , time 2.60s\n",
            "Val 209/300 10/40 , dice_tc: 0.83842695 , dice_wt: 0.87802213 , dice_et: 0.84134847 , time 2.45s\n",
            "Val 209/300 11/40 , dice_tc: 0.8413511 , dice_wt: 0.8755243 , dice_et: 0.8442795 , time 2.66s\n",
            "Val 209/300 12/40 , dice_tc: 0.8443806 , dice_wt: 0.8807528 , dice_et: 0.8442795 , time 2.44s\n",
            "Val 209/300 13/40 , dice_tc: 0.83731073 , dice_wt: 0.88010406 , dice_et: 0.8353556 , time 2.50s\n",
            "Val 209/300 14/40 , dice_tc: 0.83665633 , dice_wt: 0.8831463 , dice_et: 0.7710975 , time 2.65s\n",
            "Val 209/300 15/40 , dice_tc: 0.8349129 , dice_wt: 0.8848349 , dice_et: 0.78054696 , time 2.65s\n",
            "Val 209/300 16/40 , dice_tc: 0.80507314 , dice_wt: 0.88181025 , dice_et: 0.75037056 , time 2.67s\n",
            "Val 209/300 17/40 , dice_tc: 0.806005 , dice_wt: 0.88454634 , dice_et: 0.7558836 , time 2.47s\n",
            "Val 209/300 18/40 , dice_tc: 0.81359595 , dice_wt: 0.8815623 , dice_et: 0.76719683 , time 2.71s\n",
            "Val 209/300 19/40 , dice_tc: 0.8156131 , dice_wt: 0.8832623 , dice_et: 0.77177805 , time 2.63s\n",
            "Val 209/300 20/40 , dice_tc: 0.81691617 , dice_wt: 0.88517123 , dice_et: 0.7757923 , time 2.42s\n",
            "Val 209/300 21/40 , dice_tc: 0.82191676 , dice_wt: 0.88728976 , dice_et: 0.7833251 , time 2.43s\n",
            "Val 209/300 22/40 , dice_tc: 0.81517375 , dice_wt: 0.89015615 , dice_et: 0.7804694 , time 2.65s\n",
            "Val 209/300 23/40 , dice_tc: 0.81529313 , dice_wt: 0.8928106 , dice_et: 0.7819216 , time 2.71s\n",
            "Val 209/300 24/40 , dice_tc: 0.81541896 , dice_wt: 0.895713 , dice_et: 0.783346 , time 2.66s\n",
            "Val 209/300 25/40 , dice_tc: 0.8131554 , dice_wt: 0.89780414 , dice_et: 0.783346 , time 2.63s\n",
            "Val 209/300 26/40 , dice_tc: 0.8168605 , dice_wt: 0.8978405 , dice_et: 0.78878945 , time 2.67s\n",
            "Val 209/300 27/40 , dice_tc: 0.8159679 , dice_wt: 0.89732736 , dice_et: 0.7848773 , time 2.67s\n",
            "Val 209/300 28/40 , dice_tc: 0.8180492 , dice_wt: 0.8971657 , dice_et: 0.78867733 , time 2.67s\n",
            "Val 209/300 29/40 , dice_tc: 0.80700296 , dice_wt: 0.8973122 , dice_et: 0.7811711 , time 2.48s\n",
            "Val 209/300 30/40 , dice_tc: 0.8082657 , dice_wt: 0.89777964 , dice_et: 0.7595565 , time 2.67s\n",
            "Val 209/300 31/40 , dice_tc: 0.8128457 , dice_wt: 0.89906806 , dice_et: 0.766459 , time 2.66s\n",
            "Val 209/300 32/40 , dice_tc: 0.80474293 , dice_wt: 0.8969156 , dice_et: 0.766459 , time 2.55s\n",
            "Val 209/300 33/40 , dice_tc: 0.806108 , dice_wt: 0.8965444 , dice_et: 0.7701132 , time 2.74s\n",
            "Val 209/300 34/40 , dice_tc: 0.8086113 , dice_wt: 0.8962914 , dice_et: 0.7742327 , time 2.47s\n",
            "Val 209/300 35/40 , dice_tc: 0.79525554 , dice_wt: 0.89283615 , dice_et: 0.7615722 , time 2.63s\n",
            "Val 209/300 36/40 , dice_tc: 0.79994476 , dice_wt: 0.8940306 , dice_et: 0.76784384 , time 2.48s\n",
            "Val 209/300 37/40 , dice_tc: 0.79825246 , dice_wt: 0.8952797 , dice_et: 0.7677425 , time 2.69s\n",
            "Val 209/300 38/40 , dice_tc: 0.8009534 , dice_wt: 0.89602447 , dice_et: 0.77120084 , time 2.63s\n",
            "Val 209/300 39/40 , dice_tc: 0.79084414 , dice_wt: 0.89590913 , dice_et: 0.77120084 , time 2.62s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Thu Dec 18 22:49:15 2025 --- Epoch 210/300 ---\n",
            "  Step 5/80 | Loss: 0.2949 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.2075 | Êó∂Èó¥: 14.2s\n",
            "  Step 15/80 | Loss: 0.3786 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.3113 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.5901 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.0857 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.2175 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.3324 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.3215 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.2251 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.3829 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.0587 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.3438 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.2367 | Êó∂Èó¥: 14.0s\n",
            "  Step 75/80 | Loss: 0.2624 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.0859 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 210 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2845\n",
            "\n",
            "Thu Dec 18 22:52:53 2025 --- Epoch 211/300 ---\n",
            "  Step 5/80 | Loss: 0.4604 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.2931 | Êó∂Èó¥: 13.9s\n",
            "  Step 15/80 | Loss: 0.3548 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.5472 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.1036 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.1032 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.3497 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.3275 | Êó∂Èó¥: 13.2s\n",
            "  Step 45/80 | Loss: 0.2418 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.2729 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.3011 | Êó∂Èó¥: 14.2s\n",
            "  Step 60/80 | Loss: 0.3727 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.6454 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.3129 | Êó∂Èó¥: 14.4s\n",
            "  Step 75/80 | Loss: 0.0600 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.2613 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 211 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2984\n",
            "\n",
            "Thu Dec 18 22:56:30 2025 --- Epoch 212/300 ---\n",
            "  Step 5/80 | Loss: 0.1120 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.4214 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.3839 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.1620 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.2923 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.1128 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.2913 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.3678 | Êó∂Èó¥: 13.1s\n",
            "  Step 45/80 | Loss: 0.1202 | Êó∂Èó¥: 14.4s\n",
            "  Step 50/80 | Loss: 0.6430 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.1727 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.5704 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1782 | Êó∂Èó¥: 13.2s\n",
            "  Step 70/80 | Loss: 0.2989 | Êó∂Èó¥: 14.2s\n",
            "  Step 75/80 | Loss: 0.7002 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.0991 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 212 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3284\n",
            "\n",
            "Thu Dec 18 23:00:07 2025 --- Epoch 213/300 ---\n",
            "  Step 5/80 | Loss: 0.4178 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.3047 | Êó∂Èó¥: 13.0s\n",
            "  Step 15/80 | Loss: 0.3897 | Êó∂Èó¥: 13.3s\n",
            "  Step 20/80 | Loss: 0.1044 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.0544 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.1279 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.3157 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.3891 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.3954 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.1459 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.1010 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.4019 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.1741 | Êó∂Èó¥: 14.1s\n",
            "  Step 70/80 | Loss: 0.1051 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.2102 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.0715 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 213 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2754\n",
            "\n",
            "Thu Dec 18 23:03:45 2025 --- Epoch 214/300 ---\n",
            "  Step 5/80 | Loss: 0.2743 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.1220 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.3443 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.0711 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.0569 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.4045 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.2697 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.1601 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.2943 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.3990 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.2966 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.3701 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.0678 | Êó∂Èó¥: 13.2s\n",
            "  Step 70/80 | Loss: 0.2146 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.0869 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.4929 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 214 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2706\n",
            "\n",
            "Thu Dec 18 23:07:22 2025 --- Epoch 215/300 ---\n",
            "  Step 5/80 | Loss: 0.0438 | Êó∂Èó¥: 13.2s\n",
            "  Step 10/80 | Loss: 0.1613 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.2885 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.1921 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.4720 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.0913 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.0827 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.4922 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.4498 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.5308 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.1693 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.1598 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.0830 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1853 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.0784 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.1182 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 215 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2627\n",
            "\n",
            "Thu Dec 18 23:10:59 2025 --- Epoch 216/300 ---\n",
            "  Step 5/80 | Loss: 0.2188 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.2619 | Êó∂Èó¥: 13.0s\n",
            "  Step 15/80 | Loss: 0.2939 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.1792 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.1165 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.4147 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.1722 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.0514 | Êó∂Èó¥: 14.2s\n",
            "  Step 45/80 | Loss: 0.3089 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.4907 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.3324 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.4689 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.3743 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.3844 | Êó∂Èó¥: 13.2s\n",
            "  Step 75/80 | Loss: 0.2635 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.1732 | Êó∂Èó¥: 12.8s\n",
            "üö© Epoch 216 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2657\n",
            "\n",
            "Thu Dec 18 23:14:36 2025 --- Epoch 217/300 ---\n",
            "  Step 5/80 | Loss: 0.2257 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.5157 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.3805 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.2863 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.1567 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.1018 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.4202 | Êó∂Èó¥: 13.3s\n",
            "  Step 40/80 | Loss: 0.5682 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.2419 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.1825 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.5000 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.0422 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.2378 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.2201 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.2188 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.0570 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 217 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2904\n",
            "\n",
            "Thu Dec 18 23:18:13 2025 --- Epoch 218/300 ---\n",
            "  Step 5/80 | Loss: 0.0966 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.1877 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.4663 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.0977 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.5580 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.2533 | Êó∂Èó¥: 14.3s\n",
            "  Step 35/80 | Loss: 0.0975 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.0875 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.4240 | Êó∂Èó¥: 13.1s\n",
            "  Step 50/80 | Loss: 0.5215 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.2191 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.0695 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.4034 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.2254 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.2326 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.1658 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 218 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2855\n",
            "\n",
            "Thu Dec 18 23:21:50 2025 --- Epoch 219/300 ---\n",
            "  Step 5/80 | Loss: 0.2945 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.0800 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.5967 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.0829 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.4070 | Êó∂Èó¥: 13.1s\n",
            "  Step 30/80 | Loss: 0.1257 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.2616 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.1185 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.1728 | Êó∂Èó¥: 14.1s\n",
            "  Step 50/80 | Loss: 0.1322 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.1109 | Êó∂Èó¥: 14.0s\n",
            "  Step 60/80 | Loss: 0.1471 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.1624 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.1800 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.0951 | Êó∂Èó¥: 14.0s\n",
            "  Step 80/80 | Loss: 0.0591 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 219 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2775\n",
            "Val 219/300 0/40 , dice_tc: 0.88904744 , dice_wt: 0.9491534 , dice_et: 0.8913912 , time 2.68s\n",
            "Val 219/300 1/40 , dice_tc: 0.9288406 , dice_wt: 0.96338236 , dice_et: 0.8913912 , time 2.67s\n",
            "Val 219/300 2/40 , dice_tc: 0.94148654 , dice_wt: 0.95049864 , dice_et: 0.9286281 , time 2.68s\n",
            "Val 219/300 3/40 , dice_tc: 0.92689085 , dice_wt: 0.9367657 , dice_et: 0.918541 , time 2.66s\n",
            "Val 219/300 4/40 , dice_tc: 0.8552414 , dice_wt: 0.8595637 , dice_et: 0.7622707 , time 2.56s\n",
            "Val 219/300 5/40 , dice_tc: 0.83981544 , dice_wt: 0.85894156 , dice_et: 0.76380336 , time 2.52s\n",
            "Val 219/300 6/40 , dice_tc: 0.8399888 , dice_wt: 0.8546133 , dice_et: 0.77786523 , time 2.67s\n",
            "Val 219/300 7/40 , dice_tc: 0.8403018 , dice_wt: 0.85527927 , dice_et: 0.78882253 , time 2.51s\n",
            "Val 219/300 8/40 , dice_tc: 0.83729535 , dice_wt: 0.86500245 , dice_et: 0.79667395 , time 2.71s\n",
            "Val 219/300 9/40 , dice_tc: 0.8257144 , dice_wt: 0.8701445 , dice_et: 0.8100293 , time 2.57s\n",
            "Val 219/300 10/40 , dice_tc: 0.83096904 , dice_wt: 0.87445873 , dice_et: 0.81746674 , time 2.44s\n",
            "Val 219/300 11/40 , dice_tc: 0.83426255 , dice_wt: 0.8722243 , dice_et: 0.8223345 , time 2.65s\n",
            "Val 219/300 12/40 , dice_tc: 0.838376 , dice_wt: 0.8777314 , dice_et: 0.8223345 , time 2.52s\n",
            "Val 219/300 13/40 , dice_tc: 0.83126295 , dice_wt: 0.878039 , dice_et: 0.8150732 , time 2.58s\n",
            "Val 219/300 14/40 , dice_tc: 0.8300506 , dice_wt: 0.8809402 , dice_et: 0.75237525 , time 2.62s\n",
            "Val 219/300 15/40 , dice_tc: 0.8302504 , dice_wt: 0.8830606 , dice_et: 0.76310575 , time 2.66s\n",
            "Val 219/300 16/40 , dice_tc: 0.7999381 , dice_wt: 0.88016737 , dice_et: 0.7333076 , time 2.64s\n",
            "Val 219/300 17/40 , dice_tc: 0.8007285 , dice_wt: 0.88308436 , dice_et: 0.739193 , time 2.44s\n",
            "Val 219/300 18/40 , dice_tc: 0.8079729 , dice_wt: 0.8803705 , dice_et: 0.75103253 , time 2.71s\n",
            "Val 219/300 19/40 , dice_tc: 0.8105528 , dice_wt: 0.88206244 , dice_et: 0.7571876 , time 2.62s\n",
            "Val 219/300 20/40 , dice_tc: 0.8123426 , dice_wt: 0.8838267 , dice_et: 0.762185 , time 2.43s\n",
            "Val 219/300 21/40 , dice_tc: 0.8175177 , dice_wt: 0.88593274 , dice_et: 0.7704442 , time 2.47s\n",
            "Val 219/300 22/40 , dice_tc: 0.8112123 , dice_wt: 0.8887123 , dice_et: 0.7674224 , time 2.67s\n",
            "Val 219/300 23/40 , dice_tc: 0.8116248 , dice_wt: 0.891438 , dice_et: 0.7696502 , time 2.71s\n",
            "Val 219/300 24/40 , dice_tc: 0.81216514 , dice_wt: 0.89434415 , dice_et: 0.77199954 , time 2.67s\n",
            "Val 219/300 25/40 , dice_tc: 0.81038964 , dice_wt: 0.8963882 , dice_et: 0.77199954 , time 2.65s\n",
            "Val 219/300 26/40 , dice_tc: 0.81426287 , dice_wt: 0.89657664 , dice_et: 0.7779481 , time 2.70s\n",
            "Val 219/300 27/40 , dice_tc: 0.8133598 , dice_wt: 0.89607954 , dice_et: 0.77414095 , time 2.74s\n",
            "Val 219/300 28/40 , dice_tc: 0.8155611 , dice_wt: 0.89586586 , dice_et: 0.77813214 , time 2.64s\n",
            "Val 219/300 29/40 , dice_tc: 0.8060867 , dice_wt: 0.8961691 , dice_et: 0.7737712 , time 2.44s\n",
            "Val 219/300 30/40 , dice_tc: 0.80752474 , dice_wt: 0.89673907 , dice_et: 0.75458324 , time 2.62s\n",
            "Val 219/300 31/40 , dice_tc: 0.8121877 , dice_wt: 0.89806855 , dice_et: 0.7616543 , time 2.60s\n",
            "Val 219/300 32/40 , dice_tc: 0.809733 , dice_wt: 0.89616936 , dice_et: 0.7616543 , time 2.50s\n",
            "Val 219/300 33/40 , dice_tc: 0.8110149 , dice_wt: 0.8957654 , dice_et: 0.7655504 , time 2.62s\n",
            "Val 219/300 34/40 , dice_tc: 0.8136481 , dice_wt: 0.8955891 , dice_et: 0.770155 , time 2.45s\n",
            "Val 219/300 35/40 , dice_tc: 0.7993793 , dice_wt: 0.8915875 , dice_et: 0.7566778 , time 2.60s\n",
            "Val 219/300 36/40 , dice_tc: 0.8039992 , dice_wt: 0.8928826 , dice_et: 0.7631434 , time 2.43s\n",
            "Val 219/300 37/40 , dice_tc: 0.8017526 , dice_wt: 0.8941094 , dice_et: 0.7633883 , time 2.67s\n",
            "Val 219/300 38/40 , dice_tc: 0.8044178 , dice_wt: 0.89470977 , dice_et: 0.7672439 , time 2.63s\n",
            "Val 219/300 39/40 , dice_tc: 0.7941889 , dice_wt: 0.8943434 , dice_et: 0.7672439 , time 2.62s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Thu Dec 18 23:27:14 2025 --- Epoch 220/300 ---\n",
            "  Step 5/80 | Loss: 0.4138 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.3810 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.7001 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.1225 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.5607 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.0588 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.2997 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.3919 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.2656 | Êó∂Èó¥: 14.1s\n",
            "  Step 50/80 | Loss: 0.4520 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.0700 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.3361 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.3884 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.0759 | Êó∂Èó¥: 13.3s\n",
            "  Step 75/80 | Loss: 0.3663 | Êó∂Èó¥: 13.0s\n",
            "  Step 80/80 | Loss: 0.1317 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 220 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2993\n",
            "\n",
            "Thu Dec 18 23:30:52 2025 --- Epoch 221/300 ---\n",
            "  Step 5/80 | Loss: 0.4496 | Êó∂Èó¥: 13.9s\n",
            "  Step 10/80 | Loss: 0.1714 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.3311 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.3096 | Êó∂Èó¥: 12.5s\n",
            "  Step 25/80 | Loss: 0.5785 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.2945 | Êó∂Èó¥: 14.5s\n",
            "  Step 35/80 | Loss: 0.0789 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.0875 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.3146 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.2039 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.2265 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.1153 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.0947 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.2917 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.0705 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.4272 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 221 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2869\n",
            "\n",
            "Thu Dec 18 23:34:28 2025 --- Epoch 222/300 ---\n",
            "  Step 5/80 | Loss: 0.4035 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.3499 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.3643 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.1715 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.3416 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.3897 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.3379 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.4335 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.2315 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.3259 | Êó∂Èó¥: 13.1s\n",
            "  Step 55/80 | Loss: 0.1031 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.3785 | Êó∂Èó¥: 14.2s\n",
            "  Step 65/80 | Loss: 0.5150 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.1370 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.2382 | Êó∂Èó¥: 13.2s\n",
            "  Step 80/80 | Loss: 0.0852 | Êó∂Èó¥: 12.8s\n",
            "üö© Epoch 222 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2628\n",
            "\n",
            "Thu Dec 18 23:38:05 2025 --- Epoch 223/300 ---\n",
            "  Step 5/80 | Loss: 0.4932 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.3939 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.5077 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.1123 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.1536 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.1986 | Êó∂Èó¥: 14.1s\n",
            "  Step 35/80 | Loss: 0.6255 | Êó∂Èó¥: 13.1s\n",
            "  Step 40/80 | Loss: 0.2696 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.3600 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.5452 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.5238 | Êó∂Èó¥: 14.1s\n",
            "  Step 60/80 | Loss: 0.5918 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.2838 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.4502 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.2816 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.1602 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 223 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3100\n",
            "\n",
            "Thu Dec 18 23:41:42 2025 --- Epoch 224/300 ---\n",
            "  Step 5/80 | Loss: 0.1802 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.1059 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.3206 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.5665 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.0975 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.3049 | Êó∂Èó¥: 13.1s\n",
            "  Step 35/80 | Loss: 0.3933 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.3174 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.4292 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.3073 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.3919 | Êó∂Èó¥: 13.1s\n",
            "  Step 60/80 | Loss: 0.6040 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1627 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.1564 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.3020 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.0646 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 224 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2687\n",
            "\n",
            "Thu Dec 18 23:45:18 2025 --- Epoch 225/300 ---\n",
            "  Step 5/80 | Loss: 0.6046 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.1138 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.1955 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3060 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.5989 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.3891 | Êó∂Èó¥: 13.1s\n",
            "  Step 35/80 | Loss: 0.2513 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.4695 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.4817 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.3531 | Êó∂Èó¥: 13.3s\n",
            "  Step 55/80 | Loss: 0.1864 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.4848 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.6247 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.1464 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.0678 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.1564 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 225 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2918\n",
            "\n",
            "Thu Dec 18 23:48:55 2025 --- Epoch 226/300 ---\n",
            "  Step 5/80 | Loss: 0.3983 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.0996 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.3447 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.1311 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.5150 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.4121 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.2945 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.3206 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.3598 | Êó∂Èó¥: 13.2s\n",
            "  Step 50/80 | Loss: 0.2711 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.4058 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.0587 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.3065 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.2172 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.0813 | Êó∂Èó¥: 13.1s\n",
            "  Step 80/80 | Loss: 0.6091 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 226 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2720\n",
            "\n",
            "Thu Dec 18 23:52:31 2025 --- Epoch 227/300 ---\n",
            "  Step 5/80 | Loss: 0.2974 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.2833 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.2301 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.3650 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.1378 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.3282 | Êó∂Èó¥: 13.0s\n",
            "  Step 35/80 | Loss: 0.0989 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.1066 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.3897 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.3085 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.1285 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.1296 | Êó∂Èó¥: 13.2s\n",
            "  Step 65/80 | Loss: 0.2339 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.0534 | Êó∂Èó¥: 14.2s\n",
            "  Step 75/80 | Loss: 0.1225 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.4485 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 227 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2779\n",
            "\n",
            "Thu Dec 18 23:56:08 2025 --- Epoch 228/300 ---\n",
            "  Step 5/80 | Loss: 0.3568 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.5145 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.0924 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2071 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.2548 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.2860 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.2610 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.2862 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.3414 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.5166 | Êó∂Èó¥: 13.2s\n",
            "  Step 55/80 | Loss: 0.5949 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.2130 | Êó∂Èó¥: 13.2s\n",
            "  Step 65/80 | Loss: 0.2598 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.4208 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.1914 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.3616 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 228 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2776\n",
            "\n",
            "Thu Dec 18 23:59:43 2025 --- Epoch 229/300 ---\n",
            "  Step 5/80 | Loss: 0.0841 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.1983 | Êó∂Èó¥: 14.4s\n",
            "  Step 15/80 | Loss: 0.5618 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.2998 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.1701 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.0701 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.0846 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.2785 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.1629 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.3128 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.0865 | Êó∂Èó¥: 13.2s\n",
            "  Step 60/80 | Loss: 0.0891 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.1575 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.5490 | Êó∂Èó¥: 13.0s\n",
            "  Step 75/80 | Loss: 0.5179 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 229 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2785\n",
            "Val 229/300 0/40 , dice_tc: 0.89354056 , dice_wt: 0.9519091 , dice_et: 0.8929202 , time 2.66s\n",
            "Val 229/300 1/40 , dice_tc: 0.9334817 , dice_wt: 0.9645562 , dice_et: 0.8929202 , time 2.66s\n",
            "Val 229/300 2/40 , dice_tc: 0.9454572 , dice_wt: 0.9471781 , dice_et: 0.93104994 , time 2.74s\n",
            "Val 229/300 3/40 , dice_tc: 0.92652243 , dice_wt: 0.9337884 , dice_et: 0.917402 , time 2.67s\n",
            "Val 229/300 4/40 , dice_tc: 0.85276 , dice_wt: 0.8560651 , dice_et: 0.76416886 , time 2.51s\n",
            "Val 229/300 5/40 , dice_tc: 0.8371186 , dice_wt: 0.8559768 , dice_et: 0.7650579 , time 2.53s\n",
            "Val 229/300 6/40 , dice_tc: 0.8390264 , dice_wt: 0.853166 , dice_et: 0.7806731 , time 2.66s\n",
            "Val 229/300 7/40 , dice_tc: 0.84084743 , dice_wt: 0.8548144 , dice_et: 0.7932316 , time 2.59s\n",
            "Val 229/300 8/40 , dice_tc: 0.83675027 , dice_wt: 0.86423993 , dice_et: 0.79726136 , time 2.72s\n",
            "Val 229/300 9/40 , dice_tc: 0.82526743 , dice_wt: 0.86936456 , dice_et: 0.81080586 , time 2.52s\n",
            "Val 229/300 10/40 , dice_tc: 0.8302756 , dice_wt: 0.87361056 , dice_et: 0.8179396 , time 2.45s\n",
            "Val 229/300 11/40 , dice_tc: 0.83357245 , dice_wt: 0.8692948 , dice_et: 0.82281995 , time 2.67s\n",
            "Val 229/300 12/40 , dice_tc: 0.8369027 , dice_wt: 0.8749269 , dice_et: 0.82281995 , time 2.51s\n",
            "Val 229/300 13/40 , dice_tc: 0.82991374 , dice_wt: 0.87503827 , dice_et: 0.81572384 , time 2.50s\n",
            "Val 229/300 14/40 , dice_tc: 0.8276816 , dice_wt: 0.8784166 , dice_et: 0.7529758 , time 2.62s\n",
            "Val 229/300 15/40 , dice_tc: 0.8274139 , dice_wt: 0.8804878 , dice_et: 0.7636317 , time 2.64s\n",
            "Val 229/300 16/40 , dice_tc: 0.79709613 , dice_wt: 0.8777217 , dice_et: 0.7335008 , time 2.67s\n",
            "Val 229/300 17/40 , dice_tc: 0.7988789 , dice_wt: 0.8807207 , dice_et: 0.7399392 , time 2.45s\n",
            "Val 229/300 18/40 , dice_tc: 0.8059215 , dice_wt: 0.8783212 , dice_et: 0.75139105 , time 2.64s\n",
            "Val 229/300 19/40 , dice_tc: 0.80857575 , dice_wt: 0.8801565 , dice_et: 0.7575736 , time 2.63s\n",
            "Val 229/300 20/40 , dice_tc: 0.8110794 , dice_wt: 0.88210857 , dice_et: 0.76311815 , time 2.43s\n",
            "Val 229/300 21/40 , dice_tc: 0.8159407 , dice_wt: 0.88454604 , dice_et: 0.77094054 , time 2.48s\n",
            "Val 229/300 22/40 , dice_tc: 0.81111383 , dice_wt: 0.887598 , dice_et: 0.7694912 , time 2.63s\n",
            "Val 229/300 23/40 , dice_tc: 0.8113787 , dice_wt: 0.8904583 , dice_et: 0.77150214 , time 2.64s\n",
            "Val 229/300 24/40 , dice_tc: 0.81111217 , dice_wt: 0.893489 , dice_et: 0.7730592 , time 2.66s\n",
            "Val 229/300 25/40 , dice_tc: 0.81085396 , dice_wt: 0.89561164 , dice_et: 0.7730592 , time 2.70s\n",
            "Val 229/300 26/40 , dice_tc: 0.8147733 , dice_wt: 0.8956295 , dice_et: 0.7790864 , time 2.71s\n",
            "Val 229/300 27/40 , dice_tc: 0.8139285 , dice_wt: 0.89500743 , dice_et: 0.77557373 , time 2.70s\n",
            "Val 229/300 28/40 , dice_tc: 0.81618094 , dice_wt: 0.8947235 , dice_et: 0.7797833 , time 2.61s\n",
            "Val 229/300 29/40 , dice_tc: 0.8081923 , dice_wt: 0.89520925 , dice_et: 0.77740604 , time 2.51s\n",
            "Val 229/300 30/40 , dice_tc: 0.8095996 , dice_wt: 0.89562726 , dice_et: 0.75786626 , time 2.67s\n",
            "Val 229/300 31/40 , dice_tc: 0.8141555 , dice_wt: 0.8969833 , dice_et: 0.76481545 , time 2.61s\n",
            "Val 229/300 32/40 , dice_tc: 0.78948414 , dice_wt: 0.89492047 , dice_et: 0.76481545 , time 2.52s\n",
            "Val 229/300 33/40 , dice_tc: 0.7913676 , dice_wt: 0.89459765 , dice_et: 0.7686531 , time 2.67s\n",
            "Val 229/300 34/40 , dice_tc: 0.7941325 , dice_wt: 0.89452773 , dice_et: 0.772804 , time 2.44s\n",
            "Val 229/300 35/40 , dice_tc: 0.7793257 , dice_wt: 0.8906114 , dice_et: 0.7583091 , time 2.72s\n",
            "Val 229/300 36/40 , dice_tc: 0.78442556 , dice_wt: 0.8919188 , dice_et: 0.7646713 , time 2.50s\n",
            "Val 229/300 37/40 , dice_tc: 0.78375006 , dice_wt: 0.89322644 , dice_et: 0.7646961 , time 2.68s\n",
            "Val 229/300 38/40 , dice_tc: 0.78652537 , dice_wt: 0.8937689 , dice_et: 0.76817155 , time 2.64s\n",
            "Val 229/300 39/40 , dice_tc: 0.7760976 , dice_wt: 0.8937417 , dice_et: 0.76817155 , time 2.64s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 00:05:06 2025 --- Epoch 230/300 ---\n",
            "  Step 5/80 | Loss: 0.2199 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.6157 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.4271 | Êó∂Èó¥: 13.0s\n",
            "  Step 20/80 | Loss: 0.3651 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.3074 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.3213 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.1008 | Êó∂Èó¥: 13.1s\n",
            "  Step 40/80 | Loss: 0.0842 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.0683 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.1978 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.3199 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.0908 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.1439 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.0617 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.4805 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.1514 | Êó∂Èó¥: 11.8s\n",
            "üö© Epoch 230 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2786\n",
            "\n",
            "Fri Dec 19 00:08:43 2025 --- Epoch 231/300 ---\n",
            "  Step 5/80 | Loss: 0.3351 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.1007 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.4330 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.0592 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.0965 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.1536 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.6641 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.3560 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.3782 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.3509 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.3434 | Êó∂Èó¥: 13.1s\n",
            "  Step 60/80 | Loss: 0.1974 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.3780 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.2718 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.3102 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.0677 | Êó∂Èó¥: 13.0s\n",
            "üö© Epoch 231 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2864\n",
            "\n",
            "Fri Dec 19 00:12:20 2025 --- Epoch 232/300 ---\n",
            "  Step 5/80 | Loss: 0.1043 | Êó∂Èó¥: 13.9s\n",
            "  Step 10/80 | Loss: 0.3828 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.0618 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.1373 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.3441 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.0503 | Êó∂Èó¥: 12.9s\n",
            "  Step 35/80 | Loss: 0.1177 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.0492 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.5601 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.3449 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.1684 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.5096 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.1614 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.2705 | Êó∂Èó¥: 14.0s\n",
            "  Step 75/80 | Loss: 0.4396 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.1664 | Êó∂Èó¥: 12.9s\n",
            "üö© Epoch 232 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2928\n",
            "\n",
            "Fri Dec 19 00:15:56 2025 --- Epoch 233/300 ---\n",
            "  Step 5/80 | Loss: 0.0543 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.3319 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.3111 | Êó∂Èó¥: 13.1s\n",
            "  Step 20/80 | Loss: 0.2896 | Êó∂Èó¥: 14.1s\n",
            "  Step 25/80 | Loss: 0.2688 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.0773 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.3506 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.3319 | Êó∂Èó¥: 13.2s\n",
            "  Step 45/80 | Loss: 0.0697 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.3866 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.2673 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.0757 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.3970 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.3701 | Êó∂Èó¥: 13.3s\n",
            "  Step 75/80 | Loss: 0.3541 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.1916 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 233 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2860\n",
            "\n",
            "Fri Dec 19 00:19:33 2025 --- Epoch 234/300 ---\n",
            "  Step 5/80 | Loss: 0.0707 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.0550 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.2296 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.3901 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.3404 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.2979 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.5938 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.5205 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.2917 | Êó∂Èó¥: 12.9s\n",
            "  Step 50/80 | Loss: 0.5986 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.2199 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.1341 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.1929 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.0720 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.2794 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.5943 | Êó∂Èó¥: 12.8s\n",
            "üö© Epoch 234 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3204\n",
            "\n",
            "Fri Dec 19 00:23:10 2025 --- Epoch 235/300 ---\n",
            "  Step 5/80 | Loss: 0.0563 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.1589 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.1553 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2829 | Êó∂Èó¥: 13.2s\n",
            "  Step 25/80 | Loss: 0.3467 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.4425 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.5671 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.1724 | Êó∂Èó¥: 14.1s\n",
            "  Step 45/80 | Loss: 0.4526 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.2840 | Êó∂Èó¥: 13.2s\n",
            "  Step 55/80 | Loss: 0.1689 | Êó∂Èó¥: 13.0s\n",
            "  Step 60/80 | Loss: 0.3516 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.2038 | Êó∂Èó¥: 13.2s\n",
            "  Step 70/80 | Loss: 0.4966 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.5110 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.3051 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 235 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2795\n",
            "\n",
            "Fri Dec 19 00:26:46 2025 --- Epoch 236/300 ---\n",
            "  Step 5/80 | Loss: 0.2728 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.1671 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.1677 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.2219 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.0885 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.5731 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.1875 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.2516 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.1249 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.0608 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.4987 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.0828 | Êó∂Èó¥: 14.2s\n",
            "  Step 65/80 | Loss: 0.3076 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.0743 | Êó∂Èó¥: 13.2s\n",
            "  Step 75/80 | Loss: 0.5109 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.1229 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 236 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2819\n",
            "\n",
            "Fri Dec 19 00:30:23 2025 --- Epoch 237/300 ---\n",
            "  Step 5/80 | Loss: 0.0794 | Êó∂Èó¥: 13.9s\n",
            "  Step 10/80 | Loss: 0.3095 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.2941 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2854 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.3162 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.3981 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.1033 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.2954 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.0855 | Êó∂Èó¥: 14.1s\n",
            "  Step 50/80 | Loss: 0.6796 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.1196 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.5811 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.2684 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.2972 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.5175 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.0378 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 237 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3375\n",
            "\n",
            "Fri Dec 19 00:33:59 2025 --- Epoch 238/300 ---\n",
            "  Step 5/80 | Loss: 0.3458 | Êó∂Èó¥: 13.2s\n",
            "  Step 10/80 | Loss: 0.2082 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.2847 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.4303 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.3106 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.3886 | Êó∂Èó¥: 13.1s\n",
            "  Step 35/80 | Loss: 0.1366 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.3357 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.0799 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.4311 | Êó∂Èó¥: 12.9s\n",
            "  Step 55/80 | Loss: 0.5123 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.2916 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.1205 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.2253 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.0928 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.3545 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 238 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2773\n",
            "\n",
            "Fri Dec 19 00:37:35 2025 --- Epoch 239/300 ---\n",
            "  Step 5/80 | Loss: 0.3226 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.3158 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.4914 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3384 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.1507 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.6015 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.0529 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.2169 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.3957 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.3914 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.3613 | Êó∂Èó¥: 13.2s\n",
            "  Step 60/80 | Loss: 0.2998 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.0707 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.0587 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.2914 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.0439 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 239 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2896\n",
            "Val 239/300 0/40 , dice_tc: 0.8877724 , dice_wt: 0.95241886 , dice_et: 0.8907206 , time 2.74s\n",
            "Val 239/300 1/40 , dice_tc: 0.92618006 , dice_wt: 0.96419317 , dice_et: 0.8907206 , time 2.67s\n",
            "Val 239/300 2/40 , dice_tc: 0.9405496 , dice_wt: 0.95069623 , dice_et: 0.9296933 , time 2.67s\n",
            "Val 239/300 3/40 , dice_tc: 0.9263848 , dice_wt: 0.93871963 , dice_et: 0.9194835 , time 2.62s\n",
            "Val 239/300 4/40 , dice_tc: 0.86139995 , dice_wt: 0.86634856 , dice_et: 0.79057074 , time 2.51s\n",
            "Val 239/300 5/40 , dice_tc: 0.8572011 , dice_wt: 0.8670895 , dice_et: 0.8013169 , time 2.54s\n",
            "Val 239/300 6/40 , dice_tc: 0.8628544 , dice_wt: 0.8673405 , dice_et: 0.81833917 , time 2.65s\n",
            "Val 239/300 7/40 , dice_tc: 0.86331344 , dice_wt: 0.86716586 , dice_et: 0.82681435 , time 2.44s\n",
            "Val 239/300 8/40 , dice_tc: 0.86007977 , dice_wt: 0.8757247 , dice_et: 0.8301489 , time 2.72s\n",
            "Val 239/300 9/40 , dice_tc: 0.85012454 , dice_wt: 0.8804804 , dice_et: 0.8431853 , time 2.60s\n",
            "Val 239/300 10/40 , dice_tc: 0.8529533 , dice_wt: 0.8836642 , dice_et: 0.84722996 , time 2.47s\n",
            "Val 239/300 11/40 , dice_tc: 0.8533024 , dice_wt: 0.8814919 , dice_et: 0.848214 , time 2.63s\n",
            "Val 239/300 12/40 , dice_tc: 0.85503775 , dice_wt: 0.88626057 , dice_et: 0.848214 , time 2.51s\n",
            "Val 239/300 13/40 , dice_tc: 0.8459315 , dice_wt: 0.8850067 , dice_et: 0.83748883 , time 2.53s\n",
            "Val 239/300 14/40 , dice_tc: 0.84448045 , dice_wt: 0.88786036 , dice_et: 0.7730666 , time 2.64s\n",
            "Val 239/300 15/40 , dice_tc: 0.8428174 , dice_wt: 0.88960737 , dice_et: 0.7825719 , time 2.66s\n",
            "Val 239/300 16/40 , dice_tc: 0.8118149 , dice_wt: 0.8862033 , dice_et: 0.7517186 , time 2.63s\n",
            "Val 239/300 17/40 , dice_tc: 0.81237525 , dice_wt: 0.8886149 , dice_et: 0.75676787 , time 2.53s\n",
            "Val 239/300 18/40 , dice_tc: 0.8198622 , dice_wt: 0.8863618 , dice_et: 0.76845455 , time 2.71s\n",
            "Val 239/300 19/40 , dice_tc: 0.8219948 , dice_wt: 0.88782203 , dice_et: 0.77381885 , time 2.72s\n",
            "Val 239/300 20/40 , dice_tc: 0.82345146 , dice_wt: 0.8894348 , dice_et: 0.77779716 , time 2.45s\n",
            "Val 239/300 21/40 , dice_tc: 0.8281708 , dice_wt: 0.8913687 , dice_et: 0.78532743 , time 2.48s\n",
            "Val 239/300 22/40 , dice_tc: 0.8220679 , dice_wt: 0.8939747 , dice_et: 0.7817504 , time 2.70s\n",
            "Val 239/300 23/40 , dice_tc: 0.8215291 , dice_wt: 0.89642686 , dice_et: 0.7829636 , time 2.76s\n",
            "Val 239/300 24/40 , dice_tc: 0.82205564 , dice_wt: 0.899249 , dice_et: 0.7853089 , time 2.65s\n",
            "Val 239/300 25/40 , dice_tc: 0.81972367 , dice_wt: 0.9011474 , dice_et: 0.7853089 , time 2.70s\n",
            "Val 239/300 26/40 , dice_tc: 0.8233312 , dice_wt: 0.90118116 , dice_et: 0.7908552 , time 2.69s\n",
            "Val 239/300 27/40 , dice_tc: 0.8220014 , dice_wt: 0.90062016 , dice_et: 0.7864968 , time 2.69s\n",
            "Val 239/300 28/40 , dice_tc: 0.8236738 , dice_wt: 0.9007223 , dice_et: 0.7897959 , time 2.64s\n",
            "Val 239/300 29/40 , dice_tc: 0.8102714 , dice_wt: 0.9008648 , dice_et: 0.77702105 , time 2.50s\n",
            "Val 239/300 30/40 , dice_tc: 0.81158406 , dice_wt: 0.9012522 , dice_et: 0.75906074 , time 2.64s\n",
            "Val 239/300 31/40 , dice_tc: 0.81636375 , dice_wt: 0.9025049 , dice_et: 0.7661596 , time 2.67s\n",
            "Val 239/300 32/40 , dice_tc: 0.79162544 , dice_wt: 0.89949375 , dice_et: 0.7661596 , time 2.51s\n",
            "Val 239/300 33/40 , dice_tc: 0.7932916 , dice_wt: 0.8992171 , dice_et: 0.7700116 , time 2.68s\n",
            "Val 239/300 34/40 , dice_tc: 0.79636705 , dice_wt: 0.898993 , dice_et: 0.7744146 , time 2.45s\n",
            "Val 239/300 35/40 , dice_tc: 0.7847918 , dice_wt: 0.89577854 , dice_et: 0.7635029 , time 2.66s\n",
            "Val 239/300 36/40 , dice_tc: 0.78975505 , dice_wt: 0.8969444 , dice_et: 0.7696966 , time 2.47s\n",
            "Val 239/300 37/40 , dice_tc: 0.7883995 , dice_wt: 0.8981554 , dice_et: 0.76961356 , time 2.69s\n",
            "Val 239/300 38/40 , dice_tc: 0.7913773 , dice_wt: 0.89849687 , dice_et: 0.77329206 , time 2.62s\n",
            "Val 239/300 39/40 , dice_tc: 0.78214586 , dice_wt: 0.89838207 , dice_et: 0.77329206 , time 2.65s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 00:42:58 2025 --- Epoch 240/300 ---\n",
            "  Step 5/80 | Loss: 0.3972 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.3613 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.0458 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.2993 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.0883 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.0871 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.4044 | Êó∂Èó¥: 13.3s\n",
            "  Step 40/80 | Loss: 0.2487 | Êó∂Èó¥: 14.2s\n",
            "  Step 45/80 | Loss: 0.2758 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.3720 | Êó∂Èó¥: 13.0s\n",
            "  Step 55/80 | Loss: 0.3211 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.4907 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.4395 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.2012 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.4505 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.0522 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 240 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2896\n",
            "\n",
            "Fri Dec 19 00:46:36 2025 --- Epoch 241/300 ---\n",
            "  Step 5/80 | Loss: 0.1060 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.1815 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.4925 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.1568 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.1912 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.2854 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.1877 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.0814 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.3180 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.1581 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.1726 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.0740 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.3721 | Êó∂Èó¥: 13.1s\n",
            "  Step 70/80 | Loss: 0.5414 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.0708 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.0420 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 241 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3028\n",
            "\n",
            "Fri Dec 19 00:50:12 2025 --- Epoch 242/300 ---\n",
            "  Step 5/80 | Loss: 0.3853 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.2739 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.2541 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3015 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.3814 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.2780 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.1507 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.3966 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.0760 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.1934 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.0922 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.6778 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.1687 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.1490 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.6000 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.4739 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 242 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2908\n",
            "\n",
            "Fri Dec 19 00:53:48 2025 --- Epoch 243/300 ---\n",
            "  Step 5/80 | Loss: 0.5698 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.1053 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.5574 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.2736 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.1186 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.0749 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.1370 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.5020 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.3255 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.4763 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.6096 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.1978 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.2997 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.3747 | Êó∂Èó¥: 13.3s\n",
            "  Step 75/80 | Loss: 0.0637 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.6163 | Êó∂Èó¥: 11.9s\n",
            "üö© Epoch 243 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2880\n",
            "\n",
            "Fri Dec 19 00:57:25 2025 --- Epoch 244/300 ---\n",
            "  Step 5/80 | Loss: 0.2056 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.3321 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.3012 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.1441 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.0817 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.3803 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.1514 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.4354 | Êó∂Èó¥: 13.1s\n",
            "  Step 45/80 | Loss: 0.3881 | Êó∂Èó¥: 14.0s\n",
            "  Step 50/80 | Loss: 0.3960 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.3075 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.0773 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.5085 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.2941 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.5660 | Êó∂Èó¥: 14.0s\n",
            "  Step 80/80 | Loss: 0.1085 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 244 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2747\n",
            "\n",
            "Fri Dec 19 01:01:01 2025 --- Epoch 245/300 ---\n",
            "  Step 5/80 | Loss: 0.2993 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.2632 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.2757 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.3834 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.1482 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.7000 | Êó∂Èó¥: 14.1s\n",
            "  Step 35/80 | Loss: 0.6600 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.1827 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.4882 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.5596 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.3221 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.2969 | Êó∂Èó¥: 12.7s\n",
            "  Step 65/80 | Loss: 0.3549 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.3464 | Êó∂Èó¥: 14.1s\n",
            "  Step 75/80 | Loss: 0.4211 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.0487 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 245 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2898\n",
            "\n",
            "Fri Dec 19 01:04:38 2025 --- Epoch 246/300 ---\n",
            "  Step 5/80 | Loss: 0.0881 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.3823 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.5668 | Êó∂Èó¥: 13.0s\n",
            "  Step 20/80 | Loss: 0.0828 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.5858 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.2688 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.2769 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.3215 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.0814 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.1229 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.3128 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.3788 | Êó∂Èó¥: 13.2s\n",
            "  Step 65/80 | Loss: 0.4013 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.4914 | Êó∂Èó¥: 14.1s\n",
            "  Step 75/80 | Loss: 0.1239 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.4008 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 246 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3010\n",
            "\n",
            "Fri Dec 19 01:08:15 2025 --- Epoch 247/300 ---\n",
            "  Step 5/80 | Loss: 0.4046 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.2811 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.4051 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.2968 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.2683 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.2239 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.1504 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.3033 | Êó∂Èó¥: 13.0s\n",
            "  Step 45/80 | Loss: 0.6022 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.2307 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.2723 | Êó∂Èó¥: 14.0s\n",
            "  Step 60/80 | Loss: 0.0581 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.0547 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.3856 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.0930 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 247 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3088\n",
            "\n",
            "Fri Dec 19 01:11:51 2025 --- Epoch 248/300 ---\n",
            "  Step 5/80 | Loss: 0.1296 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.2678 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.3952 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.1868 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.0904 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.0748 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.1624 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.0664 | Êó∂Èó¥: 14.2s\n",
            "  Step 45/80 | Loss: 0.4140 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.5586 | Êó∂Èó¥: 14.1s\n",
            "  Step 55/80 | Loss: 0.6004 | Êó∂Èó¥: 12.8s\n",
            "  Step 60/80 | Loss: 0.1957 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.3720 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1547 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.3250 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.2085 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 248 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2994\n",
            "\n",
            "Fri Dec 19 01:15:28 2025 --- Epoch 249/300 ---\n",
            "  Step 5/80 | Loss: 0.3696 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.2984 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.1550 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.3478 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.1891 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.0868 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.4430 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.4054 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.1024 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.0688 | Êó∂Èó¥: 14.1s\n",
            "  Step 55/80 | Loss: 0.5167 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.4467 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.2173 | Êó∂Èó¥: 13.2s\n",
            "  Step 70/80 | Loss: 0.4878 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.5034 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.3733 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 249 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3085\n",
            "Val 249/300 0/40 , dice_tc: 0.88855135 , dice_wt: 0.9508575 , dice_et: 0.88721806 , time 2.60s\n",
            "Val 249/300 1/40 , dice_tc: 0.9309585 , dice_wt: 0.9644488 , dice_et: 0.88721806 , time 2.69s\n",
            "Val 249/300 2/40 , dice_tc: 0.9436521 , dice_wt: 0.9461379 , dice_et: 0.92794216 , time 2.69s\n",
            "Val 249/300 3/40 , dice_tc: 0.92558753 , dice_wt: 0.93177617 , dice_et: 0.91545814 , time 2.63s\n",
            "Val 249/300 4/40 , dice_tc: 0.8485829 , dice_wt: 0.8552725 , dice_et: 0.7720743 , time 2.47s\n",
            "Val 249/300 5/40 , dice_tc: 0.8324178 , dice_wt: 0.8552076 , dice_et: 0.7698412 , time 2.50s\n",
            "Val 249/300 6/40 , dice_tc: 0.8338122 , dice_wt: 0.8512063 , dice_et: 0.7833796 , time 2.73s\n",
            "Val 249/300 7/40 , dice_tc: 0.83854085 , dice_wt: 0.85133266 , dice_et: 0.7977924 , time 2.53s\n",
            "Val 249/300 8/40 , dice_tc: 0.8362996 , dice_wt: 0.86125875 , dice_et: 0.80279213 , time 2.72s\n",
            "Val 249/300 9/40 , dice_tc: 0.8261239 , dice_wt: 0.86706847 , dice_et: 0.81733567 , time 2.51s\n",
            "Val 249/300 10/40 , dice_tc: 0.8314798 , dice_wt: 0.8716576 , dice_et: 0.8240944 , time 2.43s\n",
            "Val 249/300 11/40 , dice_tc: 0.83405924 , dice_wt: 0.8670364 , dice_et: 0.8278949 , time 2.70s\n",
            "Val 249/300 12/40 , dice_tc: 0.835024 , dice_wt: 0.8726849 , dice_et: 0.8278949 , time 2.49s\n",
            "Val 249/300 13/40 , dice_tc: 0.82776266 , dice_wt: 0.87311697 , dice_et: 0.8198889 , time 2.52s\n",
            "Val 249/300 14/40 , dice_tc: 0.82884175 , dice_wt: 0.87647897 , dice_et: 0.75682056 , time 2.61s\n",
            "Val 249/300 15/40 , dice_tc: 0.8293536 , dice_wt: 0.8790577 , dice_et: 0.7676216 , time 2.63s\n",
            "Val 249/300 16/40 , dice_tc: 0.79899734 , dice_wt: 0.87636554 , dice_et: 0.7371567 , time 2.68s\n",
            "Val 249/300 17/40 , dice_tc: 0.80196804 , dice_wt: 0.8794439 , dice_et: 0.7442347 , time 2.50s\n",
            "Val 249/300 18/40 , dice_tc: 0.80988324 , dice_wt: 0.8777337 , dice_et: 0.7563787 , time 2.64s\n",
            "Val 249/300 19/40 , dice_tc: 0.81234705 , dice_wt: 0.8795506 , dice_et: 0.762223 , time 2.63s\n",
            "Val 249/300 20/40 , dice_tc: 0.8147462 , dice_wt: 0.8811103 , dice_et: 0.76753974 , time 2.50s\n",
            "Val 249/300 21/40 , dice_tc: 0.8196255 , dice_wt: 0.8838577 , dice_et: 0.77526295 , time 2.48s\n",
            "Val 249/300 22/40 , dice_tc: 0.8136036 , dice_wt: 0.8868319 , dice_et: 0.772709 , time 2.70s\n",
            "Val 249/300 23/40 , dice_tc: 0.81357074 , dice_wt: 0.8896863 , dice_et: 0.77421933 , time 2.70s\n",
            "Val 249/300 24/40 , dice_tc: 0.81460345 , dice_wt: 0.892747 , dice_et: 0.7769784 , time 2.68s\n",
            "Val 249/300 25/40 , dice_tc: 0.81270194 , dice_wt: 0.89494985 , dice_et: 0.7769784 , time 2.76s\n",
            "Val 249/300 26/40 , dice_tc: 0.81655276 , dice_wt: 0.894711 , dice_et: 0.7829061 , time 2.71s\n",
            "Val 249/300 27/40 , dice_tc: 0.8157119 , dice_wt: 0.89410925 , dice_et: 0.7793001 , time 2.67s\n",
            "Val 249/300 28/40 , dice_tc: 0.81771487 , dice_wt: 0.8938911 , dice_et: 0.7831574 , time 2.60s\n",
            "Val 249/300 29/40 , dice_tc: 0.80839884 , dice_wt: 0.8942436 , dice_et: 0.7730119 , time 2.53s\n",
            "Val 249/300 30/40 , dice_tc: 0.80868316 , dice_wt: 0.8944887 , dice_et: 0.7512604 , time 2.66s\n",
            "Val 249/300 31/40 , dice_tc: 0.8134097 , dice_wt: 0.8960067 , dice_et: 0.75849 , time 2.63s\n",
            "Val 249/300 32/40 , dice_tc: 0.7887609 , dice_wt: 0.89207613 , dice_et: 0.75849 , time 2.47s\n",
            "Val 249/300 33/40 , dice_tc: 0.7908854 , dice_wt: 0.89190686 , dice_et: 0.7623872 , time 2.63s\n",
            "Val 249/300 34/40 , dice_tc: 0.79362583 , dice_wt: 0.8918381 , dice_et: 0.7665688 , time 2.50s\n",
            "Val 249/300 35/40 , dice_tc: 0.7788374 , dice_wt: 0.8880199 , dice_et: 0.7519409 , time 2.63s\n",
            "Val 249/300 36/40 , dice_tc: 0.78399605 , dice_wt: 0.88943934 , dice_et: 0.75853866 , time 2.46s\n",
            "Val 249/300 37/40 , dice_tc: 0.7831776 , dice_wt: 0.89073354 , dice_et: 0.7587511 , time 2.62s\n",
            "Val 249/300 38/40 , dice_tc: 0.78612065 , dice_wt: 0.8913727 , dice_et: 0.7624326 , time 2.63s\n",
            "Val 249/300 39/40 , dice_tc: 0.7730466 , dice_wt: 0.8912152 , dice_et: 0.7624326 , time 2.69s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 01:20:51 2025 --- Epoch 250/300 ---\n",
            "  Step 5/80 | Loss: 0.7000 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.2713 | Êó∂Èó¥: 14.3s\n",
            "  Step 15/80 | Loss: 0.4885 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.3899 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.3818 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.0714 | Êó∂Èó¥: 13.2s\n",
            "  Step 35/80 | Loss: 0.0585 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.3715 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.3411 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.4614 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.3741 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.5086 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.0950 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.2886 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.6023 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.5718 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 250 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3113\n",
            "\n",
            "Fri Dec 19 01:24:28 2025 --- Epoch 251/300 ---\n",
            "  Step 5/80 | Loss: 0.2695 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.3888 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.4020 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.5984 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.2512 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.5990 | Êó∂Èó¥: 14.1s\n",
            "  Step 35/80 | Loss: 0.6089 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.3070 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.2784 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.3156 | Êó∂Èó¥: 14.1s\n",
            "  Step 55/80 | Loss: 0.4073 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.3747 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.0771 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.1603 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.4882 | Êó∂Èó¥: 13.0s\n",
            "  Step 80/80 | Loss: 0.4745 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 251 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3009\n",
            "\n",
            "Fri Dec 19 01:28:05 2025 --- Epoch 252/300 ---\n",
            "  Step 5/80 | Loss: 0.3227 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.0811 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.1661 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3817 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.4606 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.3752 | Êó∂Èó¥: 13.4s\n",
            "  Step 35/80 | Loss: 0.3133 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.0728 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.0930 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.4745 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.5319 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.1377 | Êó∂Èó¥: 14.1s\n",
            "  Step 65/80 | Loss: 0.2284 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.2776 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.1646 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.0449 | Êó∂Èó¥: 11.9s\n",
            "üö© Epoch 252 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2562\n",
            "\n",
            "Fri Dec 19 01:31:41 2025 --- Epoch 253/300 ---\n",
            "  Step 5/80 | Loss: 0.4240 | Êó∂Èó¥: 13.2s\n",
            "  Step 10/80 | Loss: 0.3795 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.0806 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2244 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.3288 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.2990 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.1146 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.2386 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.4634 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.2953 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.5062 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.4958 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.1722 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.0612 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.2576 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.3215 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 253 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2919\n",
            "\n",
            "Fri Dec 19 01:35:19 2025 --- Epoch 254/300 ---\n",
            "  Step 5/80 | Loss: 0.3866 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.2822 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.2664 | Êó∂Èó¥: 13.3s\n",
            "  Step 20/80 | Loss: 0.2853 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.3153 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.2687 | Êó∂Èó¥: 14.4s\n",
            "  Step 35/80 | Loss: 0.4161 | Êó∂Èó¥: 13.3s\n",
            "  Step 40/80 | Loss: 0.2738 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.3319 | Êó∂Èó¥: 14.1s\n",
            "  Step 50/80 | Loss: 0.2927 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.3016 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.4445 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.3173 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.3074 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.4188 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.0384 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 254 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3075\n",
            "\n",
            "Fri Dec 19 01:38:57 2025 --- Epoch 255/300 ---\n",
            "  Step 5/80 | Loss: 0.1453 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.1661 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.2742 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.3993 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.1087 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.0909 | Êó∂Èó¥: 13.2s\n",
            "  Step 35/80 | Loss: 0.4134 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.1973 | Êó∂Èó¥: 14.1s\n",
            "  Step 45/80 | Loss: 0.4327 | Êó∂Èó¥: 14.0s\n",
            "  Step 50/80 | Loss: 0.2888 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.3422 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.0705 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.3175 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.1885 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.3883 | Êó∂Èó¥: 14.5s\n",
            "  Step 80/80 | Loss: 0.2854 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 255 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2641\n",
            "\n",
            "Fri Dec 19 01:42:36 2025 --- Epoch 256/300 ---\n",
            "  Step 5/80 | Loss: 0.0774 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.5884 | Êó∂Èó¥: 13.2s\n",
            "  Step 15/80 | Loss: 0.4480 | Êó∂Èó¥: 14.5s\n",
            "  Step 20/80 | Loss: 0.3769 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.1448 | Êó∂Èó¥: 14.4s\n",
            "  Step 30/80 | Loss: 0.2984 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.2103 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.1669 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.1223 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.5925 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.5454 | Êó∂Èó¥: 14.0s\n",
            "  Step 60/80 | Loss: 0.1081 | Êó∂Èó¥: 14.3s\n",
            "  Step 65/80 | Loss: 0.4191 | Êó∂Èó¥: 13.2s\n",
            "  Step 70/80 | Loss: 0.0515 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.1651 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.1330 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 256 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2679\n",
            "\n",
            "Fri Dec 19 01:46:15 2025 --- Epoch 257/300 ---\n",
            "  Step 5/80 | Loss: 0.1084 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.4075 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.1814 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.3703 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.2386 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.2695 | Êó∂Èó¥: 14.2s\n",
            "  Step 35/80 | Loss: 0.1929 | Êó∂Èó¥: 14.3s\n",
            "  Step 40/80 | Loss: 0.1654 | Êó∂Èó¥: 14.4s\n",
            "  Step 45/80 | Loss: 0.5458 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.3753 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.1056 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.0870 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.4338 | Êó∂Èó¥: 14.1s\n",
            "  Step 70/80 | Loss: 0.1526 | Êó∂Èó¥: 13.2s\n",
            "  Step 75/80 | Loss: 0.5533 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.4144 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 257 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3053\n",
            "\n",
            "Fri Dec 19 01:49:55 2025 --- Epoch 258/300 ---\n",
            "  Step 5/80 | Loss: 0.3912 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.0651 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.1229 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.6061 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.1505 | Êó∂Èó¥: 14.4s\n",
            "  Step 30/80 | Loss: 0.4812 | Êó∂Èó¥: 14.3s\n",
            "  Step 35/80 | Loss: 0.1047 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.1105 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.2868 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.4877 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.7003 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.1932 | Êó∂Èó¥: 14.1s\n",
            "  Step 65/80 | Loss: 0.3020 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.4591 | Êó∂Èó¥: 14.3s\n",
            "  Step 75/80 | Loss: 0.1353 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.4519 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 258 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2783\n",
            "\n",
            "Fri Dec 19 01:53:34 2025 --- Epoch 259/300 ---\n",
            "  Step 5/80 | Loss: 0.5693 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.1796 | Êó∂Èó¥: 14.1s\n",
            "  Step 15/80 | Loss: 0.2964 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.2602 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.3228 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.2039 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.3682 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.3205 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.1052 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.4214 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.0889 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.0589 | Êó∂Èó¥: 14.6s\n",
            "  Step 65/80 | Loss: 0.2882 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.0744 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.1436 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.3397 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 259 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2599\n",
            "Val 259/300 0/40 , dice_tc: 0.8960505 , dice_wt: 0.95481694 , dice_et: 0.8965799 , time 2.77s\n",
            "Val 259/300 1/40 , dice_tc: 0.9322637 , dice_wt: 0.9654083 , dice_et: 0.8965799 , time 2.69s\n",
            "Val 259/300 2/40 , dice_tc: 0.944261 , dice_wt: 0.9531307 , dice_et: 0.93200004 , time 2.69s\n",
            "Val 259/300 3/40 , dice_tc: 0.9309997 , dice_wt: 0.94244826 , dice_et: 0.9231414 , time 2.68s\n",
            "Val 259/300 4/40 , dice_tc: 0.8641003 , dice_wt: 0.86706334 , dice_et: 0.7835277 , time 2.60s\n",
            "Val 259/300 5/40 , dice_tc: 0.8516562 , dice_wt: 0.866201 , dice_et: 0.7866615 , time 2.54s\n",
            "Val 259/300 6/40 , dice_tc: 0.8519568 , dice_wt: 0.86461866 , dice_et: 0.79921275 , time 2.69s\n",
            "Val 259/300 7/40 , dice_tc: 0.85172385 , dice_wt: 0.8641817 , dice_et: 0.80819225 , time 2.54s\n",
            "Val 259/300 8/40 , dice_tc: 0.8489171 , dice_wt: 0.87298375 , dice_et: 0.8145219 , time 2.74s\n",
            "Val 259/300 9/40 , dice_tc: 0.8408421 , dice_wt: 0.87774265 , dice_et: 0.8286979 , time 2.64s\n",
            "Val 259/300 10/40 , dice_tc: 0.8448353 , dice_wt: 0.8814172 , dice_et: 0.83450365 , time 2.47s\n",
            "Val 259/300 11/40 , dice_tc: 0.8468531 , dice_wt: 0.8801544 , dice_et: 0.8374167 , time 2.65s\n",
            "Val 259/300 12/40 , dice_tc: 0.8494889 , dice_wt: 0.8850577 , dice_et: 0.8374167 , time 2.48s\n",
            "Val 259/300 13/40 , dice_tc: 0.84063685 , dice_wt: 0.8835999 , dice_et: 0.8272628 , time 2.52s\n",
            "Val 259/300 14/40 , dice_tc: 0.84082955 , dice_wt: 0.886712 , dice_et: 0.7636272 , time 2.63s\n",
            "Val 259/300 15/40 , dice_tc: 0.83997226 , dice_wt: 0.88823915 , dice_et: 0.7739072 , time 2.69s\n",
            "Val 259/300 16/40 , dice_tc: 0.80889004 , dice_wt: 0.88466287 , dice_et: 0.7431834 , time 2.71s\n",
            "Val 259/300 17/40 , dice_tc: 0.80926055 , dice_wt: 0.88708246 , dice_et: 0.74834996 , time 2.53s\n",
            "Val 259/300 18/40 , dice_tc: 0.8172304 , dice_wt: 0.88631105 , dice_et: 0.7607487 , time 2.79s\n",
            "Val 259/300 19/40 , dice_tc: 0.8195473 , dice_wt: 0.88795453 , dice_et: 0.7667091 , time 2.69s\n",
            "Val 259/300 20/40 , dice_tc: 0.82049096 , dice_wt: 0.88959795 , dice_et: 0.7706567 , time 2.45s\n",
            "Val 259/300 21/40 , dice_tc: 0.82550246 , dice_wt: 0.89134526 , dice_et: 0.7786913 , time 2.50s\n",
            "Val 259/300 22/40 , dice_tc: 0.81830895 , dice_wt: 0.89420104 , dice_et: 0.7753457 , time 2.68s\n",
            "Val 259/300 23/40 , dice_tc: 0.8184685 , dice_wt: 0.8965426 , dice_et: 0.7771456 , time 2.73s\n",
            "Val 259/300 24/40 , dice_tc: 0.8188253 , dice_wt: 0.89929444 , dice_et: 0.7791123 , time 2.66s\n",
            "Val 259/300 25/40 , dice_tc: 0.81463253 , dice_wt: 0.9011872 , dice_et: 0.7791123 , time 2.70s\n",
            "Val 259/300 26/40 , dice_tc: 0.81827784 , dice_wt: 0.9013112 , dice_et: 0.7846822 , time 2.67s\n",
            "Val 259/300 27/40 , dice_tc: 0.8174604 , dice_wt: 0.9011016 , dice_et: 0.7805789 , time 2.78s\n",
            "Val 259/300 28/40 , dice_tc: 0.81951296 , dice_wt: 0.9015572 , dice_et: 0.78436047 , time 2.62s\n",
            "Val 259/300 29/40 , dice_tc: 0.806069 , dice_wt: 0.90173346 , dice_et: 0.7730241 , time 2.55s\n",
            "Val 259/300 30/40 , dice_tc: 0.8074841 , dice_wt: 0.90242624 , dice_et: 0.75302905 , time 2.66s\n",
            "Val 259/300 31/40 , dice_tc: 0.8124013 , dice_wt: 0.903587 , dice_et: 0.76031554 , time 2.66s\n",
            "Val 259/300 32/40 , dice_tc: 0.7877831 , dice_wt: 0.90081114 , dice_et: 0.76031554 , time 2.61s\n",
            "Val 259/300 33/40 , dice_tc: 0.78942347 , dice_wt: 0.90051806 , dice_et: 0.7640637 , time 2.71s\n",
            "Val 259/300 34/40 , dice_tc: 0.79281265 , dice_wt: 0.9002463 , dice_et: 0.7688083 , time 2.45s\n",
            "Val 259/300 35/40 , dice_tc: 0.7802564 , dice_wt: 0.8972542 , dice_et: 0.7567411 , time 2.69s\n",
            "Val 259/300 36/40 , dice_tc: 0.78539425 , dice_wt: 0.8984037 , dice_et: 0.7631936 , time 2.53s\n",
            "Val 259/300 37/40 , dice_tc: 0.7842328 , dice_wt: 0.8996561 , dice_et: 0.7634716 , time 2.70s\n",
            "Val 259/300 38/40 , dice_tc: 0.7875469 , dice_wt: 0.90039796 , dice_et: 0.7675386 , time 2.66s\n",
            "Val 259/300 39/40 , dice_tc: 0.7777353 , dice_wt: 0.900431 , dice_et: 0.7675386 , time 2.67s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 01:59:02 2025 --- Epoch 260/300 ---\n",
            "  Step 5/80 | Loss: 0.2474 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.4093 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.2351 | Êó∂Èó¥: 14.3s\n",
            "  Step 20/80 | Loss: 0.1617 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.2034 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.3621 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.1695 | Êó∂Èó¥: 14.2s\n",
            "  Step 40/80 | Loss: 0.2230 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.5900 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.5199 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.3104 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.4309 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.3834 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.1886 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.1902 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.1297 | Êó∂Èó¥: 12.8s\n",
            "üö© Epoch 260 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2878\n",
            "\n",
            "Fri Dec 19 02:02:44 2025 --- Epoch 261/300 ---\n",
            "  Step 5/80 | Loss: 0.3293 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.0853 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.3953 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.0417 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.2776 | Êó∂Èó¥: 14.1s\n",
            "  Step 30/80 | Loss: 0.1888 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.2945 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.3043 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.0579 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.1109 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.4330 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.2956 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.2999 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.0790 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.3136 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.7004 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 261 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3162\n",
            "\n",
            "Fri Dec 19 02:06:23 2025 --- Epoch 262/300 ---\n",
            "  Step 5/80 | Loss: 0.3467 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.3942 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.5309 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3985 | Êó∂Èó¥: 14.3s\n",
            "  Step 25/80 | Loss: 0.1919 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.0580 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.5428 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.4196 | Êó∂Èó¥: 13.2s\n",
            "  Step 45/80 | Loss: 0.2773 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.3642 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.1995 | Êó∂Èó¥: 14.4s\n",
            "  Step 60/80 | Loss: 0.4495 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.0435 | Êó∂Èó¥: 13.1s\n",
            "  Step 70/80 | Loss: 0.3096 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.5103 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.0388 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 262 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2809\n",
            "\n",
            "Fri Dec 19 02:10:02 2025 --- Epoch 263/300 ---\n",
            "  Step 5/80 | Loss: 0.3699 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.5057 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.4212 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.6751 | Êó∂Èó¥: 13.1s\n",
            "  Step 25/80 | Loss: 0.5882 | Êó∂Èó¥: 14.4s\n",
            "  Step 30/80 | Loss: 0.3785 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.3951 | Êó∂Èó¥: 13.3s\n",
            "  Step 40/80 | Loss: 0.3713 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.2667 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.3717 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.0702 | Êó∂Èó¥: 14.3s\n",
            "  Step 60/80 | Loss: 0.2477 | Êó∂Èó¥: 14.3s\n",
            "  Step 65/80 | Loss: 0.6072 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.1157 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.1612 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.1750 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 263 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2715\n",
            "\n",
            "Fri Dec 19 02:13:41 2025 --- Epoch 264/300 ---\n",
            "  Step 5/80 | Loss: 0.4936 | Êó∂Èó¥: 14.2s\n",
            "  Step 10/80 | Loss: 0.2134 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.2100 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.0841 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.0995 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.2811 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.1003 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.2805 | Êó∂Èó¥: 14.3s\n",
            "  Step 45/80 | Loss: 0.3431 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.3811 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.5171 | Êó∂Èó¥: 14.2s\n",
            "  Step 60/80 | Loss: 0.5396 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.2341 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.2652 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.3774 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.1192 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 264 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2752\n",
            "\n",
            "Fri Dec 19 02:17:21 2025 --- Epoch 265/300 ---\n",
            "  Step 5/80 | Loss: 0.3065 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.3034 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.5639 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.0725 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.1108 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.0562 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.3366 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.3684 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.2791 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.4743 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.2767 | Êó∂Èó¥: 14.0s\n",
            "  Step 60/80 | Loss: 0.1915 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.3290 | Êó∂Èó¥: 14.2s\n",
            "  Step 70/80 | Loss: 0.4097 | Êó∂Èó¥: 13.3s\n",
            "  Step 75/80 | Loss: 0.3674 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.1652 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 265 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2913\n",
            "\n",
            "Fri Dec 19 02:21:00 2025 --- Epoch 266/300 ---\n",
            "  Step 5/80 | Loss: 0.1027 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.3024 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.1311 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.2517 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.4961 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.0785 | Êó∂Èó¥: 14.4s\n",
            "  Step 35/80 | Loss: 0.3750 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.1561 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.0839 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.2288 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.4613 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.3872 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.0536 | Êó∂Èó¥: 14.2s\n",
            "  Step 70/80 | Loss: 0.2220 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.2597 | Êó∂Èó¥: 14.2s\n",
            "  Step 80/80 | Loss: 0.7002 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 266 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3102\n",
            "\n",
            "Fri Dec 19 02:24:39 2025 --- Epoch 267/300 ---\n",
            "  Step 5/80 | Loss: 0.0467 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.1954 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.2721 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.1650 | Êó∂Èó¥: 14.1s\n",
            "  Step 25/80 | Loss: 0.2055 | Êó∂Èó¥: 13.0s\n",
            "  Step 30/80 | Loss: 0.1355 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.7003 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.0587 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.6323 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.3214 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.1346 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.5495 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.1266 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.4014 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.1933 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.4788 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 267 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2836\n",
            "\n",
            "Fri Dec 19 02:28:18 2025 --- Epoch 268/300 ---\n",
            "  Step 5/80 | Loss: 0.4160 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.1132 | Êó∂Èó¥: 13.9s\n",
            "  Step 15/80 | Loss: 0.3914 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.5931 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.5768 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.0468 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.4274 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.3024 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.5169 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.3739 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.3451 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.3088 | Êó∂Èó¥: 14.2s\n",
            "  Step 65/80 | Loss: 0.1185 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.0622 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.1355 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.0511 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 268 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2930\n",
            "\n",
            "Fri Dec 19 02:31:58 2025 --- Epoch 269/300 ---\n",
            "  Step 5/80 | Loss: 0.3973 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.3808 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.0912 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.0821 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.6084 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.2246 | Êó∂Èó¥: 14.9s\n",
            "  Step 35/80 | Loss: 0.2953 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.3045 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.0560 | Êó∂Èó¥: 14.1s\n",
            "  Step 50/80 | Loss: 0.1132 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.3439 | Êó∂Èó¥: 14.1s\n",
            "  Step 60/80 | Loss: 0.3671 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.1430 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.3862 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.0473 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 12.0s\n",
            "üö© Epoch 269 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3034\n",
            "Val 269/300 0/40 , dice_tc: 0.89812344 , dice_wt: 0.9544932 , dice_et: 0.8929787 , time 2.65s\n",
            "Val 269/300 1/40 , dice_tc: 0.9336362 , dice_wt: 0.9664384 , dice_et: 0.8929787 , time 2.69s\n",
            "Val 269/300 2/40 , dice_tc: 0.9458389 , dice_wt: 0.9474911 , dice_et: 0.9314947 , time 2.69s\n",
            "Val 269/300 3/40 , dice_tc: 0.9312709 , dice_wt: 0.9356102 , dice_et: 0.9214323 , time 2.69s\n",
            "Val 269/300 4/40 , dice_tc: 0.86142236 , dice_wt: 0.8560683 , dice_et: 0.7864199 , time 2.53s\n",
            "Val 269/300 5/40 , dice_tc: 0.836356 , dice_wt: 0.85440826 , dice_et: 0.7716898 , time 2.54s\n",
            "Val 269/300 6/40 , dice_tc: 0.8369121 , dice_wt: 0.85101897 , dice_et: 0.7842849 , time 2.66s\n",
            "Val 269/300 7/40 , dice_tc: 0.83795464 , dice_wt: 0.85207784 , dice_et: 0.79507226 , time 2.55s\n",
            "Val 269/300 8/40 , dice_tc: 0.836132 , dice_wt: 0.8622962 , dice_et: 0.80044687 , time 2.73s\n",
            "Val 269/300 9/40 , dice_tc: 0.82627517 , dice_wt: 0.8678333 , dice_et: 0.81428933 , time 2.57s\n",
            "Val 269/300 10/40 , dice_tc: 0.83158547 , dice_wt: 0.8725933 , dice_et: 0.8214731 , time 2.46s\n",
            "Val 269/300 11/40 , dice_tc: 0.8341096 , dice_wt: 0.86846 , dice_et: 0.8256611 , time 2.67s\n",
            "Val 269/300 12/40 , dice_tc: 0.8383235 , dice_wt: 0.8741527 , dice_et: 0.8256611 , time 2.62s\n",
            "Val 269/300 13/40 , dice_tc: 0.832189 , dice_wt: 0.87464964 , dice_et: 0.81873745 , time 2.53s\n",
            "Val 269/300 14/40 , dice_tc: 0.8299312 , dice_wt: 0.8779177 , dice_et: 0.75575763 , time 2.63s\n",
            "Val 269/300 15/40 , dice_tc: 0.83102447 , dice_wt: 0.87993157 , dice_et: 0.7667832 , time 2.69s\n",
            "Val 269/300 16/40 , dice_tc: 0.8011018 , dice_wt: 0.8772285 , dice_et: 0.73662513 , time 2.70s\n",
            "Val 269/300 17/40 , dice_tc: 0.8034455 , dice_wt: 0.88032645 , dice_et: 0.7437484 , time 2.52s\n",
            "Val 269/300 18/40 , dice_tc: 0.81140196 , dice_wt: 0.8790939 , dice_et: 0.7560357 , time 2.67s\n",
            "Val 269/300 19/40 , dice_tc: 0.81422484 , dice_wt: 0.8811598 , dice_et: 0.7621679 , time 2.68s\n",
            "Val 269/300 20/40 , dice_tc: 0.8166733 , dice_wt: 0.88315237 , dice_et: 0.7674887 , time 2.44s\n",
            "Val 269/300 21/40 , dice_tc: 0.8216145 , dice_wt: 0.8856041 , dice_et: 0.77534807 , time 2.52s\n",
            "Val 269/300 22/40 , dice_tc: 0.81789 , dice_wt: 0.88877296 , dice_et: 0.7752098 , time 2.72s\n",
            "Val 269/300 23/40 , dice_tc: 0.81727415 , dice_wt: 0.8915372 , dice_et: 0.77628917 , time 2.70s\n",
            "Val 269/300 24/40 , dice_tc: 0.81734115 , dice_wt: 0.8945434 , dice_et: 0.7781645 , time 2.72s\n",
            "Val 269/300 25/40 , dice_tc: 0.81813514 , dice_wt: 0.89667547 , dice_et: 0.7781645 , time 2.67s\n",
            "Val 269/300 26/40 , dice_tc: 0.8215995 , dice_wt: 0.896365 , dice_et: 0.7838101 , time 2.71s\n",
            "Val 269/300 27/40 , dice_tc: 0.8204459 , dice_wt: 0.8959502 , dice_et: 0.7801405 , time 2.72s\n",
            "Val 269/300 28/40 , dice_tc: 0.8224194 , dice_wt: 0.8957495 , dice_et: 0.78408515 , time 2.61s\n",
            "Val 269/300 29/40 , dice_tc: 0.8091968 , dice_wt: 0.8957912 , dice_et: 0.77054876 , time 2.54s\n",
            "Val 269/300 30/40 , dice_tc: 0.8105138 , dice_wt: 0.8963012 , dice_et: 0.75007045 , time 2.67s\n",
            "Val 269/300 31/40 , dice_tc: 0.81513095 , dice_wt: 0.8975377 , dice_et: 0.75733304 , time 2.66s\n",
            "Val 269/300 32/40 , dice_tc: 0.79043 , dice_wt: 0.89478654 , dice_et: 0.75733304 , time 2.46s\n",
            "Val 269/300 33/40 , dice_tc: 0.7924253 , dice_wt: 0.8945254 , dice_et: 0.76127946 , time 2.61s\n",
            "Val 269/300 34/40 , dice_tc: 0.79550374 , dice_wt: 0.89440906 , dice_et: 0.76580286 , time 2.44s\n",
            "Val 269/300 35/40 , dice_tc: 0.7829621 , dice_wt: 0.89148456 , dice_et: 0.7536946 , time 2.71s\n",
            "Val 269/300 36/40 , dice_tc: 0.7880174 , dice_wt: 0.8927854 , dice_et: 0.76023215 , time 2.48s\n",
            "Val 269/300 37/40 , dice_tc: 0.78688675 , dice_wt: 0.8940603 , dice_et: 0.7603674 , time 2.66s\n",
            "Val 269/300 38/40 , dice_tc: 0.7900081 , dice_wt: 0.8951132 , dice_et: 0.7642908 , time 2.66s\n",
            "Val 269/300 39/40 , dice_tc: 0.7785037 , dice_wt: 0.8950718 , dice_et: 0.7642908 , time 2.64s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 02:37:24 2025 --- Epoch 270/300 ---\n",
            "  Step 5/80 | Loss: 0.1505 | Êó∂Èó¥: 14.4s\n",
            "  Step 10/80 | Loss: 0.1203 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.3112 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.1833 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.2606 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.5009 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.2835 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.4795 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.3694 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.1685 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.6095 | Êó∂Èó¥: 14.0s\n",
            "  Step 60/80 | Loss: 0.0536 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.4137 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.5021 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.4023 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.4608 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 270 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3255\n",
            "\n",
            "Fri Dec 19 02:41:03 2025 --- Epoch 271/300 ---\n",
            "  Step 5/80 | Loss: 0.0546 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.3179 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.4261 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2897 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.5979 | Êó∂Èó¥: 14.1s\n",
            "  Step 30/80 | Loss: 0.1561 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.0545 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.3994 | Êó∂Èó¥: 13.2s\n",
            "  Step 45/80 | Loss: 0.7000 | Êó∂Èó¥: 14.0s\n",
            "  Step 50/80 | Loss: 0.2176 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.3680 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.2969 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.0863 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.1420 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.1020 | Êó∂Èó¥: 14.0s\n",
            "  Step 80/80 | Loss: 0.2145 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 271 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2581\n",
            "\n",
            "Fri Dec 19 02:44:42 2025 --- Epoch 272/300 ---\n",
            "  Step 5/80 | Loss: 0.2711 | Êó∂Èó¥: 13.1s\n",
            "  Step 10/80 | Loss: 0.0691 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.2876 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.2886 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.2597 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.0823 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.3926 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.4268 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.3851 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.3926 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.0905 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.2004 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.5100 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.2909 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.2930 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.4993 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 272 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2612\n",
            "\n",
            "Fri Dec 19 02:48:20 2025 --- Epoch 273/300 ---\n",
            "  Step 5/80 | Loss: 0.3097 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.3831 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.0847 | Êó∂Èó¥: 13.3s\n",
            "  Step 20/80 | Loss: 0.3826 | Êó∂Èó¥: 14.0s\n",
            "  Step 25/80 | Loss: 0.2331 | Êó∂Èó¥: 14.3s\n",
            "  Step 30/80 | Loss: 0.4131 | Êó∂Èó¥: 14.2s\n",
            "  Step 35/80 | Loss: 0.2882 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.1090 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.3680 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.5608 | Êó∂Èó¥: 14.2s\n",
            "  Step 55/80 | Loss: 0.1417 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.3307 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.2509 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.7000 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.4090 | Êó∂Èó¥: 14.1s\n",
            "  Step 80/80 | Loss: 0.0851 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 273 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2940\n",
            "\n",
            "Fri Dec 19 02:51:59 2025 --- Epoch 274/300 ---\n",
            "  Step 5/80 | Loss: 0.2168 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.0769 | Êó∂Èó¥: 13.3s\n",
            "  Step 15/80 | Loss: 0.1515 | Êó∂Èó¥: 13.1s\n",
            "  Step 20/80 | Loss: 0.0664 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.6283 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.5359 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.1028 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.3175 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.1886 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.1510 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.4162 | Êó∂Èó¥: 14.2s\n",
            "  Step 60/80 | Loss: 0.4546 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.3791 | Êó∂Èó¥: 14.4s\n",
            "  Step 70/80 | Loss: 0.1085 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.3718 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 274 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2741\n",
            "\n",
            "Fri Dec 19 02:55:38 2025 --- Epoch 275/300 ---\n",
            "  Step 5/80 | Loss: 0.5934 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.3363 | Êó∂Èó¥: 13.2s\n",
            "  Step 15/80 | Loss: 0.0774 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.4252 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.5405 | Êó∂Èó¥: 14.5s\n",
            "  Step 30/80 | Loss: 0.5110 | Êó∂Èó¥: 14.1s\n",
            "  Step 35/80 | Loss: 0.3761 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.0970 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.2168 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.1531 | Êó∂Èó¥: 14.2s\n",
            "  Step 55/80 | Loss: 0.2252 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.3026 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.0903 | Êó∂Èó¥: 14.2s\n",
            "  Step 70/80 | Loss: 0.2913 | Êó∂Èó¥: 14.2s\n",
            "  Step 75/80 | Loss: 0.1801 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.4824 | Êó∂Èó¥: 11.8s\n",
            "üö© Epoch 275 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2727\n",
            "\n",
            "Fri Dec 19 02:59:18 2025 --- Epoch 276/300 ---\n",
            "  Step 5/80 | Loss: 0.3174 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.3144 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.3790 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.2252 | Êó∂Èó¥: 13.7s\n",
            "  Step 25/80 | Loss: 0.3612 | Êó∂Èó¥: 14.0s\n",
            "  Step 30/80 | Loss: 0.3911 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.0677 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.4192 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.0472 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.0488 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.1356 | Êó∂Èó¥: 13.4s\n",
            "  Step 60/80 | Loss: 0.5619 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.6404 | Êó∂Èó¥: 14.2s\n",
            "  Step 70/80 | Loss: 0.2904 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.2668 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.5315 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 276 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3075\n",
            "\n",
            "Fri Dec 19 03:02:56 2025 --- Epoch 277/300 ---\n",
            "  Step 5/80 | Loss: 0.0813 | Êó∂Èó¥: 13.7s\n",
            "  Step 10/80 | Loss: 0.3994 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.0508 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.2588 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.3928 | Êó∂Èó¥: 14.1s\n",
            "  Step 30/80 | Loss: 0.1772 | Êó∂Èó¥: 13.9s\n",
            "  Step 35/80 | Loss: 0.1817 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.4056 | Êó∂Èó¥: 14.3s\n",
            "  Step 45/80 | Loss: 0.3299 | Êó∂Èó¥: 14.1s\n",
            "  Step 50/80 | Loss: 0.3321 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.5200 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.0755 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.4294 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.4018 | Êó∂Èó¥: 13.8s\n",
            "  Step 75/80 | Loss: 0.3733 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.2729 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 277 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2798\n",
            "\n",
            "Fri Dec 19 03:06:35 2025 --- Epoch 278/300 ---\n",
            "  Step 5/80 | Loss: 0.0904 | Êó∂Èó¥: 14.7s\n",
            "  Step 10/80 | Loss: 0.2569 | Êó∂Èó¥: 14.2s\n",
            "  Step 15/80 | Loss: 0.5786 | Êó∂Èó¥: 14.4s\n",
            "  Step 20/80 | Loss: 0.1240 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.0738 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.2774 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.2618 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.0725 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.2682 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.2460 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.1446 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.0797 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.0654 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.2065 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.2448 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.0748 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 278 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2797\n",
            "\n",
            "Fri Dec 19 03:10:15 2025 --- Epoch 279/300 ---\n",
            "  Step 5/80 | Loss: 0.2410 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.5011 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.1993 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.0749 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.5959 | Êó∂Èó¥: 14.2s\n",
            "  Step 30/80 | Loss: 0.2933 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.2271 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.2995 | Êó∂Èó¥: 14.3s\n",
            "  Step 45/80 | Loss: 0.2312 | Êó∂Èó¥: 14.0s\n",
            "  Step 50/80 | Loss: 0.4463 | Êó∂Èó¥: 13.9s\n",
            "  Step 55/80 | Loss: 0.2682 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.3286 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.1966 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.2876 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.3348 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.4271 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 279 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2888\n",
            "Val 279/300 0/40 , dice_tc: 0.8956372 , dice_wt: 0.9542949 , dice_et: 0.90033907 , time 2.63s\n",
            "Val 279/300 1/40 , dice_tc: 0.92802835 , dice_wt: 0.96567196 , dice_et: 0.90033907 , time 2.62s\n",
            "Val 279/300 2/40 , dice_tc: 0.94117063 , dice_wt: 0.9542771 , dice_et: 0.9337995 , time 2.69s\n",
            "Val 279/300 3/40 , dice_tc: 0.9286966 , dice_wt: 0.94481295 , dice_et: 0.9235668 , time 2.67s\n",
            "Val 279/300 4/40 , dice_tc: 0.86490536 , dice_wt: 0.87074244 , dice_et: 0.78209186 , time 2.52s\n",
            "Val 279/300 5/40 , dice_tc: 0.8783479 , dice_wt: 0.8744917 , dice_et: 0.8150487 , time 2.52s\n",
            "Val 279/300 6/40 , dice_tc: 0.8806554 , dice_wt: 0.87664604 , dice_et: 0.82911223 , time 2.68s\n",
            "Val 279/300 7/40 , dice_tc: 0.8725308 , dice_wt: 0.873287 , dice_et: 0.82912785 , time 2.49s\n",
            "Val 279/300 8/40 , dice_tc: 0.86939675 , dice_wt: 0.8812616 , dice_et: 0.83287835 , time 2.73s\n",
            "Val 279/300 9/40 , dice_tc: 0.86015636 , dice_wt: 0.8852766 , dice_et: 0.8458429 , time 2.50s\n",
            "Val 279/300 10/40 , dice_tc: 0.8617394 , dice_wt: 0.8879715 , dice_et: 0.84947824 , time 2.44s\n",
            "Val 279/300 11/40 , dice_tc: 0.8621194 , dice_wt: 0.88738805 , dice_et: 0.85104394 , time 2.65s\n",
            "Val 279/300 12/40 , dice_tc: 0.86201787 , dice_wt: 0.8916965 , dice_et: 0.85104394 , time 2.52s\n",
            "Val 279/300 13/40 , dice_tc: 0.8526825 , dice_wt: 0.88920534 , dice_et: 0.84011704 , time 2.52s\n",
            "Val 279/300 14/40 , dice_tc: 0.84807575 , dice_wt: 0.89176875 , dice_et: 0.77549267 , time 2.62s\n",
            "Val 279/300 15/40 , dice_tc: 0.84678626 , dice_wt: 0.892769 , dice_et: 0.78519374 , time 2.68s\n",
            "Val 279/300 16/40 , dice_tc: 0.8172994 , dice_wt: 0.8895152 , dice_et: 0.7556747 , time 2.65s\n",
            "Val 279/300 17/40 , dice_tc: 0.8170425 , dice_wt: 0.8915629 , dice_et: 0.7598711 , time 2.54s\n",
            "Val 279/300 18/40 , dice_tc: 0.82432234 , dice_wt: 0.88968164 , dice_et: 0.7713661 , time 2.69s\n",
            "Val 279/300 19/40 , dice_tc: 0.8265869 , dice_wt: 0.8911954 , dice_et: 0.7769473 , time 2.62s\n",
            "Val 279/300 20/40 , dice_tc: 0.82792944 , dice_wt: 0.8926586 , dice_et: 0.7810415 , time 2.44s\n",
            "Val 279/300 21/40 , dice_tc: 0.83268493 , dice_wt: 0.8943797 , dice_et: 0.7886051 , time 2.44s\n",
            "Val 279/300 22/40 , dice_tc: 0.82864845 , dice_wt: 0.89704514 , dice_et: 0.78697747 , time 2.77s\n",
            "Val 279/300 23/40 , dice_tc: 0.82696366 , dice_wt: 0.8993249 , dice_et: 0.78698444 , time 2.73s\n",
            "Val 279/300 24/40 , dice_tc: 0.826347 , dice_wt: 0.90191513 , dice_et: 0.788173 , time 2.72s\n",
            "Val 279/300 25/40 , dice_tc: 0.8245261 , dice_wt: 0.9036965 , dice_et: 0.788173 , time 2.72s\n",
            "Val 279/300 26/40 , dice_tc: 0.8280026 , dice_wt: 0.9039096 , dice_et: 0.7935769 , time 2.78s\n",
            "Val 279/300 27/40 , dice_tc: 0.8264465 , dice_wt: 0.9034532 , dice_et: 0.7890387 , time 2.69s\n",
            "Val 279/300 28/40 , dice_tc: 0.828232 , dice_wt: 0.90393996 , dice_et: 0.79257065 , time 2.62s\n",
            "Val 279/300 29/40 , dice_tc: 0.81180847 , dice_wt: 0.9036471 , dice_et: 0.77619135 , time 2.44s\n",
            "Val 279/300 30/40 , dice_tc: 0.81296545 , dice_wt: 0.90421593 , dice_et: 0.756301 , time 2.63s\n",
            "Val 279/300 31/40 , dice_tc: 0.8176426 , dice_wt: 0.9053311 , dice_et: 0.76341456 , time 2.69s\n",
            "Val 279/300 32/40 , dice_tc: 0.7928656 , dice_wt: 0.90293205 , dice_et: 0.76341456 , time 2.50s\n",
            "Val 279/300 33/40 , dice_tc: 0.79431945 , dice_wt: 0.9024701 , dice_et: 0.76688725 , time 2.71s\n",
            "Val 279/300 34/40 , dice_tc: 0.7976135 , dice_wt: 0.90213406 , dice_et: 0.7715178 , time 2.44s\n",
            "Val 279/300 35/40 , dice_tc: 0.79012656 , dice_wt: 0.90042496 , dice_et: 0.7649862 , time 2.63s\n",
            "Val 279/300 36/40 , dice_tc: 0.7950392 , dice_wt: 0.90132254 , dice_et: 0.77124214 , time 2.53s\n",
            "Val 279/300 37/40 , dice_tc: 0.7934401 , dice_wt: 0.9024606 , dice_et: 0.7709299 , time 2.66s\n",
            "Val 279/300 38/40 , dice_tc: 0.7965836 , dice_wt: 0.90335035 , dice_et: 0.7748884 , time 2.64s\n",
            "Val 279/300 39/40 , dice_tc: 0.78601015 , dice_wt: 0.9032251 , dice_et: 0.7748884 , time 2.63s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 03:15:41 2025 --- Epoch 280/300 ---\n",
            "  Step 5/80 | Loss: 0.2973 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.1364 | Êó∂Èó¥: 14.2s\n",
            "  Step 15/80 | Loss: 0.0697 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.4104 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.2572 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.3505 | Êó∂Èó¥: 13.0s\n",
            "  Step 35/80 | Loss: 0.2917 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.2761 | Êó∂Èó¥: 13.3s\n",
            "  Step 45/80 | Loss: 0.1362 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.1899 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.3294 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.0481 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.4452 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1226 | Êó∂Èó¥: 13.1s\n",
            "  Step 75/80 | Loss: 0.0522 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.6201 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 280 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3233\n",
            "\n",
            "Fri Dec 19 03:19:17 2025 --- Epoch 281/300 ---\n",
            "  Step 5/80 | Loss: 0.1785 | Êó∂Èó¥: 14.1s\n",
            "  Step 10/80 | Loss: 0.4240 | Êó∂Èó¥: 14.3s\n",
            "  Step 15/80 | Loss: 0.4095 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.0923 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.3184 | Êó∂Èó¥: 13.6s\n",
            "  Step 30/80 | Loss: 0.3997 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.0998 | Êó∂Èó¥: 13.2s\n",
            "  Step 40/80 | Loss: 0.2955 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.1225 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.5367 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.2901 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.3071 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.2781 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.3925 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.2704 | Êó∂Èó¥: 13.2s\n",
            "  Step 80/80 | Loss: 0.2323 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 281 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3046\n",
            "\n",
            "Fri Dec 19 03:22:54 2025 --- Epoch 282/300 ---\n",
            "  Step 5/80 | Loss: 0.0456 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.3494 | Êó∂Èó¥: 12.8s\n",
            "  Step 15/80 | Loss: 0.3206 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.5452 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.2861 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.0726 | Êó∂Èó¥: 14.0s\n",
            "  Step 35/80 | Loss: 0.1943 | Êó∂Èó¥: 13.0s\n",
            "  Step 40/80 | Loss: 0.3808 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.1411 | Êó∂Èó¥: 13.9s\n",
            "  Step 50/80 | Loss: 0.2277 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.5172 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.3151 | Êó∂Èó¥: 14.0s\n",
            "  Step 65/80 | Loss: 0.1050 | Êó∂Èó¥: 13.8s\n",
            "  Step 70/80 | Loss: 0.2783 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.3369 | Êó∂Èó¥: 13.5s\n",
            "  Step 80/80 | Loss: 0.5324 | Êó∂Èó¥: 11.9s\n",
            "üö© Epoch 282 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2646\n",
            "\n",
            "Fri Dec 19 03:26:30 2025 --- Epoch 283/300 ---\n",
            "  Step 5/80 | Loss: 0.0786 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.2805 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.3412 | Êó∂Èó¥: 13.9s\n",
            "  Step 20/80 | Loss: 0.1270 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.4563 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.3024 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.0996 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.2993 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.2297 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.0621 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.3923 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.3228 | Êó∂Èó¥: 13.2s\n",
            "  Step 65/80 | Loss: 0.3706 | Êó∂Èó¥: 12.9s\n",
            "  Step 70/80 | Loss: 0.1083 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.0679 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 283 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3068\n",
            "\n",
            "Fri Dec 19 03:30:06 2025 --- Epoch 284/300 ---\n",
            "  Step 5/80 | Loss: 0.1336 | Êó∂Èó¥: 14.0s\n",
            "  Step 10/80 | Loss: 0.4952 | Êó∂Èó¥: 13.9s\n",
            "  Step 15/80 | Loss: 0.3054 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.0539 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.3716 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.3048 | Êó∂Èó¥: 13.4s\n",
            "  Step 35/80 | Loss: 0.1692 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.1332 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.0642 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.1123 | Êó∂Èó¥: 13.4s\n",
            "  Step 55/80 | Loss: 0.3862 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.2743 | Êó∂Èó¥: 12.9s\n",
            "  Step 65/80 | Loss: 0.2897 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.3058 | Êó∂Èó¥: 14.1s\n",
            "  Step 75/80 | Loss: 0.1786 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.5072 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 284 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2846\n",
            "\n",
            "Fri Dec 19 03:33:42 2025 --- Epoch 285/300 ---\n",
            "  Step 5/80 | Loss: 0.0875 | Êó∂Èó¥: 13.9s\n",
            "  Step 10/80 | Loss: 0.3046 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.5678 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.2801 | Êó∂Èó¥: 13.8s\n",
            "  Step 25/80 | Loss: 0.2817 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.1609 | Êó∂Èó¥: 13.4s\n",
            "  Step 35/80 | Loss: 0.0901 | Êó∂Èó¥: 12.9s\n",
            "  Step 40/80 | Loss: 0.3906 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.5779 | Êó∂Èó¥: 13.0s\n",
            "  Step 50/80 | Loss: 0.0852 | Êó∂Èó¥: 14.1s\n",
            "  Step 55/80 | Loss: 0.4394 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.2858 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.0537 | Êó∂Èó¥: 13.5s\n",
            "  Step 70/80 | Loss: 0.3153 | Êó∂Èó¥: 13.6s\n",
            "  Step 75/80 | Loss: 0.0546 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.2283 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 285 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2766\n",
            "\n",
            "Fri Dec 19 03:37:18 2025 --- Epoch 286/300 ---\n",
            "  Step 5/80 | Loss: 0.1292 | Êó∂Èó¥: 13.9s\n",
            "  Step 10/80 | Loss: 0.3857 | Êó∂Èó¥: 13.1s\n",
            "  Step 15/80 | Loss: 0.0583 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.1851 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.3881 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.1725 | Êó∂Èó¥: 13.4s\n",
            "  Step 35/80 | Loss: 0.5340 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.4965 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.5757 | Êó∂Èó¥: 13.3s\n",
            "  Step 50/80 | Loss: 0.0883 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.0787 | Êó∂Èó¥: 13.0s\n",
            "  Step 60/80 | Loss: 0.3999 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.4776 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.1660 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.4757 | Êó∂Èó¥: 13.7s\n",
            "  Step 80/80 | Loss: 0.2804 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 286 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3048\n",
            "\n",
            "Fri Dec 19 03:40:55 2025 --- Epoch 287/300 ---\n",
            "  Step 5/80 | Loss: 0.0981 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.5939 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.4326 | Êó∂Èó¥: 13.4s\n",
            "  Step 20/80 | Loss: 0.2336 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.1014 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.2781 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.1581 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.0936 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.0519 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.1155 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.2387 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.5311 | Êó∂Èó¥: 13.4s\n",
            "  Step 65/80 | Loss: 0.2908 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.2602 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.2946 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.0493 | Êó∂Èó¥: 12.5s\n",
            "üö© Epoch 287 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2421\n",
            "\n",
            "Fri Dec 19 03:44:32 2025 --- Epoch 288/300 ---\n",
            "  Step 5/80 | Loss: 0.1475 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.1724 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.2651 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.1679 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.2306 | Êó∂Èó¥: 13.2s\n",
            "  Step 30/80 | Loss: 0.1671 | Êó∂Èó¥: 14.1s\n",
            "  Step 35/80 | Loss: 0.3085 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.7015 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.3507 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.4980 | Êó∂Èó¥: 14.2s\n",
            "  Step 55/80 | Loss: 0.3202 | Êó∂Èó¥: 13.8s\n",
            "  Step 60/80 | Loss: 0.1038 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1191 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.2766 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.0894 | Êó∂Èó¥: 14.0s\n",
            "  Step 80/80 | Loss: 0.1182 | Êó∂Èó¥: 12.1s\n",
            "üö© Epoch 288 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2729\n",
            "\n",
            "Fri Dec 19 03:48:09 2025 --- Epoch 289/300 ---\n",
            "  Step 5/80 | Loss: 0.2162 | Êó∂Èó¥: 13.5s\n",
            "  Step 10/80 | Loss: 0.3058 | Êó∂Èó¥: 13.4s\n",
            "  Step 15/80 | Loss: 0.1474 | Êó∂Èó¥: 13.7s\n",
            "  Step 20/80 | Loss: 0.4631 | Êó∂Èó¥: 13.5s\n",
            "  Step 25/80 | Loss: 0.2637 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.2730 | Êó∂Èó¥: 14.1s\n",
            "  Step 35/80 | Loss: 0.3783 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.3706 | Êó∂Èó¥: 13.7s\n",
            "  Step 45/80 | Loss: 0.3174 | Êó∂Èó¥: 13.7s\n",
            "  Step 50/80 | Loss: 0.0510 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.0956 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.1384 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.3864 | Êó∂Èó¥: 13.6s\n",
            "  Step 70/80 | Loss: 0.2461 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.4836 | Êó∂Èó¥: 13.4s\n",
            "  Step 80/80 | Loss: 0.0566 | Êó∂Èó¥: 12.6s\n",
            "üö© Epoch 289 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2460\n",
            "Val 289/300 0/40 , dice_tc: 0.9012919 , dice_wt: 0.95425916 , dice_et: 0.90520597 , time 2.68s\n",
            "Val 289/300 1/40 , dice_tc: 0.9349878 , dice_wt: 0.9663035 , dice_et: 0.90520597 , time 2.63s\n",
            "Val 289/300 2/40 , dice_tc: 0.9456845 , dice_wt: 0.9469293 , dice_et: 0.93587065 , time 2.68s\n",
            "Val 289/300 3/40 , dice_tc: 0.93275654 , dice_wt: 0.93644774 , dice_et: 0.9257031 , time 2.72s\n",
            "Val 289/300 4/40 , dice_tc: 0.8678132 , dice_wt: 0.85879374 , dice_et: 0.7699579 , time 2.52s\n",
            "Val 289/300 5/40 , dice_tc: 0.869178 , dice_wt: 0.86197656 , dice_et: 0.79263675 , time 2.53s\n",
            "Val 289/300 6/40 , dice_tc: 0.8687571 , dice_wt: 0.86018723 , dice_et: 0.8056844 , time 2.68s\n",
            "Val 289/300 7/40 , dice_tc: 0.86357236 , dice_wt: 0.8584256 , dice_et: 0.8102208 , time 2.55s\n",
            "Val 289/300 8/40 , dice_tc: 0.86130667 , dice_wt: 0.86822647 , dice_et: 0.8177836 , time 2.73s\n",
            "Val 289/300 9/40 , dice_tc: 0.8519182 , dice_wt: 0.87359583 , dice_et: 0.83259976 , time 2.54s\n",
            "Val 289/300 10/40 , dice_tc: 0.854312 , dice_wt: 0.8774898 , dice_et: 0.8374594 , time 2.46s\n",
            "Val 289/300 11/40 , dice_tc: 0.85510546 , dice_wt: 0.8726308 , dice_et: 0.8399076 , time 2.65s\n",
            "Val 289/300 12/40 , dice_tc: 0.8575555 , dice_wt: 0.8779869 , dice_et: 0.8399076 , time 2.50s\n",
            "Val 289/300 13/40 , dice_tc: 0.84823114 , dice_wt: 0.8773121 , dice_et: 0.82974696 , time 2.47s\n",
            "Val 289/300 14/40 , dice_tc: 0.84776056 , dice_wt: 0.8806907 , dice_et: 0.7659203 , time 2.61s\n",
            "Val 289/300 15/40 , dice_tc: 0.8450918 , dice_wt: 0.88262993 , dice_et: 0.7763572 , time 2.67s\n",
            "Val 289/300 16/40 , dice_tc: 0.8150493 , dice_wt: 0.8797019 , dice_et: 0.74680614 , time 2.63s\n",
            "Val 289/300 17/40 , dice_tc: 0.8141862 , dice_wt: 0.8823283 , dice_et: 0.7507632 , time 2.54s\n",
            "Val 289/300 18/40 , dice_tc: 0.8216122 , dice_wt: 0.88080245 , dice_et: 0.7628163 , time 2.64s\n",
            "Val 289/300 19/40 , dice_tc: 0.8241147 , dice_wt: 0.8828367 , dice_et: 0.76917356 , time 2.61s\n",
            "Val 289/300 20/40 , dice_tc: 0.82594043 , dice_wt: 0.88466173 , dice_et: 0.7740617 , time 2.42s\n",
            "Val 289/300 21/40 , dice_tc: 0.8307123 , dice_wt: 0.8872866 , dice_et: 0.781895 , time 2.47s\n",
            "Val 289/300 22/40 , dice_tc: 0.82386124 , dice_wt: 0.8903137 , dice_et: 0.7786319 , time 2.73s\n",
            "Val 289/300 23/40 , dice_tc: 0.8224997 , dice_wt: 0.8928718 , dice_et: 0.7789078 , time 2.65s\n",
            "Val 289/300 24/40 , dice_tc: 0.82310057 , dice_wt: 0.8958761 , dice_et: 0.7814695 , time 2.72s\n",
            "Val 289/300 25/40 , dice_tc: 0.82082206 , dice_wt: 0.8979002 , dice_et: 0.7814695 , time 2.71s\n",
            "Val 289/300 26/40 , dice_tc: 0.824479 , dice_wt: 0.8979373 , dice_et: 0.7872307 , time 2.76s\n",
            "Val 289/300 27/40 , dice_tc: 0.8230939 , dice_wt: 0.8975342 , dice_et: 0.7826994 , time 2.75s\n",
            "Val 289/300 28/40 , dice_tc: 0.8250848 , dice_wt: 0.8975466 , dice_et: 0.7864829 , time 2.61s\n",
            "Val 289/300 29/40 , dice_tc: 0.80707306 , dice_wt: 0.89755374 , dice_et: 0.768771 , time 2.49s\n",
            "Val 289/300 30/40 , dice_tc: 0.8086034 , dice_wt: 0.8981219 , dice_et: 0.74844134 , time 2.65s\n",
            "Val 289/300 31/40 , dice_tc: 0.81332797 , dice_wt: 0.89950794 , dice_et: 0.75579953 , time 2.67s\n",
            "Val 289/300 32/40 , dice_tc: 0.7886817 , dice_wt: 0.8914919 , dice_et: 0.75579953 , time 2.53s\n",
            "Val 289/300 33/40 , dice_tc: 0.79005456 , dice_wt: 0.8914029 , dice_et: 0.75942814 , time 2.67s\n",
            "Val 289/300 34/40 , dice_tc: 0.7935621 , dice_wt: 0.89139247 , dice_et: 0.7644035 , time 2.45s\n",
            "Val 289/300 35/40 , dice_tc: 0.7828963 , dice_wt: 0.88876086 , dice_et: 0.75451356 , time 2.68s\n",
            "Val 289/300 36/40 , dice_tc: 0.78800845 , dice_wt: 0.8901859 , dice_et: 0.7610884 , time 2.44s\n",
            "Val 289/300 37/40 , dice_tc: 0.78624654 , dice_wt: 0.8915715 , dice_et: 0.7605611 , time 2.68s\n",
            "Val 289/300 38/40 , dice_tc: 0.78970015 , dice_wt: 0.89252627 , dice_et: 0.7649705 , time 2.65s\n",
            "Val 289/300 39/40 , dice_tc: 0.779513 , dice_wt: 0.89264697 , dice_et: 0.7649705 , time 2.65s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n",
            "\n",
            "Fri Dec 19 03:53:33 2025 --- Epoch 290/300 ---\n",
            "  Step 5/80 | Loss: 0.1377 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.2777 | Êó∂Èó¥: 14.3s\n",
            "  Step 15/80 | Loss: 0.0821 | Êó∂Èó¥: 13.2s\n",
            "  Step 20/80 | Loss: 0.3035 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.1545 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.3784 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.1070 | Êó∂Èó¥: 14.3s\n",
            "  Step 40/80 | Loss: 0.2611 | Êó∂Èó¥: 13.5s\n",
            "  Step 45/80 | Loss: 0.6317 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.3018 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.3284 | Êó∂Èó¥: 13.2s\n",
            "  Step 60/80 | Loss: 0.1668 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.2263 | Êó∂Èó¥: 13.7s\n",
            "  Step 70/80 | Loss: 0.3376 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.2904 | Êó∂Èó¥: 14.0s\n",
            "  Step 80/80 | Loss: 0.1249 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 290 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2830\n",
            "\n",
            "Fri Dec 19 03:57:11 2025 --- Epoch 291/300 ---\n",
            "  Step 5/80 | Loss: 0.2063 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.2262 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.3932 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.6001 | Êó∂Èó¥: 13.6s\n",
            "  Step 25/80 | Loss: 0.4956 | Êó∂Èó¥: 13.4s\n",
            "  Step 30/80 | Loss: 0.0869 | Êó∂Èó¥: 13.5s\n",
            "  Step 35/80 | Loss: 0.1202 | Êó∂Èó¥: 13.6s\n",
            "  Step 40/80 | Loss: 0.1449 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.0873 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.3078 | Êó∂Èó¥: 13.8s\n",
            "  Step 55/80 | Loss: 0.7000 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.4060 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.5926 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1829 | Êó∂Èó¥: 13.9s\n",
            "  Step 75/80 | Loss: 0.1273 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.0415 | Êó∂Èó¥: 12.8s\n",
            "üö© Epoch 291 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2930\n",
            "\n",
            "Fri Dec 19 04:00:48 2025 --- Epoch 292/300 ---\n",
            "  Step 5/80 | Loss: 0.3382 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.3379 | Êó∂Èó¥: 13.8s\n",
            "  Step 15/80 | Loss: 0.2683 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.2799 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.1618 | Êó∂Èó¥: 13.9s\n",
            "  Step 30/80 | Loss: 0.2756 | Êó∂Èó¥: 14.2s\n",
            "  Step 35/80 | Loss: 0.2517 | Êó∂Èó¥: 13.7s\n",
            "  Step 40/80 | Loss: 0.4418 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.3732 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.4223 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.3087 | Êó∂Èó¥: 13.6s\n",
            "  Step 60/80 | Loss: 0.3113 | Êó∂Èó¥: 13.6s\n",
            "  Step 65/80 | Loss: 0.1858 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.0739 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.3934 | Êó∂Èó¥: 13.0s\n",
            "  Step 80/80 | Loss: 0.0541 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 292 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2934\n",
            "\n",
            "Fri Dec 19 04:04:25 2025 --- Epoch 293/300 ---\n",
            "  Step 5/80 | Loss: 0.1070 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.3897 | Êó∂Èó¥: 13.6s\n",
            "  Step 15/80 | Loss: 0.2963 | Êó∂Èó¥: 13.5s\n",
            "  Step 20/80 | Loss: 0.1005 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.6946 | Êó∂Èó¥: 13.5s\n",
            "  Step 30/80 | Loss: 0.3346 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.0929 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.3083 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.3746 | Êó∂Èó¥: 13.4s\n",
            "  Step 50/80 | Loss: 0.5319 | Êó∂Èó¥: 13.5s\n",
            "  Step 55/80 | Loss: 0.6051 | Êó∂Èó¥: 14.1s\n",
            "  Step 60/80 | Loss: 0.5099 | Êó∂Èó¥: 13.9s\n",
            "  Step 65/80 | Loss: 0.2923 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.7000 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.2804 | Êó∂Èó¥: 13.9s\n",
            "  Step 80/80 | Loss: 0.6596 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 293 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3139\n",
            "\n",
            "Fri Dec 19 04:08:03 2025 --- Epoch 294/300 ---\n",
            "  Step 5/80 | Loss: 0.1025 | Êó∂Èó¥: 13.6s\n",
            "  Step 10/80 | Loss: 0.2137 | Êó∂Èó¥: 13.7s\n",
            "  Step 15/80 | Loss: 0.4121 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.0905 | Êó∂Èó¥: 14.1s\n",
            "  Step 25/80 | Loss: 0.3159 | Êó∂Èó¥: 13.3s\n",
            "  Step 30/80 | Loss: 0.1847 | Êó∂Èó¥: 13.3s\n",
            "  Step 35/80 | Loss: 0.3988 | Êó∂Èó¥: 13.4s\n",
            "  Step 40/80 | Loss: 0.3498 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.2654 | Êó∂Èó¥: 14.2s\n",
            "  Step 50/80 | Loss: 0.5031 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.0685 | Êó∂Èó¥: 13.9s\n",
            "  Step 60/80 | Loss: 0.3161 | Êó∂Èó¥: 13.5s\n",
            "  Step 65/80 | Loss: 0.3854 | Êó∂Èó¥: 13.1s\n",
            "  Step 70/80 | Loss: 0.2828 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.2820 | Êó∂Èó¥: 13.2s\n",
            "  Step 80/80 | Loss: 0.0663 | Êó∂Èó¥: 12.7s\n",
            "üö© Epoch 294 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2789\n",
            "\n",
            "Fri Dec 19 04:11:39 2025 --- Epoch 295/300 ---\n",
            "  Step 5/80 | Loss: 0.1942 | Êó∂Èó¥: 13.0s\n",
            "  Step 10/80 | Loss: 0.3792 | Êó∂Èó¥: 14.0s\n",
            "  Step 15/80 | Loss: 0.1803 | Êó∂Èó¥: 13.8s\n",
            "  Step 20/80 | Loss: 0.3760 | Êó∂Èó¥: 13.9s\n",
            "  Step 25/80 | Loss: 0.1387 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.0869 | Êó∂Èó¥: 13.8s\n",
            "  Step 35/80 | Loss: 0.0641 | Êó∂Èó¥: 14.1s\n",
            "  Step 40/80 | Loss: 0.0867 | Êó∂Èó¥: 13.4s\n",
            "  Step 45/80 | Loss: 0.0956 | Êó∂Èó¥: 13.5s\n",
            "  Step 50/80 | Loss: 0.0978 | Êó∂Èó¥: 13.0s\n",
            "  Step 55/80 | Loss: 0.3909 | Êó∂Èó¥: 13.7s\n",
            "  Step 60/80 | Loss: 0.2507 | Êó∂Èó¥: 13.8s\n",
            "  Step 65/80 | Loss: 0.0904 | Êó∂Èó¥: 13.4s\n",
            "  Step 70/80 | Loss: 0.2174 | Êó∂Èó¥: 14.3s\n",
            "  Step 75/80 | Loss: 0.2043 | Êó∂Èó¥: 13.6s\n",
            "  Step 80/80 | Loss: 0.1304 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 295 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2518\n",
            "\n",
            "Fri Dec 19 04:15:17 2025 --- Epoch 296/300 ---\n",
            "  Step 5/80 | Loss: 0.2865 | Êó∂Èó¥: 13.1s\n",
            "  Step 10/80 | Loss: 0.3772 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.4456 | Êó∂Èó¥: 13.6s\n",
            "  Step 20/80 | Loss: 0.3933 | Êó∂Èó¥: 14.2s\n",
            "  Step 25/80 | Loss: 0.3724 | Êó∂Èó¥: 13.8s\n",
            "  Step 30/80 | Loss: 0.3848 | Êó∂Èó¥: 13.6s\n",
            "  Step 35/80 | Loss: 0.0766 | Êó∂Èó¥: 13.8s\n",
            "  Step 40/80 | Loss: 0.4234 | Êó∂Èó¥: 14.0s\n",
            "  Step 45/80 | Loss: 0.3347 | Êó∂Èó¥: 13.2s\n",
            "  Step 50/80 | Loss: 0.1609 | Êó∂Èó¥: 13.6s\n",
            "  Step 55/80 | Loss: 0.4582 | Êó∂Èó¥: 12.9s\n",
            "  Step 60/80 | Loss: 0.5089 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1046 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.1086 | Êó∂Èó¥: 13.7s\n",
            "  Step 75/80 | Loss: 0.1128 | Êó∂Èó¥: 13.8s\n",
            "  Step 80/80 | Loss: 0.7000 | Êó∂Èó¥: 12.4s\n",
            "üö© Epoch 296 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.3173\n",
            "\n",
            "Fri Dec 19 04:18:53 2025 --- Epoch 297/300 ---\n",
            "  Step 5/80 | Loss: 0.3400 | Êó∂Èó¥: 13.8s\n",
            "  Step 10/80 | Loss: 0.3309 | Êó∂Èó¥: 14.1s\n",
            "  Step 15/80 | Loss: 0.0694 | Êó∂Èó¥: 14.2s\n",
            "  Step 20/80 | Loss: 0.2722 | Êó∂Èó¥: 13.4s\n",
            "  Step 25/80 | Loss: 0.3849 | Êó∂Èó¥: 14.2s\n",
            "  Step 30/80 | Loss: 0.2400 | Êó∂Èó¥: 13.2s\n",
            "  Step 35/80 | Loss: 0.2870 | Êó∂Èó¥: 14.0s\n",
            "  Step 40/80 | Loss: 0.2967 | Êó∂Èó¥: 13.8s\n",
            "  Step 45/80 | Loss: 0.1828 | Êó∂Èó¥: 13.6s\n",
            "  Step 50/80 | Loss: 0.1545 | Êó∂Èó¥: 13.7s\n",
            "  Step 55/80 | Loss: 0.0765 | Êó∂Èó¥: 13.3s\n",
            "  Step 60/80 | Loss: 0.4060 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.3634 | Êó∂Èó¥: 13.9s\n",
            "  Step 70/80 | Loss: 0.6060 | Êó∂Èó¥: 13.4s\n",
            "  Step 75/80 | Loss: 0.1731 | Êó∂Èó¥: 13.2s\n",
            "  Step 80/80 | Loss: 0.3044 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 297 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2846\n",
            "\n",
            "Fri Dec 19 04:22:31 2025 --- Epoch 298/300 ---\n",
            "  Step 5/80 | Loss: 0.1071 | Êó∂Èó¥: 13.3s\n",
            "  Step 10/80 | Loss: 0.2690 | Êó∂Èó¥: 13.5s\n",
            "  Step 15/80 | Loss: 0.1129 | Êó∂Èó¥: 14.0s\n",
            "  Step 20/80 | Loss: 0.2841 | Êó∂Èó¥: 13.1s\n",
            "  Step 25/80 | Loss: 0.1116 | Êó∂Èó¥: 13.7s\n",
            "  Step 30/80 | Loss: 0.1889 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.1198 | Êó∂Èó¥: 13.5s\n",
            "  Step 40/80 | Loss: 0.0846 | Êó∂Èó¥: 13.9s\n",
            "  Step 45/80 | Loss: 0.0708 | Êó∂Èó¥: 13.8s\n",
            "  Step 50/80 | Loss: 0.5066 | Êó∂Èó¥: 13.3s\n",
            "  Step 55/80 | Loss: 0.3943 | Êó∂Èó¥: 13.5s\n",
            "  Step 60/80 | Loss: 0.1735 | Êó∂Èó¥: 13.7s\n",
            "  Step 65/80 | Loss: 0.1319 | Êó∂Èó¥: 14.0s\n",
            "  Step 70/80 | Loss: 0.4763 | Êó∂Èó¥: 14.2s\n",
            "  Step 75/80 | Loss: 0.1149 | Êó∂Èó¥: 13.1s\n",
            "  Step 80/80 | Loss: 0.5110 | Êó∂Èó¥: 12.2s\n",
            "üö© Epoch 298 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2364\n",
            "\n",
            "Fri Dec 19 04:26:07 2025 --- Epoch 299/300 ---\n",
            "  Step 5/80 | Loss: 0.2508 | Êó∂Èó¥: 13.4s\n",
            "  Step 10/80 | Loss: 0.1769 | Êó∂Èó¥: 13.9s\n",
            "  Step 15/80 | Loss: 0.1810 | Êó∂Èó¥: 14.1s\n",
            "  Step 20/80 | Loss: 0.2730 | Êó∂Èó¥: 13.3s\n",
            "  Step 25/80 | Loss: 0.4550 | Êó∂Èó¥: 14.2s\n",
            "  Step 30/80 | Loss: 0.1197 | Êó∂Èó¥: 13.7s\n",
            "  Step 35/80 | Loss: 0.5666 | Êó∂Èó¥: 13.9s\n",
            "  Step 40/80 | Loss: 0.3266 | Êó∂Èó¥: 13.6s\n",
            "  Step 45/80 | Loss: 0.3088 | Êó∂Èó¥: 14.0s\n",
            "  Step 50/80 | Loss: 0.2027 | Êó∂Èó¥: 14.0s\n",
            "  Step 55/80 | Loss: 0.2031 | Êó∂Èó¥: 13.0s\n",
            "  Step 60/80 | Loss: 0.3009 | Êó∂Èó¥: 13.3s\n",
            "  Step 65/80 | Loss: 0.2068 | Êó∂Èó¥: 13.3s\n",
            "  Step 70/80 | Loss: 0.1685 | Êó∂Èó¥: 13.5s\n",
            "  Step 75/80 | Loss: 0.4066 | Êó∂Èó¥: 13.3s\n",
            "  Step 80/80 | Loss: 0.1163 | Êó∂Èó¥: 12.3s\n",
            "üö© Epoch 299 ËÆ≠ÁªÉÂÆåÊàêÔºåÂπ≥Âùá Loss: 0.2645\n",
            "Val 299/300 0/40 , dice_tc: 0.8911568 , dice_wt: 0.95368814 , dice_et: 0.89143395 , time 2.75s\n",
            "Val 299/300 1/40 , dice_tc: 0.92889404 , dice_wt: 0.9636899 , dice_et: 0.89143395 , time 2.66s\n",
            "Val 299/300 2/40 , dice_tc: 0.9424203 , dice_wt: 0.9452391 , dice_et: 0.93036306 , time 2.73s\n",
            "Val 299/300 3/40 , dice_tc: 0.92903745 , dice_wt: 0.9364954 , dice_et: 0.92108184 , time 2.60s\n",
            "Val 299/300 4/40 , dice_tc: 0.8603309 , dice_wt: 0.8605405 , dice_et: 0.786974 , time 2.52s\n",
            "Val 299/300 5/40 , dice_tc: 0.86092454 , dice_wt: 0.86301214 , dice_et: 0.8037276 , time 2.58s\n",
            "Val 299/300 6/40 , dice_tc: 0.86026484 , dice_wt: 0.8618351 , dice_et: 0.8136547 , time 2.68s\n",
            "Val 299/300 7/40 , dice_tc: 0.8608456 , dice_wt: 0.86111844 , dice_et: 0.8230008 , time 2.44s\n",
            "Val 299/300 8/40 , dice_tc: 0.8579773 , dice_wt: 0.8702344 , dice_et: 0.82630634 , time 2.68s\n",
            "Val 299/300 9/40 , dice_tc: 0.84888744 , dice_wt: 0.87520254 , dice_et: 0.84011877 , time 2.49s\n",
            "Val 299/300 10/40 , dice_tc: 0.8521614 , dice_wt: 0.8789871 , dice_et: 0.84499943 , time 2.46s\n",
            "Val 299/300 11/40 , dice_tc: 0.8532261 , dice_wt: 0.8762908 , dice_et: 0.846978 , time 2.63s\n",
            "Val 299/300 12/40 , dice_tc: 0.85440505 , dice_wt: 0.88136715 , dice_et: 0.846978 , time 2.51s\n",
            "Val 299/300 13/40 , dice_tc: 0.84890383 , dice_wt: 0.8804941 , dice_et: 0.8408384 , time 2.46s\n",
            "Val 299/300 14/40 , dice_tc: 0.8454588 , dice_wt: 0.8839137 , dice_et: 0.77615845 , time 2.65s\n",
            "Val 299/300 15/40 , dice_tc: 0.8443857 , dice_wt: 0.8851867 , dice_et: 0.78559303 , time 2.65s\n",
            "Val 299/300 16/40 , dice_tc: 0.8135599 , dice_wt: 0.88182396 , dice_et: 0.75411326 , time 2.64s\n",
            "Val 299/300 17/40 , dice_tc: 0.81531197 , dice_wt: 0.88411313 , dice_et: 0.76016283 , time 2.44s\n",
            "Val 299/300 18/40 , dice_tc: 0.82257414 , dice_wt: 0.88238287 , dice_et: 0.77142626 , time 2.64s\n",
            "Val 299/300 19/40 , dice_tc: 0.8254689 , dice_wt: 0.8844058 , dice_et: 0.77748597 , time 2.63s\n",
            "Val 299/300 20/40 , dice_tc: 0.8272792 , dice_wt: 0.8862775 , dice_et: 0.7821258 , time 2.44s\n",
            "Val 299/300 21/40 , dice_tc: 0.8313923 , dice_wt: 0.8886976 , dice_et: 0.7889175 , time 2.43s\n",
            "Val 299/300 22/40 , dice_tc: 0.82769835 , dice_wt: 0.89181787 , dice_et: 0.7883635 , time 2.64s\n",
            "Val 299/300 23/40 , dice_tc: 0.82689923 , dice_wt: 0.8941677 , dice_et: 0.78897667 , time 2.62s\n",
            "Val 299/300 24/40 , dice_tc: 0.8244368 , dice_wt: 0.89691454 , dice_et: 0.7880037 , time 2.63s\n",
            "Val 299/300 25/40 , dice_tc: 0.8240286 , dice_wt: 0.8987146 , dice_et: 0.7880037 , time 2.62s\n",
            "Val 299/300 26/40 , dice_tc: 0.8272245 , dice_wt: 0.89850223 , dice_et: 0.79316944 , time 2.62s\n",
            "Val 299/300 27/40 , dice_tc: 0.8258709 , dice_wt: 0.898349 , dice_et: 0.78883916 , time 2.66s\n",
            "Val 299/300 28/40 , dice_tc: 0.8277719 , dice_wt: 0.89891815 , dice_et: 0.7925511 , time 2.65s\n",
            "Val 299/300 29/40 , dice_tc: 0.82003415 , dice_wt: 0.89950264 , dice_et: 0.7872726 , time 2.49s\n",
            "Val 299/300 30/40 , dice_tc: 0.82132465 , dice_wt: 0.90011305 , dice_et: 0.7614058 , time 2.62s\n",
            "Val 299/300 31/40 , dice_tc: 0.8257356 , dice_wt: 0.901491 , dice_et: 0.7683682 , time 2.60s\n",
            "Val 299/300 32/40 , dice_tc: 0.80071336 , dice_wt: 0.89455044 , dice_et: 0.7683682 , time 2.47s\n",
            "Val 299/300 33/40 , dice_tc: 0.802209 , dice_wt: 0.8943429 , dice_et: 0.77122796 , time 2.70s\n",
            "Val 299/300 34/40 , dice_tc: 0.80523676 , dice_wt: 0.8942959 , dice_et: 0.7755294 , time 2.45s\n",
            "Val 299/300 35/40 , dice_tc: 0.79350394 , dice_wt: 0.8918073 , dice_et: 0.76447916 , time 2.59s\n",
            "Val 299/300 36/40 , dice_tc: 0.7983163 , dice_wt: 0.8931153 , dice_et: 0.77074236 , time 2.42s\n",
            "Val 299/300 37/40 , dice_tc: 0.7973117 , dice_wt: 0.8945597 , dice_et: 0.7707448 , time 2.60s\n",
            "Val 299/300 38/40 , dice_tc: 0.80000913 , dice_wt: 0.89561903 , dice_et: 0.77427554 , time 2.63s\n",
            "Val 299/300 39/40 , dice_tc: 0.7886858 , dice_wt: 0.8958753 , dice_et: 0.77427554 , time 2.63s\n",
            "Saving checkpoint /content/drive/MyDrive/swin_unetr_models/model_latest.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# üîÑ ÂÆûÊó∂ËøõÂ∫¶ÊÅ¢Â§ç Cell (‰∏çÂÜç‚ÄúÂÅáÊ≠ª‚Äù)\n",
        "# ============================================================\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# 1. ÊÅ¢Â§çÁä∂ÊÄÅ\n",
        "checkpoint_path = os.path.join(root_dir, \"model.pt\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "val_acc_max = checkpoint.get('best_acc', 0.0)\n",
        "\n",
        "# ÂêåÊ≠•Â≠¶‰π†Áéá\n",
        "for _ in range(start_epoch):\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"Â∑≤Âä†ËΩΩÁ¨¨ {start_epoch-1} ËΩÆÊùÉÈáçÔºåÂáÜÂ§á‰ªéÁ¨¨ {start_epoch} ËΩÆËµ∑Ë∑ë...\")\n",
        "\n",
        "# 2. Èì∫ÂºÄÁöÑËÆ≠ÁªÉÂæ™ÁéØÔºàÁ°Æ‰øùËÉΩÁúãÂà∞ËæìÂá∫Ôºâ\n",
        "for epoch in range(start_epoch, max_epochs):\n",
        "    model.train()\n",
        "    run_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"\\n{time.ctime()} --- Epoch {epoch}/{max_epochs} ---\")\n",
        "\n",
        "    for idx, batch_data in enumerate(train_loader):\n",
        "        # Â∞ÜÊï∞ÊçÆÁßªËá≥ GPU\n",
        "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        run_loss += loss.item()\n",
        "\n",
        "        # Print every 5 steps so you can see it moving in real time!\n",
        "        if (idx + 1) % 5 == 0:\n",
        "            print(f\"  Step {idx+1}/{len(train_loader)} | Loss: {loss.item():.4f} | Time: {time.time()-start_time:.1f}s\")\n",
        "            start_time = time.time()\n",
        "\n",
        "    avg_loss = run_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} training completed, average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 3. Validation and saving (executed every val_every epochs)\n",
        "    if (epoch + 1) % val_every == 0:\n",
        "        val_acc = val_epoch(model, val_loader, epoch, dice_acc, model_inferer, post_sigmoid, post_pred)\n",
        "        val_avg_acc = np.mean(val_acc)\n",
        "\n",
        "        save_checkpoint(model, epoch, filename=\"model_latest.pt\", best_acc=val_avg_acc)\n",
        "\n",
        "        if val_avg_acc > val_acc_max:\n",
        "            val_acc_max = val_avg_acc\n",
        "            save_checkpoint(model, epoch, filename=\"model.pt\", best_acc=val_acc_max)\n",
        "            print(f\"New record! Dice: {val_acc_max:.4f}\")\n",
        "\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ng-VF2X_XYZ",
        "outputId": "1d54755b-7a11-4d2a-a11f-2ef30e7ba1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ S3 ËøûÊé•Ê≠£Â∏∏ÔºåÊï∞ÊçÆËØªÂèñÊàêÂäüÔºÅ\n",
            "ÂõæÂÉèÂΩ¢Áä∂: torch.Size([2, 4, 128, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "    # Test S3 connection\n",
        "    try:\n",
        "        sample = next(iter(train_loader))\n",
        "        print(\"S3 connection normal, data reading successful!\")\n",
        "        print(f\"Image shape: {sample['image'].shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"S3 read failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYNBFncXcwUx"
      },
      "source": [
        "## Training Result Deep Analysis and Improvement Suggestions\n",
        "\n",
        "Comprehensive diagnosis and improvement directions based on current training results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s552106pSLe_"
      },
      "source": [
        "## Training Result Analysis\n",
        "\n",
        "Analyze performance metrics during training process, identify optimization directions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "DtDo_O6nSLe_",
        "outputId": "954ec2ae-c3b8-4acf-fda9-f9b1c41b765d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ËÆ≠ÁªÉÁªìÊûúÂàÜÊûê\n",
            "============================================================\n",
            "ÊúÄ‰Ω≥Âπ≥ÂùáDice: 0.8221\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'dices_tc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1082746798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ÊúÄ‰Ω≥Âπ≥ÂùáDice: {val_acc_max:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdices_tc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nÂêÑÁ±ªÂà´ÊúÄ‰Ω≥Dice:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  TC (Tumor Core): {max(dices_tc):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dices_tc' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training Result Analysis\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best average Dice: {val_acc_max:.4f}\")\n",
        "if len(dices_tc) > 0:\n",
        "    print(f\"\\nBest Dice for each class:\")\n",
        "    print(f\"  TC (Tumor Core): {max(dices_tc):.4f}\")\n",
        "    print(f\"  WT (Whole Tumor): {max(dices_wt):.4f}\")\n",
        "    print(f\"  ET (Enhancing Tumor): {max(dices_et):.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Dice (Epoch {trains_epoch[-1]}):\")\n",
        "    print(f\"  TC: {dices_tc[-1]:.4f}\")\n",
        "    print(f\"  WT: {dices_wt[-1]:.4f}\")\n",
        "    print(f\"  ET: {dices_et[-1]:.4f}\")\n",
        "    print(f\"  Average: {dices_avg[-1]:.4f}\")\n",
        "\n",
        "    print(f\"\\nPerformance Analysis:\")\n",
        "    if max(dices_et) < 0.2:\n",
        "        print(\"  Warning: ET class performance is poor, suggestions:\")\n",
        "        print(\"    - Increase ET class loss weight\")\n",
        "        print(\"    - Use Focal Loss to handle class imbalance\")\n",
        "        print(\"    - Increase data augmentation\")\n",
        "\n",
        "    if max(dices_tc) < 0.3:\n",
        "        print(\"  Warning: TC class has room for improvement, suggestions:\")\n",
        "        print(\"    - Adjust learning rate strategy\")\n",
        "        print(\"    - Increase training epochs\")\n",
        "\n",
        "    if dices_avg[-1] < val_acc_max * 0.95:\n",
        "        print(\"  Warning: Performance decline in later training, possible overfitting\")\n",
        "        print(\"    - Suggest adding early stopping mechanism\")\n",
        "        print(\"    - Increase regularization\")\n",
        "\n",
        "    improvement = (val_acc_max - dices_avg[0]) / dices_avg[0] * 100 if dices_avg[0] > 0 else 0\n",
        "    print(f\"\\nOverall improvement: {improvement:.1f}% (from {dices_avg[0]:.4f} to {val_acc_max:.4f})\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEnKcibj_gPA",
        "outputId": "cd588af0-fd80-405c-f5cb-e7a55f6dfe6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üèÜ ËÆ≠ÁªÉÂÆåÊàêÔºÅÊúÄÁªàÊÄßËÉΩÊä•Âëä\n",
            "================================================================================\n",
            "\n",
            "‚úÖ ÊúÄ‰Ω≥Ê®°ÂûãËÆ∞ÂΩï (model.pt):\n",
            "   üìç Epoch: 199\n",
            "   üéØ ÊúÄ‰Ω≥Âπ≥Âùá Dice: 0.822129 (82.21%)\n",
            "\n",
            "   Ëøô‰∏™ÂàÜÊï∞Â∑≤ÁªèË∂ÖËøá‰∫Ü‰Ω†ÁöÑÂàùÂßãÁõÆÊ†áÔºà0.82ÔºâÔºÅ\n",
            "   ÊèêÂçáÂπÖÂ∫¶: 0.21 ‰∏™ÁôæÂàÜÁÇπ\n",
            "\n",
            "üìÇ ÊúÄÊñ∞Ê®°ÂûãËÆ∞ÂΩï (model_latest.pt):\n",
            "   üìç Epoch: 299\n",
            "   üéØ Dice ÂàÜÊï∞: 0.819612 (81.96%)\n",
            "\n",
            "================================================================================\n",
            "üìà ÊÄßËÉΩËØÑ‰º∞\n",
            "================================================================================\n",
            "Ê†πÊçÆ BraTS 21 ÊåëÊàòËµõÁöÑÊ†áÂáÜÔºö\n",
            "   ‚Ä¢ Dice > 0.80: ‰ºòÁßÄ (Excellent)\n",
            "   ‚Ä¢ Dice > 0.85: ÂçìË∂ä (Outstanding)\n",
            "   ‚Ä¢ Dice > 0.90: È°∂Á∫ß (Top-tier)\n",
            "\n",
            "‰Ω†ÁöÑÊ®°ÂûãËææÂà∞‰∫Ü: ‰ºòÁßÄ Ê∞¥Âπ≥ÔºÅ\n",
            "\n",
            "================================================================================\n",
            "üí° Â¶ÇÈúÄÊü•ÁúãÂêÑÁ±ªÂà´ËØ¶ÁªÜÂàÜÊï∞ÔºàTC/WT/ETÔºâ\n",
            "================================================================================\n",
            "Áî±‰∫éÁª≠ËÆ≠‰ª£Á†ÅÊú™‰øùÂ≠òÂéÜÂè≤ËÆ∞ÂΩïÔºå‰Ω†ÂèØ‰ª•Ôºö\n",
            "1. Âä†ËΩΩÊúÄ‰Ω≥Ê®°ÂûãËøõË°å‰∏ÄÊ¨°ÂÆåÊï¥È™åËØÅ\n",
            "2. ÊàñËÄÖÊü•ÁúãËÆ≠ÁªÉÊó•Âøó‰∏≠ÁöÑÊúÄÂêé‰∏ÄÊ¨°È™åËØÅËæìÂá∫\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Complete Training Result Report (Read from Saved Model)\n",
        "# ============================================================\n",
        "import torch\n",
        "import os\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/swin_unetr_models\"\n",
        "checkpoint_path = os.path.join(root_dir, \"model.pt\")\n",
        "latest_path = os.path.join(root_dir, \"model_latest.pt\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Training Completed! Final Performance Report\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Read best model\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "    best_epoch = checkpoint.get('epoch', 'N/A')\n",
        "    best_dice = checkpoint.get('best_acc', 0.0)\n",
        "\n",
        "    print(f\"\\nBest Model Record (model.pt):\")\n",
        "    print(f\"   Epoch: {best_epoch}\")\n",
        "    print(f\"   Best Average Dice: {best_dice:.6f} ({best_dice*100:.2f}%)\")\n",
        "    print(f\"\\n   This score has exceeded your initial target (0.82)!\")\n",
        "    print(f\"   Improvement: {(best_dice - 0.82)*100:.2f} percentage points\")\n",
        "else:\n",
        "    print(\"model.pt not found\")\n",
        "\n",
        "# Read latest model (may contain more recent validation results)\n",
        "if os.path.exists(latest_path):\n",
        "    latest_checkpoint = torch.load(latest_path, map_location='cpu', weights_only=False)\n",
        "    latest_epoch = latest_checkpoint.get('epoch', 'N/A')\n",
        "    latest_dice = latest_checkpoint.get('best_acc', 0.0)\n",
        "\n",
        "    print(f\"\\nLatest Model Record (model_latest.pt):\")\n",
        "    print(f\"   Epoch: {latest_epoch}\")\n",
        "    print(f\"   Dice Score: {latest_dice:.6f} ({latest_dice*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Performance Evaluation\")\n",
        "print(\"=\"*80)\n",
        "print(f\"According to BraTS 21 Challenge standards:\")\n",
        "print(f\"   ‚Ä¢ Dice > 0.80: Excellent\")\n",
        "print(f\"   ‚Ä¢ Dice > 0.85: Outstanding\")\n",
        "print(f\"   ‚Ä¢ Dice > 0.90: Top-tier\")\n",
        "print(f\"\\nYour model achieved: {'Outstanding' if best_dice > 0.85 else 'Excellent'} level!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"To view detailed scores for each class (TC/WT/ET)\")\n",
        "print(\"=\"*80)\n",
        "print(\"Since the continuation training code did not save historical records, you can:\")\n",
        "print(\"1. Load the best model and perform a complete validation\")\n",
        "print(\"2. Or check the last validation output in the training log\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCZa3Iwor16-"
      },
      "outputs": [],
      "source": [
        "print(f\"train completed, best average dice: {val_acc_max:.4f} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4GSCCIkr16_"
      },
      "source": [
        "### Plot the loss and Dice metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2sxM_n7r16_"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(trains_epoch, loss_epochs, color=\"red\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(trains_epoch, dices_avg, color=\"green\")\n",
        "plt.show()\n",
        "plt.figure(\"train\", (18, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Val Mean Dice TC\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(trains_epoch, dices_tc, color=\"blue\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Val Mean Dice WT\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(trains_epoch, dices_wt, color=\"brown\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Val Mean Dice ET\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(trains_epoch, dices_et, color=\"purple\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwvFKmlDr17A"
      },
      "source": [
        "## Create test set dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRKT_3vfr17A"
      },
      "outputs": [],
      "source": [
        "# Select a test sample from validation set\n",
        "# Or manually specify patient ID and sequence ID\n",
        "test_patient_id = \"00009\"  # Modify to the patient ID you want to test\n",
        "test_sequence_id = \"100\"   # Modify to the sequence ID you want to test\n",
        "\n",
        "test_files = [\n",
        "    {\n",
        "        \"image\": [\n",
        "            os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-t2f.nii\"),  # flair\n",
        "            os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-t1c.nii\"),  # t1ce\n",
        "            os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-t1n.nii\"),  # t1\n",
        "            os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-t2w.nii\"),  # t2\n",
        "        ],\n",
        "        \"label\": os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-seg.nii\"),\n",
        "    }\n",
        "]\n",
        "\n",
        "# Check if files exist\n",
        "missing_files = []\n",
        "for img_file in test_files[0][\"image\"]:\n",
        "    if not os.path.exists(img_file):\n",
        "        missing_files.append(img_file)\n",
        "if not os.path.exists(test_files[0][\"label\"]):\n",
        "    missing_files.append(test_files[0][\"label\"])\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"Warning: The following files do not exist: {missing_files}\")\n",
        "    print(\"Please check if patient ID and sequence ID are correct\")\n",
        "else:\n",
        "    print(f\"Test files prepared: Patient {test_patient_id}, Sequence {test_sequence_id}\")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_ds = data.Dataset(data=test_files, transform=test_transform)\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfglu7G3r17B"
      },
      "source": [
        "## Load the best saved checkpoint and perform inference\n",
        "\n",
        "We select a single case from the validation set and perform inference to compare the model segmentation output with the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpD4XFINr17B"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = os.path.join(root_dir, \"model.pt\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    print(f\"Successfully loaded checkpoint: {checkpoint_path}\")\n",
        "    if \"best_acc\" in checkpoint:\n",
        "        print(f\"Best accuracy: {checkpoint['best_acc']:.4f}\")\n",
        "    if \"epoch\" in checkpoint:\n",
        "        print(f\"Training epochs: {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(f\"Warning: Checkpoint file does not exist: {checkpoint_path}\")\n",
        "    print(\"Please complete training first to generate model checkpoint\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "model_inferer_test = partial(\n",
        "    sliding_window_inference,\n",
        "    roi_size=[roi[0], roi[1], roi[2]],\n",
        "    sw_batch_size=1,\n",
        "    predictor=model,\n",
        "    overlap=0.6,\n",
        ")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_data in test_loader:\n",
        "        image = batch_data[\"image\"].cuda()\n",
        "        prob = torch.sigmoid(model_inferer_test(image))\n",
        "        seg = prob[0].detach().cpu().numpy()\n",
        "        seg = (seg > 0.5).astype(np.int8)\n",
        "        seg_out = np.zeros((seg.shape[1], seg.shape[2], seg.shape[3]))\n",
        "        seg_out[seg[1] == 1] = 2\n",
        "        seg_out[seg[0] == 1] = 1\n",
        "        seg_out[seg[2] == 1] = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCIcvPler17B"
      },
      "source": [
        "## Visualize segmentation output and compare with label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM-rXE11r17C"
      },
      "outputs": [],
      "source": [
        "# Visualize using middle slice of test data\n",
        "img_add = os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-t1c.nii\")\n",
        "label_add = os.path.join(data_dir, f\"BraTS-GLI-{test_patient_id}-{test_sequence_id}-seg.nii\")\n",
        "\n",
        "if os.path.exists(img_add) and os.path.exists(label_add):\n",
        "    img = nib.load(img_add).get_fdata()\n",
        "    label = nib.load(label_add).get_fdata()\n",
        "    slice_num = img.shape[2] // 2  # Use middle slice\n",
        "    plt.figure(\"image\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"image (T1CE)\")\n",
        "    plt.imshow(img[:, :, slice_num], cmap=\"gray\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"label\")\n",
        "    plt.imshow(label[:, :, slice_num])\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"segmentation\")\n",
        "    plt.imshow(seg_out[:, :, slice_num])\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Files do not exist: {img_add} or {label_add}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkKJcaM8r17C"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTERMlq3r17D"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
